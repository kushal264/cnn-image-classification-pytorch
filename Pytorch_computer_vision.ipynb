{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9bf807ee09294bbf9a306c4a32f9cec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6d133fe2e744607a8abcd22b5cfd727",
              "IPY_MODEL_03eda04196f54ff38c31014e337cc557",
              "IPY_MODEL_f6f9cc14fa46434bb870e369dec035aa"
            ],
            "layout": "IPY_MODEL_e83e653ab54e490683af1348902fe8fa"
          }
        },
        "c6d133fe2e744607a8abcd22b5cfd727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a86e15e74364c0d8e1f61ad3019b609",
            "placeholder": "​",
            "style": "IPY_MODEL_0e12b1a3b42744e6baa8febaef1bcbe8",
            "value": "100%"
          }
        },
        "03eda04196f54ff38c31014e337cc557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca125f4e0223424fa398b46552147d89",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ffc7a8faebf45428c95ceae24b6bf00",
            "value": 5
          }
        },
        "f6f9cc14fa46434bb870e369dec035aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c7df9e4be99426f99877f0b982bb6e6",
            "placeholder": "​",
            "style": "IPY_MODEL_01ba9d422b2d4444a7ea37e12b0efc25",
            "value": " 5/5 [00:46&lt;00:00,  8.95s/it]"
          }
        },
        "e83e653ab54e490683af1348902fe8fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a86e15e74364c0d8e1f61ad3019b609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e12b1a3b42744e6baa8febaef1bcbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca125f4e0223424fa398b46552147d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffc7a8faebf45428c95ceae24b6bf00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c7df9e4be99426f99877f0b982bb6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ba9d422b2d4444a7ea37e12b0efc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "313378c86e2f48128cf95fd9542d290d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0cd8d01f2ae43c7a3bd5eea7e5c7058",
              "IPY_MODEL_535adf058a5c4226ab69fed9fcf5e048",
              "IPY_MODEL_b9e23e026a7a48168f06e999114e5131"
            ],
            "layout": "IPY_MODEL_4e1246dd2dec4ea2912dff19c73c9514"
          }
        },
        "d0cd8d01f2ae43c7a3bd5eea7e5c7058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239977f3f16c42ff9082d34954d885f3",
            "placeholder": "​",
            "style": "IPY_MODEL_37bde70c5cdf488da2df462d2d5974f9",
            "value": "100%"
          }
        },
        "535adf058a5c4226ab69fed9fcf5e048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ffde68efe84702b797dd5fdbfded26",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3874e52833a04d9bb69d5151ab71ed17",
            "value": 3
          }
        },
        "b9e23e026a7a48168f06e999114e5131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5626693964f645a49168f9cae4fb451f",
            "placeholder": "​",
            "style": "IPY_MODEL_b15fb44501a74de68d6c0941bd43437f",
            "value": " 3/3 [00:27&lt;00:00,  9.07s/it]"
          }
        },
        "4e1246dd2dec4ea2912dff19c73c9514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "239977f3f16c42ff9082d34954d885f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37bde70c5cdf488da2df462d2d5974f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24ffde68efe84702b797dd5fdbfded26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3874e52833a04d9bb69d5151ab71ed17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5626693964f645a49168f9cae4fb451f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b15fb44501a74de68d6c0941bd43437f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4653649f7e1a491cbb0387d191c9fb20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f42f7ee44034f08bada4054100b491d",
              "IPY_MODEL_de2266de4f2242098f03fbf2db67cf74",
              "IPY_MODEL_055972a2331946cfaf3b68355ac36e21"
            ],
            "layout": "IPY_MODEL_9a178178c2fe4d338f6e74b1f17a6d87"
          }
        },
        "7f42f7ee44034f08bada4054100b491d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cef70f63167e40f38a930b4a2295d91b",
            "placeholder": "​",
            "style": "IPY_MODEL_5899c4b829544afb98aeb357183e047d",
            "value": "100%"
          }
        },
        "de2266de4f2242098f03fbf2db67cf74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d587c42d02e24231b029f50d069a71f4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66093c065b854067ad2d10b637fa552b",
            "value": 3
          }
        },
        "055972a2331946cfaf3b68355ac36e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a4c6e38cc34e62bc3ae78346b277c9",
            "placeholder": "​",
            "style": "IPY_MODEL_393b7ddd963f4f10ab63327afdcc6059",
            "value": " 3/3 [00:36&lt;00:00, 11.93s/it]"
          }
        },
        "9a178178c2fe4d338f6e74b1f17a6d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef70f63167e40f38a930b4a2295d91b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5899c4b829544afb98aeb357183e047d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d587c42d02e24231b029f50d069a71f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66093c065b854067ad2d10b637fa552b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15a4c6e38cc34e62bc3ae78346b277c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393b7ddd963f4f10ab63327afdcc6059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f64bfe53d1249dc9a13d7633ecc085a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a0b291154304d6081bcbcc14538abfc",
              "IPY_MODEL_00c2ad43dc9246d1ae68310535068f84",
              "IPY_MODEL_28879aff012b498b9e4998c296a61a3e"
            ],
            "layout": "IPY_MODEL_7006127553dd473e822b2fbc0c5609f5"
          }
        },
        "2a0b291154304d6081bcbcc14538abfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f61648423d24b918e6f2fd0960b4d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_4242529895d140c48e583e433beceda5",
            "value": "100%"
          }
        },
        "00c2ad43dc9246d1ae68310535068f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c757ef1ef66b425b9643bed05ab79723",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9877cbefb27466aafc3baa7558ecf89",
            "value": 3
          }
        },
        "28879aff012b498b9e4998c296a61a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79838c39924e4309be9a6f56ad3e0f65",
            "placeholder": "​",
            "style": "IPY_MODEL_78932f4ddd4f460faa014bf8237516a7",
            "value": " 3/3 [00:28&lt;00:00,  9.52s/it]"
          }
        },
        "7006127553dd473e822b2fbc0c5609f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f61648423d24b918e6f2fd0960b4d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4242529895d140c48e583e433beceda5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c757ef1ef66b425b9643bed05ab79723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9877cbefb27466aafc3baa7558ecf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79838c39924e4309be9a6f56ad3e0f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78932f4ddd4f460faa014bf8237516a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32560de5bdd541b5a09118a707ea4b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c568c52541e848fc8a0d7070419dd995",
              "IPY_MODEL_a29258a914d0486090072534fd99fdfb",
              "IPY_MODEL_7c67efbff78a4aa1ac5faf17ffd67bb7"
            ],
            "layout": "IPY_MODEL_6517f3cd12304fe39ae957a038401664"
          }
        },
        "c568c52541e848fc8a0d7070419dd995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16a06086214e467c865b9fa024709b66",
            "placeholder": "​",
            "style": "IPY_MODEL_f41b281a6c8d476990b79f211471fe22",
            "value": "100%"
          }
        },
        "a29258a914d0486090072534fd99fdfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2de3935b374b059085ab7d87e91ad1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64f7fbe3d9f94167af7abba3aea213a5",
            "value": 3
          }
        },
        "7c67efbff78a4aa1ac5faf17ffd67bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11d556973ebf48b1b4a143e53382aa2f",
            "placeholder": "​",
            "style": "IPY_MODEL_33dc1cf5ebbb47ec90dd4393d155392b",
            "value": " 3/3 [00:33&lt;00:00, 11.28s/it]"
          }
        },
        "6517f3cd12304fe39ae957a038401664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16a06086214e467c865b9fa024709b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41b281a6c8d476990b79f211471fe22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e2de3935b374b059085ab7d87e91ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64f7fbe3d9f94167af7abba3aea213a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11d556973ebf48b1b4a143e53382aa2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33dc1cf5ebbb47ec90dd4393d155392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YGgvYOuX4df"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PYTORCH COMPUTER VISION\n",
        "# COMPUTER VISION LIBRARIES IN PYTORCH\n",
        "torch.vision - library for pytorch computer vision\n",
        "torchvision.datasets - get dataset\n",
        "\n",
        "torchvision.models - get pretrained computer vision models\n",
        "\n",
        "torchvision.transforms - functions for manipulating your vision data\n",
        "\n",
        "torch.utils.data.Dataset - base dataset class for pytorch\n",
        "\n",
        "torch.utils.data.Dataloader - creates a python iterable over a dataset"
      ],
      "metadata": {
        "id": "BL6xFwWQdyIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQVkbSl7es-e",
        "outputId": "e203ef5b-ac02-45b2-d9d9-daa717d6320a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.24.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# root(string) means the directory where datset\n",
        "#set up training data\n",
        "from torchvision import datasets\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",#where to download data to\n",
        "    train=True,# do we want the training dataset\n",
        "    download=True,# do we want to downlooad yes/no\n",
        "    transform=ToTensor(),# to convert image to tensor\n",
        "    target_transform=None # do u want to convert label/targets to tensor as well\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8xuWE46fv1o",
        "outputId": "fff2380b-cfc9-4849-a76d-dbdb290dcfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.1MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 191kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.57MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 30.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",#where to download data to\n",
        "    train=False,# do we want the test dataset\n",
        "    download=True,# do we want to downlooad yes/no\n",
        "    transform=ToTensor(),# to convert image to tensor\n",
        "    target_transform=None # do u want to convert label/targets to tensor as well\n",
        ")"
      ],
      "metadata": {
        "id": "lNz7TV4Sifj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7pazYQgi7Tz",
        "outputId": "0386cab6-daa8-4a97-a53f-10b5e2203b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image , label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oi83faJjFNz",
        "outputId": "083cea0f-c6bb-4a0a-a3ad-cc71156c9562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "           0.0157, 0.0000, 0.0000, 0.0118],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0471, 0.0392, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "           0.3020, 0.5098, 0.2824, 0.0588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "           0.5529, 0.3451, 0.6745, 0.2588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "           0.4824, 0.7686, 0.8980, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "           0.8745, 0.9608, 0.6784, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "           0.8627, 0.9529, 0.7922, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "           0.8863, 0.7725, 0.8196, 0.2039],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "           0.9608, 0.4667, 0.6549, 0.2196],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "           0.8510, 0.8196, 0.3608, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "           0.8549, 1.0000, 0.3020, 0.0000],\n",
              "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "           0.8784, 0.9569, 0.6235, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "           0.9137, 0.9333, 0.8431, 0.0000],\n",
              "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "           0.8627, 0.9098, 0.9647, 0.0000],\n",
              "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "           0.8706, 0.8941, 0.8824, 0.0000],\n",
              "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "           0.8745, 0.8784, 0.8980, 0.1137],\n",
              "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "           0.8627, 0.8667, 0.9020, 0.2627],\n",
              "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "           0.7098, 0.8039, 0.8078, 0.4510],\n",
              "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "           0.6549, 0.6941, 0.8235, 0.3608],\n",
              "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "           0.7529, 0.8471, 0.6667, 0.0000],\n",
              "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "           0.3882, 0.2275, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUWMVZ8YjhKb",
        "outputId": "ab3135da-f041-48be-c37d-6c4387a5de99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shis-TFHkKWs",
        "outputId": "d9e57420-47a8-4790-f4af-06aea924030f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'T-shirt/top': 0,\n",
              " 'Trouser': 1,\n",
              " 'Pullover': 2,\n",
              " 'Dress': 3,\n",
              " 'Coat': 4,\n",
              " 'Sandal': 5,\n",
              " 'Shirt': 6,\n",
              " 'Sneaker': 7,\n",
              " 'Bag': 8,\n",
              " 'Ankle boot': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVEqdstskbkJ",
        "outputId": "61277021-afbd-41e6-e584-3742e487c800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 0, 0,  ..., 3, 0, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape , label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlDwPCPAkrGA",
        "outputId": "b5317124-ca61-4038-9b31-e9efdc158f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 9)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image,label= train_data[0]\n",
        "print(f\"image shape: {image.shape}, label: {label}\")\n",
        "plt.imshow(image.squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "9YiJfiRakw0Y",
        "outputId": "ec4397cc-e03f-415f-d022-0e0606b46cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image shape: torch.Size([1, 28, 28]), label: 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78bd8f0a7e90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "NxyadDVjly0S",
        "outputId": "4453403b-606e-4cf1-eafa-2b443c76637e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "fig=plt.figure(figsize=(9,9))\n",
        "rows, cols = 4,4\n",
        "for i in range(1,rows*cols+1):\n",
        "  random_idx=torch.randint(0,len(train_data),size=[1]).item()\n",
        "  img, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows,cols,i)\n",
        "  plt.imshow(train_data[random_idx][0].squeeze(),cmap=\"gray\")\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "XVGW_zQEmOx8",
        "outputId": "fba86834-609c-4e7e-e742-ba4c09f1745a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbBTzTf7v5VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prepare DATALOADER\n",
        "our dataset is in form of pytorch datasets\n",
        "dataloader turns our data into python iterable(coverting dsata into mini batches of 32)\n",
        "\n",
        "why?\n",
        "cpu or hardware can't see 6000 images at one hit so we break it into 32 imag at a time\n",
        "2. it gives our neural network more chances to update its gradient"
      ],
      "metadata": {
        "id": "BPmIB5ws0nK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size= 32\n",
        "train_dataloader= DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader= DataLoader(dataset=test_data, batch_size=batch_size,shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "BxSEZcED1cvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6000/32\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFyoo2_1297y",
        "outputId": "ac772111-e3e8-415b-9d5e-9c013b45bf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187.5"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10000/32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwaseDMz3eDS",
        "outputId": "fb6ccfff-38f2-498a-ee4c-c2d88532be26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312.5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_feature_batch,train_label_batch=next(iter(train_dataloader))\n",
        "train_feature_batch.shape,train_label_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z7IXBtK3lQ2",
        "outputId": "318977d6-4aec-42d5-8477-29e24a998064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx=torch.randint(0,len(train_feature_batch),size=[1]).item()\n",
        "img, label = train_feature_batch[random_idx],train_label_batch[random_idx]\n",
        "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"{image.size}\")\n",
        "print(f\"{label.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "NWgfYybHiMTp",
        "outputId": "8785cfc2-38f2-4beb-d9a1-1711c4dfee21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method size of Tensor object at 0x78bd8ee1f930>\n",
            "torch.Size([])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTVO(nn.Module):\n",
        "  def __init__(self,input_shape,hidden_units,output_shape):\n",
        "    super().__init__()\n",
        "    self.layer_stack=nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
        "\n",
        "        nn.Linear(in_features=hidden_units,out_features=output_shape),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layer_stack(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "QKHzK676iv9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_0=FashionMNISTVO(\n",
        "    input_shape=784,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ").to(\"cpu\")\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTfvmyTlmW2B",
        "outputId": "e3d1175b-0e31-41f3-9ae6-18c866a9782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTVO(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x =torch.rand([1,1,28,28])# img 1, color channel =1, size =28*28\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "50ol64cJnEkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5636ada6-a42b-4a34-cb7c-9a79d29ee272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0315,  0.3171,  0.0531, -0.2525,  0.5959,  0.2112,  0.3233,  0.2694,\n",
              "         -0.1004,  0.0157]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "if Path (\"helper_function.py\").is_file():\n",
        "  print(\"helper_function.py already exists\")\n",
        "else:\n",
        "  print(\"downloading helper_function.py\")\n",
        "  request =requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_function.py\",\"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td0VgBu5Hofg",
        "outputId": "70e8057d-0478-49e1-f2b9-84f12140aff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading helper_function.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_function import accuracy_fn\n",
        "\n",
        "loss_fn= nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "I9JFiVgpKbam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start:float,\n",
        "\n",
        "                     end:float,\n",
        "                     device:torch.device=None):\n",
        "  total_time=end-start\n",
        "  print(f\"train time on {device}: {total_time:3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "aB_0je4-LxFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=timer()\n",
        "end_time=timer()\n",
        "print_train_time(start=start_time,end=end_time,device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nuFcELaNfEE",
        "outputId": "8cc66801-9b70-4df8-d10a-32bcd9d043ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train time on cpu: 0.000039 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8736000021799555e-05"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_time_start_on_cpu = timer()\n",
        "epochs = 5\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---\")\n",
        "\n",
        "    train_loss = 0.0\n",
        "    model_0.train()\n",
        "\n",
        "    for batch, (x, y) in enumerate(train_dataloader):\n",
        "        y_pred = model_0(x)\n",
        "\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(x)} / {len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    # ---- testing ----\n",
        "    test_loss, test_acc = 0.0, 0.0\n",
        "    model_0.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for x, y in test_dataloader:\n",
        "            test_pred = model_0(x)\n",
        "\n",
        "            test_loss += loss_fn(test_pred, y).item()\n",
        "            test_acc += accuracy_fn(\n",
        "                y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1)\n",
        "            )\n",
        "\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "    print(\n",
        "        f\"Train loss: {train_loss:.5f} | \"\n",
        "        f\"Test loss: {test_loss:.5f} | \"\n",
        "        f\"Test acc: {test_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(\n",
        "    start=train_time_start_on_cpu,\n",
        "    end=train_time_end_on_cpu,\n",
        "    device=\"cpu\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760,
          "referenced_widgets": [
            "9bf807ee09294bbf9a306c4a32f9cec2",
            "c6d133fe2e744607a8abcd22b5cfd727",
            "03eda04196f54ff38c31014e337cc557",
            "f6f9cc14fa46434bb870e369dec035aa",
            "e83e653ab54e490683af1348902fe8fa",
            "9a86e15e74364c0d8e1f61ad3019b609",
            "0e12b1a3b42744e6baa8febaef1bcbe8",
            "ca125f4e0223424fa398b46552147d89",
            "1ffc7a8faebf45428c95ceae24b6bf00",
            "8c7df9e4be99426f99877f0b982bb6e6",
            "01ba9d422b2d4444a7ea37e12b0efc25"
          ]
        },
        "id": "mhZeedxVNwSl",
        "outputId": "b93203c6-8139-4cfc-d61e-8ea6bab7d3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bf807ee09294bbf9a306c4a32f9cec2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---\n",
            "Looked at 0 / 60000 samples\n",
            "Looked at 12800 / 60000 samples\n",
            "Looked at 25600 / 60000 samples\n",
            "Looked at 38400 / 60000 samples\n",
            "Looked at 51200 / 60000 samples\n",
            "Train loss: 0.59039 | Test loss: 0.50954 | Test acc: 82.04%\n",
            "Epoch: 1\n",
            "---\n",
            "Looked at 0 / 60000 samples\n",
            "Looked at 12800 / 60000 samples\n",
            "Looked at 25600 / 60000 samples\n",
            "Looked at 38400 / 60000 samples\n",
            "Looked at 51200 / 60000 samples\n",
            "Train loss: 0.47633 | Test loss: 0.47989 | Test acc: 83.20%\n",
            "Epoch: 2\n",
            "---\n",
            "Looked at 0 / 60000 samples\n",
            "Looked at 12800 / 60000 samples\n",
            "Looked at 25600 / 60000 samples\n",
            "Looked at 38400 / 60000 samples\n",
            "Looked at 51200 / 60000 samples\n",
            "Train loss: 0.45503 | Test loss: 0.47664 | Test acc: 83.43%\n",
            "Epoch: 3\n",
            "---\n",
            "Looked at 0 / 60000 samples\n",
            "Looked at 12800 / 60000 samples\n",
            "Looked at 25600 / 60000 samples\n",
            "Looked at 38400 / 60000 samples\n",
            "Looked at 51200 / 60000 samples\n",
            "Train loss: 0.44251 | Test loss: 0.46306 | Test acc: 83.75%\n",
            "Epoch: 4\n",
            "---\n",
            "Looked at 0 / 60000 samples\n",
            "Looked at 12800 / 60000 samples\n",
            "Looked at 25600 / 60000 samples\n",
            "Looked at 38400 / 60000 samples\n",
            "Looked at 51200 / 60000 samples\n",
            "Train loss: 0.43582 | Test loss: 0.46869 | Test acc: 83.27%\n",
            "train time on cpu: 46.562297 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "def eval_model(\n",
        "    model: torch.nn.Module,\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    accuracy_fn\n",
        "):\n",
        "    loss, acc = 0.0, 0.0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for x, y in data_loader:\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss += loss_fn(y_pred, y).item()\n",
        "            acc  += accuracy_fn(\n",
        "                y_true=y,\n",
        "                y_pred=y_pred.argmax(dim=1)\n",
        "            )\n",
        "\n",
        "    loss /= len(data_loader)\n",
        "    acc  /= len(data_loader)\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model.__class__.__name__,\n",
        "        \"model_loss\": loss,\n",
        "        \"model_acc\": acc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "QPwFHxilgsrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results = eval_model(\n",
        "    model=model_0,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "\n",
        "model_0_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ZS6Iv_lvJG",
        "outputId": "b3e2dffa-39c2-4bec-915e-8323608ef8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTVO',\n",
              " 'model_loss': 0.4686939154570095,\n",
              " 'model_acc': 83.2667731629393}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a non linear model\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack=nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self,x: torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "Yw-YLn2SmVqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1=FashionMNISTModelV2(input_shape=784,hidden_units=10,output_shape=len(class_names)).to(\"cpu\")\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-86wETN_fLqL",
        "outputId": "a14463a8-ea8d-44e8-af09-1d987da4bfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_function import accuracy_fn\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(params=model_1.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "GkzptHmVgLE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.to(device)\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.to(device)\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "2wYiKgdWiEUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQxJjNxjMuVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "epochs=3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n---\")\n",
        "  train_step(model=model_1,data_loader=train_dataloader,loss_fn=loss_fn,optimizer=optimizer,\n",
        "             )\n",
        "  test_step(model=model_1,data_loader=test_dataloader,loss_fn=loss_fn,accuracy_fn=accuracy_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "313378c86e2f48128cf95fd9542d290d",
            "d0cd8d01f2ae43c7a3bd5eea7e5c7058",
            "535adf058a5c4226ab69fed9fcf5e048",
            "b9e23e026a7a48168f06e999114e5131",
            "4e1246dd2dec4ea2912dff19c73c9514",
            "239977f3f16c42ff9082d34954d885f3",
            "37bde70c5cdf488da2df462d2d5974f9",
            "24ffde68efe84702b797dd5fdbfded26",
            "3874e52833a04d9bb69d5151ab71ed17",
            "5626693964f645a49168f9cae4fb451f",
            "b15fb44501a74de68d6c0941bd43437f"
          ]
        },
        "id": "LIXnDQ8-OWL8",
        "outputId": "d0e7508e-2c25-4186-d8d9-4cc3fd552c06",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "313378c86e2f48128cf95fd9542d290d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "train loss : 0.0007176587057949667    train_acc : 0.026689787264066178\n",
            "train loss : 0.0007453529458084552    train_acc : 0.025014234553207504\n",
            "train loss : 0.00044827645452397873    train_acc : 0.03668000759176171\n",
            "train loss : 0.0005151175445208633    train_acc : 0.03668622933738227\n",
            "train loss : 0.0005773149566113096    train_acc : 0.03168623265564661\n",
            "train loss : 0.0005221221522562376    train_acc : 0.03501689932408301\n",
            "train loss : 0.0002937276444793316    train_acc : 0.045018675679639505\n",
            "train loss : 0.0004714116710706266    train_acc : 0.03669067662702914\n",
            "train loss : 0.00042605806908386465    train_acc : 0.03668623502753442\n",
            "train loss : 0.0005197357199431599    train_acc : 0.03501956599201468\n",
            "train loss : 0.0004016755014558284    train_acc : 0.036685343768529076\n",
            "train loss : 0.0005318443615166293    train_acc : 0.03335289885000988\n",
            "train loss : 0.00039624480780701786    train_acc : 0.038351121546053336\n",
            "train loss : 0.00040246301920348666    train_acc : 0.038353787264824565\n",
            "train loss : 0.0005218516098342002    train_acc : 0.03835378868654124\n",
            "train loss : 0.0005923092275358243    train_acc : 0.03335378868729949\n",
            "train loss : 0.0003194401200564436    train_acc : 0.04168445535396656\n",
            "train loss : 0.0001847282669033449    train_acc : 0.05002223170952212\n",
            "train loss : 0.00044685354951334454    train_acc : 0.03669334519024508\n",
            "train loss : 0.00045341653691992925    train_acc : 0.03668623645076813\n",
            "train loss : 0.0008262886386609992    train_acc : 0.02335289932610708\n",
            "train loss : 0.0005142474118222923    train_acc : 0.035012454879640595\n",
            "train loss : 0.00028449756749170883    train_acc : 0.04335200664260247\n",
            "train loss : 0.0004820518518174897    train_acc : 0.03502312107020939\n",
            "train loss : 0.0004571060971591041    train_acc : 0.03668534566457078\n",
            "train loss : 0.0003986077934178338    train_acc : 0.04168623218435444\n",
            "train loss : 0.0003673026570971633    train_acc : 0.04168889932383166\n",
            "train loss : 0.000361828082911666    train_acc : 0.041688900746306046\n",
            "train loss : 0.0006102434942850072    train_acc : 0.033355567413731364\n",
            "train loss : 0.000598523336630383    train_acc : 0.030017789635953987\n",
            "train loss : 0.000682416520287186    train_acc : 0.023349342821139176\n",
            "train loss : 0.0005294275935288375    train_acc : 0.036679119649504606\n",
            "train loss : 0.0005249676806479452    train_acc : 0.0350195621971464\n",
            "train loss : 0.00041785632757095493    train_acc : 0.04001867709983848\n",
            "train loss : 0.0006418557633039037    train_acc : 0.030021343294453245\n",
            "train loss : 0.0006546865638981436    train_acc : 0.030016011383090374\n",
            "train loss : 0.0005070008537894827    train_acc : 0.038349341872737655\n",
            "train loss : 0.0005357788105912594    train_acc : 0.03168711964899879\n",
            "train loss : 0.0005126094143178785    train_acc : 0.030016899797146133\n",
            "train loss : 0.0005773387323655334    train_acc : 0.03168267567989181\n",
            "train loss : 0.000271385451527216    train_acc : 0.04168356409369594\n",
            "train loss : 0.0005288176565914004    train_acc : 0.033355564567516635\n",
            "train loss : 0.0003572327641820668    train_acc : 0.03835112296776934\n",
            "train loss : 0.000317172920584582    train_acc : 0.04168712059891614\n",
            "train loss : 0.0005011049994976678    train_acc : 0.03502223313098609\n",
            "train loss : 0.00045352675586748925    train_acc : 0.04001867852433653\n",
            "train loss : 0.0005192626926531781    train_acc : 0.03335467662854631\n",
            "train loss : 0.0004248424142950498    train_acc : 0.036684455827535226\n",
            "train loss : 0.0007292590991004495    train_acc : 0.03001956504310802\n",
            "train loss : 0.00048581030964614785    train_acc : 0.031682677101356325\n",
            "train loss : 0.0003808612678686765    train_acc : 0.04001689742778739\n",
            "train loss : 0.0007582567784550052    train_acc : 0.028354675678628156\n",
            "train loss : 0.0006727046152037177    train_acc : 0.033348455827028604\n",
            "train loss : 0.0004403396959506184    train_acc : 0.03835111917644108\n",
            "train loss : 0.0004895764883102524    train_acc : 0.03502045393022744\n",
            "train loss : 0.0005743626190978019    train_acc : 0.03501867757542946\n",
            "train loss : 0.000416569144112998    train_acc : 0.03835200996137356\n",
            "train loss : 0.0003527197160882861    train_acc : 0.03835378773864607\n",
            "train loss : 0.0003950190116521286    train_acc : 0.03835378868679394\n",
            "train loss : 0.0005785232327551077    train_acc : 0.035020455353966286\n",
            "train loss : 0.0005331426941311185    train_acc : 0.03501867757618878\n",
            "train loss : 0.0005479895480517788    train_acc : 0.0316853432947073\n",
            "train loss : 0.0005688493009889414    train_acc : 0.02668356551642384\n",
            "train loss : 0.0003902592883934702    train_acc : 0.043347564568275425\n",
            "train loss : 0.00036390430019306763    train_acc : 0.043356452034436416\n",
            "train loss : 0.0005107587274248653    train_acc : 0.03335645677441837\n",
            "train loss : 0.000463866682354415    train_acc : 0.03668445677694635\n",
            "train loss : 0.0002956729059704165    train_acc : 0.045019565043614374\n",
            "train loss : 0.0006041644594894668    train_acc : 0.0316906771013566\n",
            "train loss : 0.0004688674035910409    train_acc : 0.04168356836112072\n",
            "train loss : 0.0006042923701510558    train_acc : 0.03168889790312593\n",
            "train loss : 0.0005896570314922706    train_acc : 0.035016900745548335\n",
            "train loss : 0.0005431285477760081    train_acc : 0.033352009013730956\n",
            "train loss : 0.0005361878135986902    train_acc : 0.030017787738140657\n",
            "train loss : 0.00037818052732993166    train_acc : 0.03834934282012701\n",
            "train loss : 0.00042871846569042234    train_acc : 0.0400204529828374\n",
            "train loss : 0.00044789345112807195    train_acc : 0.03668801090825752\n",
            "train loss : 0.00036996747351247664    train_acc : 0.040019566939151076\n",
            "train loss : 0.00029858703690262137    train_acc : 0.04502134376903422\n",
            "train loss : 0.00032697580525757207    train_acc : 0.04335734471667682\n",
            "train loss : 0.00039098325550434047    train_acc : 0.03335645725051556\n",
            "train loss : 0.0006626143649587624    train_acc : 0.025017790110533608\n",
            "train loss : 0.000336785024780452    train_acc : 0.04334667615472562\n",
            "train loss : 0.0005590112786326823    train_acc : 0.031689784893949184\n",
            "train loss : 0.0006562898843437213    train_acc : 0.03168356788527677\n",
            "train loss : 0.0003177737463998238    train_acc : 0.04001689790287215\n",
            "train loss : 0.00048753917563064826    train_acc : 0.03668800901221487\n",
            "train loss : 0.0005705494419752127    train_acc : 0.03168623360480652\n",
            "train loss : 0.0003963364038857364    train_acc : 0.040016899324589225\n",
            "train loss : 0.0005140191212163497    train_acc : 0.03002134234630645\n",
            "train loss : 0.0005567578573171226    train_acc : 0.03501601138258469\n",
            "train loss : 0.00048745342403513296    train_acc : 0.03668534187273738\n",
            "train loss : 0.0005584931726895961    train_acc : 0.033352898848998794\n",
            "train loss : 0.0003498985278000113    train_acc : 0.041684454879386136\n",
            "train loss : 0.0006867939098097161    train_acc : 0.030022231709269008\n",
            "train loss : 0.0005907197472182071    train_acc : 0.03001601185691161\n",
            "train loss : 0.00023768145665469312    train_acc : 0.04334934187299035\n",
            "train loss : 0.0005079248629613714    train_acc : 0.031689786315665595\n",
            "train loss : 0.0005145263215744551    train_acc : 0.03335023455270169\n",
            "train loss : 0.00040578986229053307    train_acc : 0.041684453458428104\n",
            "train loss : 0.0004554359125827236    train_acc : 0.03668889837517783\n",
            "train loss : 0.0007576430939572512    train_acc : 0.021686234079133428\n",
            "train loss : 0.0005201258068669074    train_acc : 0.04001156599150887\n",
            "train loss : 0.0004715446237884833    train_acc : 0.035021339501862134\n",
            "train loss : 0.0005281347658159554    train_acc : 0.03335201138106766\n",
            "train loss : 0.0005172543408407593    train_acc : 0.03335112107273657\n",
            "train loss : 0.0005253944742775987    train_acc : 0.03501778726457213\n",
            "train loss : 0.0005789655894146017    train_acc : 0.03668534281987444\n",
            "train loss : 0.0003781726338307119    train_acc : 0.04168623218283727\n",
            "train loss : 0.000498484061049241    train_acc : 0.035022232657164185\n",
            "train loss : 0.0005357257580682366    train_acc : 0.03168534519075049\n",
            "train loss : 0.0004906339323285381    train_acc : 0.0300168988507684\n",
            "train loss : 0.00048178079832266504    train_acc : 0.035016009012720416\n",
            "train loss : 0.00046449848621743874    train_acc : 0.03668534187147345\n",
            "train loss : 0.0004819835024336161    train_acc : 0.036686232182331455\n",
            "train loss : 0.00043856637768201087    train_acc : 0.038352899323830576\n",
            "train loss : 0.0004565572078338848    train_acc : 0.03668712154630604\n",
            "train loss : 0.0004936842841749886    train_acc : 0.035019566464824695\n",
            "train loss : 0.0006007110165873021    train_acc : 0.03335201043544791\n",
            "train loss : 0.0006237010402195888    train_acc : 0.02835112107223224\n",
            "train loss : 0.0004953862179778795    train_acc : 0.033348453931238524\n",
            "train loss : 0.00041403624857528807    train_acc : 0.03835111917542999\n",
            "train loss : 0.0006093578458014863    train_acc : 0.03168712059689356\n",
            "train loss : 0.0003686813776715694    train_acc : 0.04001689979765168\n",
            "train loss : 0.0005789443793967048    train_acc : 0.035021342346558744\n",
            "train loss : 0.0006202515625923514    train_acc : 0.028352011382584832\n",
            "train loss : 0.0005850453088900232    train_acc : 0.03001512107273738\n",
            "train loss : 0.0003193242528126092    train_acc : 0.03834934139790546\n",
            "train loss : 0.0004964952269224636    train_acc : 0.035020452982078884\n",
            "train loss : 0.000378234984323662    train_acc : 0.03835201090825711\n",
            "train loss : 0.0005144265724827689    train_acc : 0.03335378773915107\n",
            "train loss : 0.0004567890372506041    train_acc : 0.036684455353460876\n",
            "train loss : 0.0005759321736763123    train_acc : 0.031686231709521846\n",
            "train loss : 0.0006783332995884933    train_acc : 0.028350232656911743\n",
            "train loss : 0.00048695953827817244    train_acc : 0.035015120124083685\n",
            "train loss : 0.0005786489431176806    train_acc : 0.03501867473073285\n",
            "train loss : 0.0003186537144440605    train_acc : 0.04001867662652306\n",
            "train loss : 0.0004846357384426254    train_acc : 0.036688009960867476\n",
            "train loss : 0.0003527049649068569    train_acc : 0.04168623360531246\n",
            "train loss : 0.0004147708068650727    train_acc : 0.03835556599125617\n",
            "train loss : 0.00042731230098143477    train_acc : 0.03668712296852867\n",
            "train loss : 0.0005931113278316823    train_acc : 0.03668623313224988\n",
            "train loss : 0.00043802956761865866    train_acc : 0.0383528993243372\n",
            "train loss : 0.0005699966722065711    train_acc : 0.030020454879639648\n",
            "train loss : 0.00042941888037646264    train_acc : 0.03834934424260247\n",
            "train loss : 0.0003205735986490426    train_acc : 0.04002045298359606\n",
            "train loss : 0.0004347710715137619    train_acc : 0.038354677574924584\n",
            "train loss : 0.0007184152046259988    train_acc : 0.03335378916137329\n",
            "train loss : 0.0004078288716337107    train_acc : 0.04001778868755273\n",
            "train loss : 0.0006288419161818961    train_acc : 0.03002134282063336\n",
            "train loss : 0.00033940013938451246    train_acc : 0.038349344716171005\n",
            "train loss : 0.00039773677792386637    train_acc : 0.04335378631718195\n",
            "train loss : 0.0005497919373777507    train_acc : 0.0283564553527025\n",
            "train loss : 0.0004041706396131021    train_acc : 0.03834845677618811\n",
            "train loss : 0.0005798073087730931    train_acc : 0.0366871191769473\n",
            "train loss : 0.0003496000465538554    train_acc : 0.03835289979689437\n",
            "train loss : 0.0004194567780651933    train_acc : 0.04168712154655834\n",
            "train loss : 0.0004643840254773542    train_acc : 0.03668889979815816\n",
            "train loss : 0.0006212431930256648    train_acc : 0.03168623407989235\n",
            "train loss : 0.0004419159803784028    train_acc : 0.04001689932484261\n",
            "train loss : 0.0004542678235326341    train_acc : 0.03835467567963991\n",
            "train loss : 0.00046677250383368355    train_acc : 0.03835378916036247\n",
            "train loss : 0.0004985031173426208    train_acc : 0.03668712202088553\n",
            "train loss : 0.000567545811159653    train_acc : 0.03168623313174447\n",
            "train loss : 0.00045488935593489716    train_acc : 0.033350232657670266\n",
            "train loss : 0.0003726015020046431    train_acc : 0.040017786790750756\n",
            "train loss : 0.0004291203885358511    train_acc : 0.04002134281962173\n",
            "train loss : 0.00048153468594387276    train_acc : 0.035021344716170466\n",
            "train loss : 0.00046172309911847345    train_acc : 0.038352011383848625\n",
            "train loss : 0.0005160685357464504    train_acc : 0.03335378773940472\n",
            "train loss : 0.00044487998305304914    train_acc : 0.036684455353461015\n",
            "train loss : 0.0004758362403732859    train_acc : 0.03335289837618851\n",
            "train loss : 0.0005620878603421965    train_acc : 0.03168445487913397\n",
            "train loss : 0.00037188651623547677    train_acc : 0.04001689837593554\n",
            "train loss : 0.0004990801563393392    train_acc : 0.0400213423458005\n",
            "train loss : 0.0004180337961700509    train_acc : 0.03502134471591776\n",
            "train loss : 0.000539426976626512    train_acc : 0.03335201138384849\n",
            "train loss : 0.0006252933528551448    train_acc : 0.031684454406071386\n",
            "train loss : 0.0005633442141737689    train_acc : 0.03501689837568324\n",
            "train loss : 0.0005704988272210293    train_acc : 0.03335200901246703\n",
            "train loss : 0.0005395222149547588    train_acc : 0.03335112107147332\n",
            "train loss : 0.0005464380798574159    train_acc : 0.03168445393123812\n",
            "train loss : 0.0005978039331339644    train_acc : 0.03668356504209666\n",
            "train loss : 0.0005193695858545887    train_acc : 0.03168623123468912\n",
            "train loss : 0.0005553642773858933    train_acc : 0.0283502326566585\n",
            "train loss : 0.0005989762454490785    train_acc : 0.03834845345741689\n",
            "train loss : 0.000489997296455662    train_acc : 0.03835378584184396\n",
            "train loss : 0.0004179080896530804    train_acc : 0.041687122019115654\n",
            "train loss : 0.0005159857856206339    train_acc : 0.03335556646507686\n",
            "train loss : 0.000377779668330456    train_acc : 0.038351122968781376\n",
            "train loss : 0.0006270104119331356    train_acc : 0.028353787265583352\n",
            "train loss : 0.000559372631362764    train_acc : 0.031681788686541645\n",
            "train loss : 0.00048094068258479    train_acc : 0.03668356362063282\n",
            "train loss : 0.0004245074223109853    train_acc : 0.04001956456726433\n",
            "train loss : 0.0005418392198154995    train_acc : 0.03835467710110254\n",
            "train loss : 0.0004725047605396308    train_acc : 0.03668712249445392\n",
            "train loss : 0.0006590737442788633    train_acc : 0.031686233131997046\n",
            "train loss : 0.00040598794373091683    train_acc : 0.040016899324337066\n",
            "train loss : 0.0005422860891795275    train_acc : 0.03168800901297298\n",
            "train loss : 0.0005995344813935585    train_acc : 0.030016900271473586\n",
            "train loss : 0.0003790859705446176    train_acc : 0.038349342346811456\n",
            "train loss : 0.0003113106412000782    train_acc : 0.0433537863159183\n",
            "train loss : 0.0004108056998529597    train_acc : 0.03502312201936849\n",
            "train loss : 0.0005005972918179978    train_acc : 0.036685345665076995\n",
            "train loss : 0.0004093612870818407    train_acc : 0.03668623218435471\n",
            "train loss : 0.0006349962852076025    train_acc : 0.03001956599049832\n",
            "train loss : 0.0005977427631961863    train_acc : 0.028349343768528268\n",
            "train loss : 0.0004156952050352281    train_acc : 0.03668178631667655\n",
            "train loss : 0.0005586628662187271    train_acc : 0.03501956361936889\n",
            "train loss : 0.00044510681812215587    train_acc : 0.038352010433930335\n",
            "train loss : 0.0005617966859572953    train_acc : 0.03168712107223143\n",
            "train loss : 0.00046056637396330486    train_acc : 0.03335023313123852\n",
            "train loss : 0.0005315327651194991    train_acc : 0.030017786791003327\n",
            "train loss : 0.0004009736264550364    train_acc : 0.03834934281962187\n",
            "train loss : 0.0005509887447719999    train_acc : 0.0366871196495038\n",
            "train loss : 0.0006004412987131949    train_acc : 0.035019566463813066\n",
            "train loss : 0.0005219511265234576    train_acc : 0.03501867710211404\n",
            "train loss : 0.00042594454723329946    train_acc : 0.03668534329445446\n",
            "train loss : 0.0007491809261218057    train_acc : 0.030019565516423707\n",
            "train loss : 0.0004678223028253118    train_acc : 0.03834934376827543\n",
            "train loss : 0.00043699540315704716    train_acc : 0.036687119650009745\n",
            "train loss : 0.0004989178519652612    train_acc : 0.03668623313048001\n",
            "train loss : 0.0005057850027900911    train_acc : 0.03335289932433626\n",
            "train loss : 0.0005775356821772693    train_acc : 0.028351121546306313\n",
            "train loss : 0.0005623260956092706    train_acc : 0.0316817872648247\n",
            "train loss : 0.0004559185430643868    train_acc : 0.03668356361987458\n",
            "train loss : 0.0005091064149181174    train_acc : 0.03501956456726393\n",
            "train loss : 0.0005599835519247077    train_acc : 0.03501867710110254\n",
            "train loss : 0.0003972794203760493    train_acc : 0.03835200996112059\n",
            "train loss : 0.00047973782992833465    train_acc : 0.04002045440531259\n",
            "train loss : 0.00045193814167010244    train_acc : 0.03668801090901616\n",
            "train loss : 0.0005723568682295939    train_acc : 0.035019566939151474\n",
            "train loss : 0.0005422440387346704    train_acc : 0.03335201043570088\n",
            "train loss : 0.000338907602480652    train_acc : 0.038351121072232375\n",
            "train loss : 0.0005339846269623712    train_acc : 0.03168712059790519\n",
            "train loss : 0.00036982020617889495    train_acc : 0.03835023313098555\n",
            "train loss : 0.0006303216474367849    train_acc : 0.031687120124336526\n",
            "train loss : 0.0006905043615477411    train_acc : 0.026683566464066315\n",
            "train loss : 0.000733052468008125    train_acc : 0.026680897902114167\n",
            "train loss : 0.0004695255145897899    train_acc : 0.035014229812214456\n",
            "train loss : 0.0006176198746673516    train_acc : 0.031685340922566516\n",
            "train loss : 0.0005521778088265478    train_acc : 0.03501689884849203\n",
            "train loss : 0.0004750020347076762    train_acc : 0.033352009012719196\n",
            "train loss : 0.0005732476301745329    train_acc : 0.03168445440480678\n",
            "train loss : 0.0006251359780411061    train_acc : 0.03168356504234923\n",
            "train loss : 0.000718447114351049    train_acc : 0.026683564568022587\n",
            "train loss : 0.0005687284324144378    train_acc : 0.035014231234436276\n",
            "train loss : 0.000538457733146539    train_acc : 0.0333520075899917\n",
            "train loss : 0.00044511190483398024    train_acc : 0.036684454404048\n",
            "train loss : 0.00030856710238684243    train_acc : 0.041686231709015496\n",
            "train loss : 0.000509886134367855    train_acc : 0.03835556599024481\n",
            "train loss : 0.00047584681414959256    train_acc : 0.035020456301861465\n",
            "train loss : 0.0006124730868270842    train_acc : 0.02835201091002766\n",
            "train loss : 0.0004310974309689803    train_acc : 0.033348454405818684\n",
            "train loss : 0.0004740218040058267    train_acc : 0.033351119175683105\n",
            "train loss : 0.0005293316131017849    train_acc : 0.03168445393022703\n",
            "train loss : 0.0005325906979500019    train_acc : 0.03168356504209612\n",
            "train loss : 0.0006550495589313221    train_acc : 0.028350231234689115\n",
            "train loss : 0.00041621602083654073    train_acc : 0.040015120123325164\n",
            "train loss : 0.00041035365886956006    train_acc : 0.03502134139739911\n",
            "train loss : 0.0003329650431305963    train_acc : 0.03835201138207861\n",
            "train loss : 0.00041559490287936695    train_acc : 0.03502045440607044\n",
            "train loss : 0.00042580056271407146    train_acc : 0.035018677575683235\n",
            "train loss : 0.00042816987826072613    train_acc : 0.03668534329470703\n",
            "train loss : 0.0005016926923921037    train_acc : 0.03335289884975718\n",
            "train loss : 0.0006206929208967172    train_acc : 0.03001778821271987\n",
            "train loss : 0.0003651334285482901    train_acc : 0.04001600948704678\n",
            "train loss : 0.0005988155340321724    train_acc : 0.03168800853839309\n",
            "train loss : 0.0007420816566962755    train_acc : 0.03001690027122048\n",
            "train loss : 0.0005537969609656026    train_acc : 0.03501600901347798\n",
            "train loss : 0.0005741370255544747    train_acc : 0.03168534187147385\n",
            "train loss : 0.0004317642027820372    train_acc : 0.038350232182331453\n",
            "train loss : 0.0006003908094180788    train_acc : 0.03335378679049724\n",
            "train loss : 0.000574663063249398    train_acc : 0.031684455352954936\n",
            "train loss : 0.0007835815190105235    train_acc : 0.02501689837618824\n",
            "train loss : 0.0007431572061029449    train_acc : 0.028346675679133966\n",
            "train loss : 0.0005900484119723239    train_acc : 0.03168178489369554\n",
            "train loss : 0.00038070565445131037    train_acc : 0.04168356361860997\n",
            "train loss : 0.0005450872160910655    train_acc : 0.03668889790059659\n",
            "train loss : 0.0005759353334415181    train_acc : 0.035019567412213654\n",
            "train loss : 0.0005611515233195999    train_acc : 0.031685343769286514\n",
            "train loss : 0.0005063204307656435    train_acc : 0.03335023218334362\n",
            "train loss : 0.0003570835955057833    train_acc : 0.04001778679049778\n",
            "train loss : 0.0002593327798402105    train_acc : 0.04835467615295493\n",
            "train loss : 0.000700629453088864    train_acc : 0.02502578916061491\n",
            "train loss : 0.00047237957673614613    train_acc : 0.03668001375421899\n",
            "train loss : 0.0007090800989212319    train_acc : 0.028352896007335586\n",
            "train loss : 0.0005111589555734286    train_acc : 0.03168178821120391\n",
            "train loss : 0.0004577762229985552    train_acc : 0.04001689695371264\n",
            "train loss : 0.0005172932055973342    train_acc : 0.035021342345041985\n",
            "train loss : 0.0005570132808636232    train_acc : 0.03168534471591736\n",
            "train loss : 0.00037148027840067284    train_acc : 0.04335023218384849\n",
            "train loss : 0.0004898388991314719    train_acc : 0.03835645345716472\n",
            "train loss : 0.0005480316840176878    train_acc : 0.03335379010851049\n",
            "train loss : 0.00048513855762283357    train_acc : 0.033351122021391205\n",
            "train loss : 0.00062009503918874    train_acc : 0.03335112059841141\n",
            "train loss : 0.0005286279827249208    train_acc : 0.03335112059765249\n",
            "train loss : 0.00044268296260356333    train_acc : 0.03835112059765208\n",
            "train loss : 0.0005276378309784927    train_acc : 0.03335378726431875\n",
            "train loss : 0.0005644586020743083    train_acc : 0.03501778868654097\n",
            "train loss : 0.0005799420052811324    train_acc : 0.030018676153966153\n",
            "train loss : 0.0004414965887797697    train_acc : 0.04001600996061545\n",
            "train loss : 0.00046927353806919166    train_acc : 0.03502134187197899\n",
            "train loss : 0.0004700652473294344    train_acc : 0.03835201138233172\n",
            "train loss : 0.0005752720001916421    train_acc : 0.031687121072737245\n",
            "train loss : 0.0006102455604110071    train_acc : 0.028350233131238793\n",
            "train loss : 0.0005147993793989835    train_acc : 0.028348453457669995\n",
            "train loss : 0.00039917421024274327    train_acc : 0.04001511917517742\n",
            "train loss : 0.0005112735122663775    train_acc : 0.03335467473022676\n",
            "train loss : 0.0005691800990470368    train_acc : 0.03501778915985612\n",
            "train loss : 0.00032382663909481726    train_acc : 0.041685342820885256\n",
            "train loss : 0.000521959206564288    train_acc : 0.03502223218283781\n",
            "train loss : 0.000379931355233247    train_acc : 0.036685345190497515\n",
            "train loss : 0.0005266147674055711    train_acc : 0.03335289885076827\n",
            "train loss : 0.0003110446971162654    train_acc : 0.04501778821272041\n",
            "train loss : 0.0006550356694561378    train_acc : 0.023357342820380118\n",
            "train loss : 0.0004927231367401813    train_acc : 0.03667912391617087\n",
            "train loss : 0.0005171954321064405    train_acc : 0.03501956219942196\n",
            "train loss : 0.000511086784982003    train_acc : 0.031685343766506356\n",
            "train loss : 0.00047478798590934717    train_acc : 0.036683565516675475\n",
            "train loss : 0.0003935082950780807    train_acc : 0.03501956456827556\n",
            "train loss : 0.0005525351833872578    train_acc : 0.03168534376776975\n",
            "train loss : 0.000668886756964343    train_acc : 0.030016898850009476\n",
            "train loss : 0.00042790492558800793    train_acc : 0.03834934234605334\n",
            "train loss : 0.0006157929437435167    train_acc : 0.03335378631591789\n",
            "train loss : 0.00041539593668710265    train_acc : 0.03501778868603515\n",
            "train loss : 0.0004396886249337299    train_acc : 0.03835200948729922\n",
            "train loss : 0.0005610656341245415    train_acc : 0.033353787738393224\n",
            "train loss : 0.00045294247052936185    train_acc : 0.03501778868679381\n",
            "train loss : 0.00045955191019896334    train_acc : 0.03668534282063295\n",
            "train loss : 0.0004613400459817448    train_acc : 0.03668623218283767\n",
            "train loss : 0.0004173739751688921    train_acc : 0.03501956599049751\n",
            "train loss : 0.00042699471706816953    train_acc : 0.0400186771018616\n",
            "train loss : 0.0005204278241030093    train_acc : 0.03668800996112099\n",
            "train loss : 0.0004349736954376661    train_acc : 0.035019566938645934\n",
            "train loss : 0.00039032985838707845    train_acc : 0.04001867710236728\n",
            "train loss : 0.0004464978106290629    train_acc : 0.03835467662778793\n",
            "train loss : 0.0005728407220907991    train_acc : 0.033353789160868154\n",
            "train loss : 0.00028406304272105264    train_acc : 0.04501778868755246\n",
            "train loss : 0.0003764732293137026    train_acc : 0.04335734282063337\n",
            "train loss : 0.0005556918517013045    train_acc : 0.030023123916171006\n",
            "train loss : 0.0005025250104975838    train_acc : 0.033349345666088624\n",
            "train loss : 0.0005069021851427244    train_acc : 0.03501778631768858\n",
            "train loss : 0.0005420145800098105    train_acc : 0.03501867615270277\n",
            "train loss : 0.0005401965934652956    train_acc : 0.03668534329394811\n",
            "train loss : 0.0004415219305334419    train_acc : 0.03668623218309011\n",
            "train loss : 0.0006253419719949499    train_acc : 0.03001956599049765\n",
            "train loss : 0.0006327768166013726    train_acc : 0.033349343768528265\n",
            "train loss : 0.000686561337576927    train_acc : 0.030017786316676547\n",
            "train loss : 0.0006845986871038365    train_acc : 0.026682676152702228\n",
            "train loss : 0.0006393542499557132    train_acc : 0.030014230760614775\n",
            "train loss : 0.0005449722475799568    train_acc : 0.033349340923072325\n",
            "train loss : 0.0003723878897702425    train_acc : 0.0433511196484923\n",
            "train loss : 0.0004649218124409504    train_acc : 0.035023120597145865\n",
            "train loss : 0.0005980599115266937    train_acc : 0.03168534566431848\n",
            "train loss : 0.0004440245558521469    train_acc : 0.0383502321843543\n",
            "train loss : 0.0004826225756290065    train_acc : 0.03502045345716499\n",
            "train loss : 0.00044052178265882504    train_acc : 0.036685344241843824\n",
            "train loss : 0.0004675446424198269    train_acc : 0.03668623218359565\n",
            "train loss : 0.000602158676702357    train_acc : 0.030019565990497918\n",
            "train loss : 0.0004175940369347035    train_acc : 0.03834934376852827\n",
            "train loss : 0.00034922682652021935    train_acc : 0.04168711965000988\n",
            "train loss : 0.00041505689792320006    train_acc : 0.03668889979714667\n",
            "train loss : 0.0005433516661080915    train_acc : 0.03335290074655848\n",
            "train loss : 0.0007382914838239751    train_acc : 0.030017788213731498\n",
            "train loss : 0.000607963831446809    train_acc : 0.03168267615371399\n",
            "train loss : 0.00044589840382004963    train_acc : 0.03668356409394864\n",
            "train loss : 0.0005902079561181051    train_acc : 0.03168623123418344\n",
            "train loss : 0.000447847177072039    train_acc : 0.03835023265665823\n",
            "train loss : 0.0005414836765551481    train_acc : 0.03335378679075022\n",
            "train loss : 0.00044716569864279554    train_acc : 0.038351122019621736\n",
            "train loss : 0.0005163186453346863    train_acc : 0.03835378726507713\n",
            "train loss : 0.0008246739829202527    train_acc : 0.023353788686541376\n",
            "train loss : 0.0004921936586234104    train_acc : 0.03501245535396615\n",
            "train loss : 0.0007075174852792281    train_acc : 0.03168533997618878\n",
            "train loss : 0.0007581012781039654    train_acc : 0.025016898847987298\n",
            "train loss : 0.0006345190311715643    train_acc : 0.026680009012718928\n",
            "train loss : 0.0007375984021136626    train_acc : 0.03001422933814012\n",
            "train loss : 0.0006470836908885165    train_acc : 0.030016007588980344\n",
            "train loss : 0.0005454842333110846    train_acc : 0.03168267520404746\n",
            "train loss : 0.0005209246860124859    train_acc : 0.035016897426775495\n",
            "train loss : 0.0003416493627540895    train_acc : 0.038352009011960946\n",
            "train loss : 0.0005143253938406368    train_acc : 0.035020454404806375\n",
            "train loss : 0.0006588334067098855    train_acc : 0.03501867757568256\n",
            "train loss : 0.00039163845031650504    train_acc : 0.04001867662804036\n",
            "train loss : 0.0006934959603269852    train_acc : 0.03002134329420162\n",
            "train loss : 0.00043550414821375314    train_acc : 0.03668267804975691\n",
            "Looked at 12800/60000 samples\n",
            "train loss : 0.0005634023840320421    train_acc : 0.031686230761626535\n",
            "train loss : 0.0005412288768606765    train_acc : 0.03001689932307287\n",
            "train loss : 0.000664338249008577    train_acc : 0.026682675679638973\n",
            "train loss : 0.0005262289284483808    train_acc : 0.02668089742702914\n",
            "train loss : 0.0005921467036055891    train_acc : 0.026680896478627748\n",
            "train loss : 0.0004888844036467896    train_acc : 0.035014229811455265\n",
            "train loss : 0.000465208947475402    train_acc : 0.038352007589232774\n",
            "train loss : 0.000470558894824884    train_acc : 0.03502045440404759\n",
            "train loss : 0.0005470381631285094    train_acc : 0.03335201090901549\n",
            "train loss : 0.0005842240580693261    train_acc : 0.025017787739151472\n",
            "train loss : 0.0003999191491831025    train_acc : 0.03668000948679421\n",
            "train loss : 0.0003742319275521718    train_acc : 0.03668622933839296\n",
            "train loss : 0.0004812522115670089    train_acc : 0.03335289932231381\n",
            "train loss : 0.0003338683469233175    train_acc : 0.0450177882129719\n",
            "train loss : 0.000451770572070182    train_acc : 0.03502400948704692\n",
            "train loss : 0.000453920315185231    train_acc : 0.035018679471726426\n",
            "train loss : 0.0005094259109234699    train_acc : 0.03168534329571825\n",
            "train loss : 0.0006935131310953636    train_acc : 0.02501689884975772\n",
            "train loss : 0.0005433470795821897    train_acc : 0.03334667567938654\n",
            "train loss : 0.0004332782062313436    train_acc : 0.03501778489369567\n",
            "train loss : 0.0005109774653769822    train_acc : 0.03168534281860997\n",
            "train loss : 0.0004082527726271236    train_acc : 0.040016898849503255\n",
            "train loss : 0.0005170003682756094    train_acc : 0.0333546756793864\n",
            "train loss : 0.0006126072510671818    train_acc : 0.03501778916036234\n",
            "train loss : 0.00037646293144780223    train_acc : 0.0433520094875522\n",
            "train loss : 0.0004243167241387969    train_acc : 0.03668978773839336\n",
            "train loss : 0.00045540138414170865    train_acc : 0.03501956788679381\n",
            "train loss : 0.000564546851075123    train_acc : 0.031685343769539624\n",
            "train loss : 0.0005816145427850917    train_acc : 0.03335023218334376\n",
            "train loss : 0.0005765357930759763    train_acc : 0.03335112012383112\n",
            "train loss : 0.0003688352160622967    train_acc : 0.04001778726406604\n",
            "train loss : 0.0005128354717282704    train_acc : 0.036688009486540836\n",
            "train loss : 0.0005277062400653092    train_acc : 0.03001956693839282\n",
            "train loss : 0.0004898129285676996    train_acc : 0.03168267710236714\n",
            "train loss : 0.0003944082481798227    train_acc : 0.041683564094454596\n",
            "train loss : 0.0005096643152990943    train_acc : 0.033355564567517045\n",
            "train loss : 0.0004967719951726679    train_acc : 0.03335112296776934\n",
            "train loss : 0.0006337346538246715    train_acc : 0.033351120598916144\n",
            "train loss : 0.0006688809173321375    train_acc : 0.03001778726431942\n",
            "train loss : 0.0005564469642521018    train_acc : 0.03501600948654097\n",
            "train loss : 0.0005987446349751727    train_acc : 0.02835200853839282\n",
            "train loss : 0.0003684229935986144    train_acc : 0.040015121071220475\n",
            "train loss : 0.0005559453046208372    train_acc : 0.03335467473123799\n",
            "train loss : 0.0004752779393024384    train_acc : 0.03668445582652333\n",
            "train loss : 0.0006480653076377777    train_acc : 0.02501956504310748\n",
            "train loss : 0.00042001672527466915    train_acc : 0.03668001043468966\n",
            "train loss : 0.00045371445691819336    train_acc : 0.033352896005565166\n",
            "train loss : 0.0004307867388867073    train_acc : 0.04001778821120297\n",
            "train loss : 0.00046698263175227604    train_acc : 0.03335467615371264\n",
            "train loss : 0.0003323475732320517    train_acc : 0.04001778916061531\n",
            "train loss : 0.00044147316192064893    train_acc : 0.038354676154218996\n",
            "train loss : 0.0005570701182872692    train_acc : 0.031687122493948915\n",
            "train loss : 0.0005871217754904954    train_acc : 0.031683566465330104\n",
            "train loss : 0.0006003531223565962    train_acc : 0.03168356456878151\n",
            "train loss : 0.0004553239267352438    train_acc : 0.03668356456777001\n",
            "train loss : 0.00038504835980316174    train_acc : 0.03335289790110281\n",
            "train loss : 0.00043565152611741586    train_acc : 0.03835112154554725\n",
            "train loss : 0.00033008756700370794    train_acc : 0.04168712059815762\n",
            "train loss : 0.0006294271453352146    train_acc : 0.028355566464319016\n",
            "train loss : 0.000493670401101861    train_acc : 0.03501512296878097\n",
            "train loss : 0.0008078369908317596    train_acc : 0.02835200806558335\n",
            "train loss : 0.0006266262453615817    train_acc : 0.03168178773763498\n",
            "train loss : 0.000623110359314909    train_acc : 0.036683563620126744\n",
            "train loss : 0.0005748128909139653    train_acc : 0.03168623123393074\n",
            "train loss : 0.0005473288141896073    train_acc : 0.030016899323324766\n",
            "train loss : 0.0004390007862643777    train_acc : 0.03668267567963911\n",
            "train loss : 0.0005824352013592824    train_acc : 0.03335289742702914\n",
            "train loss : 0.0007028780505237328    train_acc : 0.03001778821196108\n",
            "train loss : 0.00034664565203588153    train_acc : 0.041682676153713044\n",
            "train loss : 0.00044665151433880067    train_acc : 0.033355564093948645\n",
            "train loss : 0.0005555277369477842    train_acc : 0.03335112296751677\n",
            "train loss : 0.0006328790095277393    train_acc : 0.02668445393224934\n",
            "train loss : 0.00047277847760024747    train_acc : 0.03501423170876387\n",
            "train loss : 0.0005207838982467287    train_acc : 0.03501867425691134\n",
            "train loss : 0.0005512656308476197    train_acc : 0.03335200995960368\n",
            "train loss : 0.0007005317275543883    train_acc : 0.02668445440531179\n",
            "train loss : 0.0003739490818058024    train_acc : 0.04168089837568283\n",
            "train loss : 0.0006156988088136819    train_acc : 0.03002222981246703\n",
            "train loss : 0.0006006928670217645    train_acc : 0.026682678522566648\n",
            "train loss : 0.00038508287919297805    train_acc : 0.0400142307618787\n",
            "train loss : 0.0002858299452032128    train_acc : 0.043354674256406336\n",
            "train loss : 0.00037026635585562205    train_acc : 0.036689789159603414\n",
            "train loss : 0.00044090102636431763    train_acc : 0.038352901220885124\n",
            "train loss : 0.0005711773265760402    train_acc : 0.03502045488065114\n",
            "train loss : 0.0006222801451804564    train_acc : 0.02835201090926968\n",
            "train loss : 0.00033288517406489704    train_acc : 0.04001512107248494\n",
            "train loss : 0.0004614336613486126    train_acc : 0.036688008064571995\n",
            "train loss : 0.00026971383324324663    train_acc : 0.04335290027096777\n",
            "train loss : 0.00029606243919494976    train_acc : 0.04335645488014452\n",
            "train loss : 0.0003784723950542406    train_acc : 0.038356456775936076\n",
            "train loss : 0.00037035181405136944    train_acc : 0.0383537901102805\n",
            "train loss : 0.0006816332967243301    train_acc : 0.033353788688058814\n",
            "train loss : 0.0004982926339801768    train_acc : 0.038351122020633635\n",
            "train loss : 0.00038357861719613055    train_acc : 0.04002045393174433\n",
            "train loss : 0.0004644623559733608    train_acc : 0.0366880109087636\n",
            "train loss : 0.0006933358632809332    train_acc : 0.03001956693915134\n",
            "train loss : 0.0006140080847911458    train_acc : 0.03168267710236755\n",
            "train loss : 0.0007157974671129954    train_acc : 0.026683564094454597\n",
            "train loss : 0.00048299780173994726    train_acc : 0.03501423123418371\n",
            "train loss : 0.00040725388076932645    train_acc : 0.03501867425665823\n",
            "train loss : 0.00041001522020735757    train_acc : 0.036685343292936885\n",
            "train loss : 0.0005291630081784791    train_acc : 0.0300195655164229\n",
            "train loss : 0.0005924797656404947    train_acc : 0.030016010434942095\n",
            "train loss : 0.0006351675720533937    train_acc : 0.030016008538898634\n",
            "train loss : 0.000614909818762224    train_acc : 0.02834934187122075\n",
            "train loss : 0.0003206700793854362    train_acc : 0.043348452982331316\n",
            "train loss : 0.00043474699500994956    train_acc : 0.033356452508257244\n",
            "train loss : 0.0005744090984420977    train_acc : 0.03335112344133774\n",
            "train loss : 0.0005493770276869751    train_acc : 0.03168445393250204\n",
            "train loss : 0.0005111320818919799    train_acc : 0.03168356504209733\n",
            "train loss : 0.0005037272571990468    train_acc : 0.03668356456802246\n",
            "train loss : 0.00034436588735577633    train_acc : 0.04001956456776961\n",
            "train loss : 0.000499181032208608    train_acc : 0.03335467710110281\n",
            "train loss : 0.0006064813254038641    train_acc : 0.031684455827787256\n",
            "train loss : 0.00042696453947055395    train_acc : 0.03668356504310815\n",
            "train loss : 0.0006093588916874897    train_acc : 0.03168623123468966\n",
            "train loss : 0.0006353914688665823    train_acc : 0.030016899323325165\n",
            "train loss : 0.0005246650091780895    train_acc : 0.03668267567963911\n",
            "train loss : 0.00044471748409864503    train_acc : 0.038352897427029145\n",
            "train loss : 0.0004270896950178051    train_acc : 0.03835378821196108\n",
            "train loss : 0.0004594200571716527    train_acc : 0.036687122020379714\n",
            "train loss : 0.0004580879780303288    train_acc : 0.03335289979841087\n",
            "train loss : 0.0003683039487506364    train_acc : 0.04168445487989249\n",
            "train loss : 0.0006037468310960719    train_acc : 0.028355565042602608\n",
            "train loss : 0.00034476598710996676    train_acc : 0.040015122968022725\n",
            "train loss : 0.00060482127550237    train_acc : 0.031688008065582945\n",
            "train loss : 0.0005479457606372991    train_acc : 0.03335023360430164\n",
            "train loss : 0.0004371179582081635    train_acc : 0.03668445345792229\n",
            "train loss : 0.0006233706672160573    train_acc : 0.03335289837517756\n",
            "train loss : 0.0003255260284529352    train_acc : 0.04335112154580009\n",
            "train loss : 0.0006993165149845434    train_acc : 0.026689787264824426\n",
            "train loss : 0.0003500341919016604    train_acc : 0.041680901219874576\n",
            "train loss : 0.00048607103345377987    train_acc : 0.040022229813983935\n",
            "train loss : 0.0005279521598408889    train_acc : 0.035021345189234124\n",
            "train loss : 0.0008002687952494087    train_acc : 0.03501867805076759\n",
            "train loss : 0.00039504293703829317    train_acc : 0.038352009961627076\n",
            "train loss : 0.0004357566744703918    train_acc : 0.043353787738646204\n",
            "train loss : 0.000447875524643295    train_acc : 0.03835645535346061\n",
            "train loss : 0.0006914271069977459    train_acc : 0.028353790109521843\n",
            "train loss : 0.0005274184933319841    train_acc : 0.035015122021391745\n",
            "train loss : 0.00033795826002709146    train_acc : 0.038352008065078075\n",
            "train loss : 0.0004694842656048921    train_acc : 0.04168712107096804\n",
            "train loss : 0.00025198634907190495    train_acc : 0.04335556646457118\n",
            "train loss : 0.0004997200212290265    train_acc : 0.038356456302114435\n",
            "train loss : 0.0002951711308395285    train_acc : 0.041687123443361125\n",
            "train loss : 0.0004261870431842505    train_acc : 0.03835556646583646\n",
            "train loss : 0.00040686563593800556    train_acc : 0.03835378963544845\n",
            "train loss : 0.0003482515358180081    train_acc : 0.04002045535447224\n",
            "train loss : 0.000417119874675711    train_acc : 0.03835467757618905\n",
            "train loss : 0.00047031358433517864    train_acc : 0.035020455828040635\n",
            "train loss : 0.0005152829027999917    train_acc : 0.03335201090977495\n",
            "train loss : 0.000570806789797509    train_acc : 0.03501778773915188\n",
            "train loss : 0.00027011650558492976    train_acc : 0.04668534282012755\n",
            "train loss : 0.0003144435819930828    train_acc : 0.04002489884950407\n",
            "train loss : 0.0003506335731472941    train_acc : 0.041688013279386406\n",
            "train loss : 0.00033821568982421695    train_acc : 0.041688900273749004\n",
            "train loss : 0.000634486526415595    train_acc : 0.028355567413479332\n",
            "train loss : 0.0005021763775349543    train_acc : 0.03668178963595386\n",
            "train loss : 0.0005396609980759939    train_acc : 0.03335289695447251\n",
            "train loss : 0.0003740521433872063    train_acc : 0.04001778821170905\n",
            "train loss : 0.000513608695780591    train_acc : 0.03502134282037958\n",
            "train loss : 0.00020079216715585674    train_acc : 0.04668534471617087\n",
            "train loss : 0.000463480813775364    train_acc : 0.040024898850515286\n",
            "train loss : 0.00035540939705291    train_acc : 0.04335467994605361\n",
            "train loss : 0.0001785981585255555    train_acc : 0.04668978916263789\n",
            "train loss : 0.00024188403701409612    train_acc : 0.045024901220886746\n",
            "train loss : 0.00032354736775512495    train_acc : 0.04002401328065114\n",
            "train loss : 0.00031592169520298826    train_acc : 0.04668801280708302\n",
            "train loss : 0.00024472024556857765    train_acc : 0.04669156694016378\n",
            "train loss : 0.00046200344481212225    train_acc : 0.033358235502368085\n",
            "train loss : 0.0005218709987508888    train_acc : 0.031684457725601266\n",
            "train loss : 0.0005714674486309745    train_acc : 0.033350231710786986\n",
            "train loss : 0.0003962173345191527    train_acc : 0.04001778679024575\n",
            "train loss : 0.0002530754859862064    train_acc : 0.04002134281962146\n",
            "train loss : 0.00046771904316243154    train_acc : 0.03168801138283713\n",
            "train loss : 0.00040654522652354083    train_acc : 0.040016900272737514\n",
            "train loss : 0.00046384705534232944    train_acc : 0.03835467568014546\n",
            "train loss : 0.00029909207224014414    train_acc : 0.04502045582702941\n",
            "train loss : 0.0004878024715518907    train_acc : 0.03669067757644108\n",
            "train loss : 0.0003803810427166799    train_acc : 0.04168623502804077\n",
            "train loss : 0.0003405564223898558    train_acc : 0.041688899325348285\n",
            "train loss : 0.0003177763808141906    train_acc : 0.038355567412973524\n",
            "train loss : 0.0004622217624340254    train_acc : 0.03335378963595358\n",
            "train loss : 0.0002485745682509186    train_acc : 0.04501778868780584\n",
            "train loss : 0.00022595258996402907    train_acc : 0.04335734282063349\n",
            "train loss : 0.00043720378416085517    train_acc : 0.03668979058283767\n",
            "train loss : 0.00045222730834715795    train_acc : 0.04001956788831085\n",
            "train loss : 0.0003954177257294095    train_acc : 0.03835467710287376\n",
            "train loss : 0.00048283847200377117    train_acc : 0.0350204558277882\n",
            "train loss : 0.00041908272696615593    train_acc : 0.03668534424310815\n",
            "train loss : 0.0003160871836799679    train_acc : 0.04001956551692966\n",
            "train loss : 0.00039230054419693724    train_acc : 0.040021343768275694\n",
            "train loss : 0.00041167255118582753    train_acc : 0.03835467805000974\n",
            "train loss : 0.0003181740671575074    train_acc : 0.043353789161626674\n",
            "train loss : 0.0003780374550835387    train_acc : 0.04335645535421954\n",
            "train loss : 0.000598934886170706    train_acc : 0.038356456776188916\n",
            "train loss : 0.00043206311734375066    train_acc : 0.043353790110280635\n",
            "train loss : 0.0003018439552385111    train_acc : 0.04335645535472548\n",
            "train loss : 0.0002233918063238567    train_acc : 0.04168979010952252\n",
            "train loss : 0.00035793243702530303    train_acc : 0.04335556788805841\n",
            "train loss : 0.0005321434373794995    train_acc : 0.0350231229695403\n",
            "train loss : 0.00044143838518116946    train_acc : 0.03668534566558375\n",
            "train loss : 0.0003487037374858499    train_acc : 0.04335289885102165\n",
            "train loss : 0.0004286562054291982    train_acc : 0.04168978821272055\n",
            "train loss : 0.0003326266815741293    train_acc : 0.040022234553713454\n",
            "train loss : 0.0004819064315059704    train_acc : 0.03668801185842865\n",
            "train loss : 0.0004357514078869236    train_acc : 0.04001956693965783\n",
            "train loss : 0.0004289607269125755    train_acc : 0.036688010435701156\n",
            "train loss : 0.00047149285528726354    train_acc : 0.03835290027223237\n",
            "train loss : 0.0004982493189962899    train_acc : 0.03502045488014519\n",
            "train loss : 0.000441106581257078    train_acc : 0.03668534424260274\n",
            "train loss : 0.0002489785207237489    train_acc : 0.04335289885026272\n",
            "train loss : 0.0003307110203559094    train_acc : 0.04168978821272014\n",
            "train loss : 0.00036152478568465205    train_acc : 0.04335556788704679\n",
            "train loss : 0.00045481034531335147    train_acc : 0.03668978963620643\n",
            "train loss : 0.000364374175116296    train_acc : 0.03335290122113931\n",
            "train loss : 0.0003764113802668491    train_acc : 0.03668445488065127\n",
            "train loss : 0.00027001581621270487    train_acc : 0.04335289837593635\n",
            "train loss : 0.00041000709895045015    train_acc : 0.03668978821246717\n",
            "train loss : 0.0004368096916726629    train_acc : 0.03835290122037999\n",
            "train loss : 0.00036699007194989795    train_acc : 0.04168712154731753\n",
            "train loss : 0.00023966412772200772    train_acc : 0.046688899798158566\n",
            "train loss : 0.00023202608098469557    train_acc : 0.045024900746559016\n",
            "train loss : 0.0003731787284748813    train_acc : 0.0383573466137315\n",
            "train loss : 0.00045341249784260524    train_acc : 0.035020457251527325\n",
            "train loss : 0.00026674612361314787    train_acc : 0.04168534424386749\n",
            "train loss : 0.00033597610020106694    train_acc : 0.04335556551693007\n",
            "train loss : 0.000235649259681826    train_acc : 0.046689789634942365\n",
            "train loss : 0.0005405446223745321    train_acc : 0.028358234554471968\n",
            "train loss : 0.0005662636068308263    train_acc : 0.031681791058429054\n",
            "train loss : 0.0005375726167947369    train_acc : 0.035016896955231164\n",
            "train loss : 0.00040078830629690967    train_acc : 0.03835200901170945\n",
            "train loss : 0.00046676610666863174    train_acc : 0.035020454404806244\n",
            "train loss : 0.00023679555370944528    train_acc : 0.04668534424234923\n",
            "train loss : 0.000376478728299218    train_acc : 0.04169156551692925\n",
            "train loss : 0.0006561597272537257    train_acc : 0.03502223550160903\n",
            "train loss : 0.0005140712271841252    train_acc : 0.03668534519226753\n",
            "train loss : 0.0004546848832453999    train_acc : 0.03835289885076921\n",
            "train loss : 0.0003456833949565297    train_acc : 0.03835378821272041\n",
            "train loss : 0.00036946556107154846    train_acc : 0.043353788687046785\n",
            "train loss : 0.0003406282616781444    train_acc : 0.03835645535396642\n",
            "train loss : 0.0004136274590149523    train_acc : 0.03835379010952211\n",
            "train loss : 0.0004716168885937826    train_acc : 0.036687122021391745\n",
            "train loss : 0.0003496249649813711    train_acc : 0.04168623313174474\n",
            "train loss : 0.00044088741091271986    train_acc : 0.04002223265767026\n",
            "train loss : 0.0003169441815479458    train_acc : 0.04502134519075076\n",
            "train loss : 0.0004695087573049473    train_acc : 0.0366906780507684\n",
            "train loss : 0.0002830499982781472    train_acc : 0.043352901694960415\n",
            "train loss : 0.00038871786724598923    train_acc : 0.04002312154757064\n",
            "train loss : 0.0003715924772778144    train_acc : 0.03502134566482538\n",
            "train loss : 0.00036501510261711327    train_acc : 0.03335201138435457\n",
            "train loss : 0.0002456265588512379    train_acc : 0.04168445440607165\n",
            "train loss : 0.0004538902896192291    train_acc : 0.03668889837568323\n",
            "train loss : 0.0005536854748862344    train_acc : 0.0316862340791337\n",
            "train loss : 0.00032111807825750444    train_acc : 0.04335023265817554\n",
            "train loss : 0.00042559631561789297    train_acc : 0.03835645345741769\n",
            "train loss : 0.0004566328468741075    train_acc : 0.03835379010851063\n",
            "train loss : 0.00040825605481162715    train_acc : 0.038353788688057874\n",
            "train loss : 0.0005421848106389008    train_acc : 0.0383537886873003\n",
            "train loss : 0.00047552739307657905    train_acc : 0.0383537886872999\n",
            "train loss : 0.000239361716136537    train_acc : 0.04835378868729989\n",
            "train loss : 0.0003674028976190489    train_acc : 0.043359122020633224\n",
            "train loss : 0.00036040496819090466    train_acc : 0.038356458198411006\n",
            "train loss : 0.0005046615152994414    train_acc : 0.035020456777705815\n",
            "train loss : 0.00026549825081027557    train_acc : 0.04168534424361477\n",
            "train loss : 0.0003475111427784432    train_acc : 0.040022232183596594\n",
            "train loss : 0.000541588283205185    train_acc : 0.031688011857164584\n",
            "train loss : 0.00038949160078717554    train_acc : 0.043350233606323826\n",
            "train loss : 0.0003039809412866145    train_acc : 0.041689786791256706\n",
            "train loss : 0.0004652266805620781    train_acc : 0.035022234552955334\n",
            "train loss : 0.0006479678220274847    train_acc : 0.03001867852509491\n",
            "train loss : 0.0004961097112055208    train_acc : 0.03501600996188005\n",
            "train loss : 0.0004547372290224899    train_acc : 0.03668534187197967\n",
            "train loss : 0.0003851286323774513    train_acc : 0.041686232182331716\n",
            "train loss : 0.0003410157326460896    train_acc : 0.04002223265716391\n",
            "train loss : 0.0004411756609155656    train_acc : 0.03502134519075049\n",
            "train loss : 0.0002489503764901836    train_acc : 0.04168534471743507\n",
            "train loss : 0.0003958595178680311    train_acc : 0.03668889885051597\n",
            "train loss : 0.00029812969909149574    train_acc : 0.04501956741272027\n",
            "train loss : 0.00035466460502489957    train_acc : 0.04002401043595345\n",
            "train loss : 0.000382397469987426    train_acc : 0.03835467947223251\n",
            "train loss : 0.0001981332640632576    train_acc : 0.046687122495718526\n",
            "train loss : 0.0006129444161913545    train_acc : 0.03169156646533105\n",
            "train loss : 0.0002784234206157187    train_acc : 0.04168356883544817\n",
            "train loss : 0.0005207013184187294    train_acc : 0.03502223123671224\n",
            "train loss : 0.0006427684758024405    train_acc : 0.03501867852332625\n",
            "train loss : 0.0003990009900905777    train_acc : 0.041685343295212444\n",
            "train loss : 0.0003709823012604702    train_acc : 0.04002223218309078\n",
            "train loss : 0.00022045118173600592    train_acc : 0.04335467852383098\n",
            "train loss : 0.0005098093209932149    train_acc : 0.033356455828546044\n",
            "train loss : 0.0003641193129570362    train_acc : 0.04168445677644189\n",
            "train loss : 0.0004099178285078446    train_acc : 0.0383555650436141\n",
            "train loss : 0.00033835707600756094    train_acc : 0.040020456301356595\n",
            "train loss : 0.00033698020584581084    train_acc : 0.045021344243360725\n",
            "train loss : 0.0003906672665183131    train_acc : 0.03669067805026313\n",
            "train loss : 0.0005842020768837773    train_acc : 0.036686235028293476\n",
            "train loss : 0.0005360400290308485    train_acc : 0.040019565992015095\n",
            "train loss : 0.0003946348272753301    train_acc : 0.040021343768529075\n",
            "train loss : 0.00019155112500948335    train_acc : 0.04835467805000988\n",
            "train loss : 0.0004330000462404673    train_acc : 0.036692455828293336\n",
            "train loss : 0.00039933991544417654    train_acc : 0.03835290264310842\n",
            "train loss : 0.00030020903943439576    train_acc : 0.04168712154807633\n",
            "train loss : 0.0002573478117593048    train_acc : 0.04335556646482564\n",
            "train loss : 0.0003075192000015581    train_acc : 0.04002312296878124\n",
            "train loss : 0.0002285090387993677    train_acc : 0.04668801233225001\n",
            "train loss : 0.00046739307230319623    train_acc : 0.03669156693991053\n",
            "train loss : 0.0002998463489065467    train_acc : 0.04168623550236795\n",
            "train loss : 0.00026718478479265904    train_acc : 0.043355565992267926\n",
            "train loss : 0.00029611703926233864    train_acc : 0.04168978963519588\n",
            "train loss : 0.0004700621293236092    train_acc : 0.03668890122113877\n",
            "train loss : 0.00028713770974086387    train_acc : 0.04168623408065127\n",
            "train loss : 0.0004715431649328253    train_acc : 0.03168889932484301\n",
            "train loss : 0.00035290356259445833    train_acc : 0.03501690074630658\n",
            "train loss : 0.0005678617076650244    train_acc : 0.03668534234706469\n",
            "train loss : 0.00028052937630105413    train_acc : 0.0416862321825851\n",
            "train loss : 0.00025010930703292045    train_acc : 0.04335556599049738\n",
            "train loss : 0.00046545385163204516    train_acc : 0.03502312296852826\n",
            "train loss : 0.0003901604658701217    train_acc : 0.04001867899891655\n",
            "train loss : 0.0003477667429100852    train_acc : 0.043354676628799424\n",
            "train loss : 0.0003284980633342558    train_acc : 0.043356455827535355\n",
            "train loss : 0.0006753739446946507    train_acc : 0.02502312344310802\n",
            "train loss : 0.0003575263307746542    train_acc : 0.04168001233250299\n",
            "train loss : 0.0002324430559945772    train_acc : 0.04502222933991067\n",
            "train loss : 0.00015642120829766976    train_acc : 0.048357345188981285\n",
            "train loss : 0.00048068409959152827    train_acc : 0.038359123917434124\n",
            "train loss : 0.0003985916627555406    train_acc : 0.0366871248660893\n",
            "train loss : 0.0005200934492245308    train_acc : 0.03501956646659525\n",
            "train loss : 0.00042960522079905905    train_acc : 0.04001867710211552\n",
            "train loss : 0.0003854778156148298    train_acc : 0.04002134329445446\n",
            "train loss : 0.00039781005365294706    train_acc : 0.04002134471642371\n",
            "train loss : 0.0004771788961155289    train_acc : 0.03835467805051543\n",
            "train loss : 0.0003238775804332343    train_acc : 0.041687122494960274\n",
            "train loss : 0.0004400169933358665    train_acc : 0.035022233131997316\n",
            "train loss : 0.00042462616793762414    train_acc : 0.04001867852433706\n",
            "train loss : 0.0004289371504764157    train_acc : 0.03835467662854631\n",
            "train loss : 0.0004431122579963674    train_acc : 0.038353789160868554\n",
            "train loss : 0.00025834996509757526    train_acc : 0.043353788687552464\n",
            "train loss : 0.0003567389295313528    train_acc : 0.038356455353966694\n",
            "train loss : 0.00020539930935148737    train_acc : 0.046687123442855444\n",
            "train loss : 0.00038020385197255584    train_acc : 0.04169156646583619\n",
            "train loss : 0.00046706898251052146    train_acc : 0.03502223550211512\n",
            "train loss : 0.00040356099183421065    train_acc : 0.03501867852560113\n",
            "train loss : 0.0003862233362175362    train_acc : 0.038352009961880325\n",
            "train loss : 0.0003666293701278674    train_acc : 0.03835378773864634\n",
            "train loss : 0.00043934249941935793    train_acc : 0.03668712202012728\n",
            "train loss : 0.0003243400238040523    train_acc : 0.04168623313174407\n",
            "train loss : 0.0004028781739119468    train_acc : 0.04002223265767026\n",
            "train loss : 0.0003068450983093709    train_acc : 0.04002134519075076\n",
            "train loss : 0.00020418508868194827    train_acc : 0.043354678050768404\n",
            "train loss : 0.00048429141544776904    train_acc : 0.040023122494960414\n",
            "train loss : 0.00045591119326540355    train_acc : 0.038354678998663974\n",
            "train loss : 0.00035150279746266145    train_acc : 0.04168712249546595\n",
            "train loss : 0.0006015196240220908    train_acc : 0.03502223313199758\n",
            "train loss : 0.000285126267263427    train_acc : 0.045018678524337066\n",
            "train loss : 0.0002961332201949168    train_acc : 0.043357343295212986\n",
            "train loss : 0.0003397647632280981    train_acc : 0.04168979058309078\n",
            "train loss : 0.0003364539561850823    train_acc : 0.041688901221644314\n",
            "train loss : 0.0003099491679095259    train_acc : 0.041688900747318215\n",
            "train loss : 0.00023026714592421974    train_acc : 0.0433555674137319\n",
            "train loss : 0.0005174992002374291    train_acc : 0.03335645630262066\n",
            "train loss : 0.0004833674175218649    train_acc : 0.036684456776694734\n",
            "train loss : 0.00027203024417785413    train_acc : 0.046686231710280905\n",
            "train loss : 0.0003779663057379756    train_acc : 0.04169156599024548\n",
            "train loss : 0.00036894007435964225    train_acc : 0.040022235501861465\n",
            "train loss : 0.0003339271477800556    train_acc : 0.04168801185893433\n",
            "train loss : 0.0003860180653853915    train_acc : 0.041688900272991436\n",
            "train loss : 0.0004914568045445402    train_acc : 0.03668890074681226\n",
            "train loss : 0.0006211572417247935    train_acc : 0.0316862340803983\n",
            "train loss : 0.00037024114256586655    train_acc : 0.03835023265817621\n",
            "train loss : 0.0003464458979168229    train_acc : 0.04168712012408436\n",
            "train loss : 0.0002858943111947906    train_acc : 0.04335556646406618\n",
            "train loss : 0.00042555445069033577    train_acc : 0.04335645630211417\n",
            "train loss : 0.00037298516015812536    train_acc : 0.0416897901100278\n",
            "train loss : 0.0003093052435746755    train_acc : 0.04335556788805868\n",
            "train loss : 0.0003791316757482333    train_acc : 0.040023122969540297\n",
            "train loss : 0.0006282099651977688    train_acc : 0.03502134566558376\n",
            "train loss : 0.00037350847289655925    train_acc : 0.04335201138435498\n",
            "train loss : 0.0005544842539573548    train_acc : 0.030023121072738322\n",
            "train loss : 0.0004217121665349719    train_acc : 0.03668267899790546\n",
            "train loss : 0.00023651420771623566    train_acc : 0.04501956409546555\n",
            "train loss : 0.0004220300372425691    train_acc : 0.036690677100850916\n",
            "train loss : 0.0004771531578411518    train_acc : 0.035019568361120454\n",
            "train loss : 0.00033846515978476786    train_acc : 0.040018677103125934\n",
            "train loss : 0.00042751104046803105    train_acc : 0.033354676627788335\n",
            "train loss : 0.0003614695372603199    train_acc : 0.038351122494201494\n",
            "train loss : 0.0005092900420031241    train_acc : 0.036687120598663574\n",
            "train loss : 0.00035931684077712174    train_acc : 0.03835289979765262\n",
            "train loss : 0.0004586120712359796    train_acc : 0.03502045487989208\n",
            "train loss : 0.00040401958572957783    train_acc : 0.03668534424260261\n",
            "train loss : 0.00026528535918738093    train_acc : 0.045019565516929386\n",
            "train loss : 0.0003555055538741024    train_acc : 0.04169067710160903\n",
            "train loss : 0.0003655262436379288    train_acc : 0.041688901694454196\n",
            "train loss : 0.000368466806816838    train_acc : 0.043355567414237045\n",
            "train loss : 0.00038740151237509396    train_acc : 0.03668978963595426\n",
            "train loss : 0.0002964821980429281    train_acc : 0.04168623455447251\n",
            "train loss : 0.00025418399638737745    train_acc : 0.043355565991762386\n",
            "train loss : 0.00044160685538645214    train_acc : 0.03335645630186227\n",
            "train loss : 0.00018187773811340012    train_acc : 0.04835112344336099\n",
            "train loss : 0.0005480005074214273    train_acc : 0.03669245393250313\n",
            "train loss : 0.00032826518767296853    train_acc : 0.038352902642097335\n",
            "Looked at 25600/60000 samples\n",
            "train loss : 0.00037019387486644963    train_acc : 0.04168712154807579\n",
            "train loss : 0.0004710593167177999    train_acc : 0.03668889979815897\n",
            "train loss : 0.000322685052995853    train_acc : 0.03835290074655902\n",
            "train loss : 0.00047027943156032826    train_acc : 0.035020454880398164\n",
            "train loss : 0.0003861927025986876    train_acc : 0.04001867757593621\n",
            "train loss : 0.00042600359085048426    train_acc : 0.0333546766280405\n",
            "train loss : 0.0003004805296027016    train_acc : 0.043351122494201616\n",
            "train loss : 0.0005427633039712568    train_acc : 0.030023120598663574\n",
            "train loss : 0.0004143988309601975    train_acc : 0.038349345664319294\n",
            "train loss : 0.0002126401570223861    train_acc : 0.046687119651020965\n",
            "train loss : 0.0002625476522396291    train_acc : 0.043358233130480546\n",
            "train loss : 0.00038496773461252587    train_acc : 0.04002312439100292\n",
            "train loss : 0.0005016762633907516    train_acc : 0.035021345666341866\n",
            "train loss : 0.0004850779447883917    train_acc : 0.028352011384355384\n",
            "train loss : 0.0004225154958552218    train_acc : 0.04001512107273832\n",
            "train loss : 0.0002819392723635772    train_acc : 0.045021341397905465\n",
            "train loss : 0.0004107632686637664    train_acc : 0.041690678048745544\n",
            "train loss : 0.0002971035469081636    train_acc : 0.041688901694959334\n",
            "train loss : 0.00037574105486491027    train_acc : 0.041688900747570645\n",
            "train loss : 0.00045809761329770204    train_acc : 0.03502223408039871\n",
            "train loss : 0.00044585617709501206    train_acc : 0.04001867852484288\n",
            "train loss : 0.000251219756615577    train_acc : 0.041688009961879914\n",
            "train loss : 0.00033072547175471643    train_acc : 0.040022233605313\n",
            "train loss : 0.0004148886252300368    train_acc : 0.036688011857922836\n",
            "train loss : 0.0006136736131058193    train_acc : 0.03835290027299089\n",
            "train loss : 0.0003754270787162964    train_acc : 0.03835378821347893\n",
            "train loss : 0.0004132090766014059    train_acc : 0.035020455353713856\n",
            "train loss : 0.00027812476151565873    train_acc : 0.04501867757618865\n",
            "train loss : 0.00042870474367774    train_acc : 0.0383573432947073\n",
            "train loss : 0.00043845244954696994    train_acc : 0.03668712391642384\n",
            "train loss : 0.0002802544597451458    train_acc : 0.04001956646608876\n",
            "train loss : 0.00022739748603840218    train_acc : 0.04668801043544858\n",
            "train loss : 0.00019407479275834158    train_acc : 0.04669156693889891\n",
            "train loss : 0.0004925882668405617    train_acc : 0.03502490216903408\n",
            "train loss : 0.0006306017640352837    train_acc : 0.03168534661449015\n",
            "train loss : 0.0006260954456966781    train_acc : 0.031683565518194394\n",
            "train loss : 0.00030884968880110007    train_acc : 0.040016897901609706\n",
            "train loss : 0.00047231657148807997    train_acc : 0.03835467567888086\n",
            "train loss : 0.000295134835470207    train_acc : 0.04335378916036207\n",
            "train loss : 0.0004746482703171987    train_acc : 0.0366897886875522\n",
            "train loss : 0.0003562666980919882    train_acc : 0.04001956788730003\n",
            "train loss : 0.00046944637817851687    train_acc : 0.03835467710287323\n",
            "train loss : 0.00039741175979891196    train_acc : 0.0400204558277882\n",
            "train loss : 0.0004023943094550796    train_acc : 0.03668801090977482\n",
            "train loss : 0.0004763587333757686    train_acc : 0.03335290027248521\n",
            "train loss : 0.000440310572731124    train_acc : 0.040017788213478664\n",
            "train loss : 0.0004491367716975464    train_acc : 0.03502134282038052\n",
            "train loss : 0.00047629423982722957    train_acc : 0.03335201138283753\n",
            "train loss : 0.0004652641579736435    train_acc : 0.03668445440607084\n",
            "train loss : 0.00032106809098801235    train_acc : 0.043352898375683235\n",
            "train loss : 0.00040863929204029123    train_acc : 0.03668978821246703\n",
            "train loss : 0.0003438436454031832    train_acc : 0.04001956788704665\n",
            "train loss : 0.0002596048907950451    train_acc : 0.043354677102873086\n",
            "train loss : 0.0004894545379424898    train_acc : 0.0383564558277882\n",
            "train loss : 0.0003632942735196826    train_acc : 0.04168712344310815\n",
            "train loss : 0.00030995629839892076    train_acc : 0.043355566465836326\n",
            "train loss : 0.0006125142480140941    train_acc : 0.03002312296878178\n",
            "train loss : 0.0005075574647217436    train_acc : 0.03834934566558335\n",
            "train loss : 0.0002487942664215739    train_acc : 0.04835378631768831\n",
            "train loss : 0.0005612426145918311    train_acc : 0.03169245535270277\n",
            "train loss : 0.00031185659054353747    train_acc : 0.04168356930952144\n",
            "train loss : 0.0004695479738038563    train_acc : 0.03335556457029841\n",
            "train loss : 0.0003022567974020281    train_acc : 0.04168445630110416\n",
            "train loss : 0.00025724417456237416    train_acc : 0.045022231710027255\n",
            "train loss : 0.00020583473336160577    train_acc : 0.043357345190245346\n",
            "train loss : 0.00041495928235950184    train_acc : 0.040023123917434796\n",
            "train loss : 0.00039887129025495367    train_acc : 0.0400213456660893\n",
            "train loss : 0.0001955763800873059    train_acc : 0.04502134471768858\n",
            "train loss : 0.0004247723333731722    train_acc : 0.04002401138384943\n",
            "train loss : 0.00018074583998323524    train_acc : 0.046688012806071384\n",
            "train loss : 0.0006304794442494234    train_acc : 0.031691566940163236\n",
            "train loss : 0.00041613196340623644    train_acc : 0.036683568835701424\n",
            "train loss : 0.00046261003926069167    train_acc : 0.040019564570045706\n",
            "train loss : 0.00037343563424912916    train_acc : 0.036688010434437354\n",
            "train loss : 0.00043297237428976357    train_acc : 0.03668623360556503\n",
            "train loss : 0.00027172260510271366    train_acc : 0.04335289932458963\n",
            "train loss : 0.00031479615366738285    train_acc : 0.04168978821297312\n",
            "train loss : 0.00043918146778952426    train_acc : 0.03502223455371359\n",
            "train loss : 0.0004196126379123784    train_acc : 0.04001867852509531\n",
            "train loss : 0.00043379157167633647    train_acc : 0.04002134329521339\n",
            "train loss : 0.00048254123735100404    train_acc : 0.03502134471642411\n",
            "train loss : 0.0002519801202954593    train_acc : 0.04001867805051543\n",
            "train loss : 0.0003217216022779434    train_acc : 0.043354676628293606\n",
            "train loss : 0.00037580650942526766    train_acc : 0.04002312249420176\n",
            "train loss : 0.0004162353829378393    train_acc : 0.035021345665330245\n",
            "train loss : 0.0003496403144495135    train_acc : 0.04335201138435484\n",
            "train loss : 0.00050152422558714    train_acc : 0.03835645440607166\n",
            "train loss : 0.0004008912779573411    train_acc : 0.04335379010901657\n",
            "train loss : 0.00044572963204583506    train_acc : 0.03835645535472481\n",
            "train loss : 0.000544979543759487    train_acc : 0.031687123442855854\n",
            "train loss : 0.0003221013463186021    train_acc : 0.04001689979916952\n",
            "train loss : 0.00034180625164861926    train_acc : 0.04002134234655956\n",
            "train loss : 0.000416086182826807    train_acc : 0.0383546780492515\n",
            "train loss : 0.00014411321537538847    train_acc : 0.0466871224949596\n",
            "train loss : 0.0004692296606236397    train_acc : 0.036691566465330644\n",
            "train loss : 0.0002575624741671845    train_acc : 0.041686235502114845\n",
            "train loss : 0.000459609546065731    train_acc : 0.03835556599226779\n",
            "train loss : 0.0005057790428361895    train_acc : 0.03168712296852921\n",
            "train loss : 0.0002592681269407252    train_acc : 0.04335023313224989\n",
            "train loss : 0.0002755762989478938    train_acc : 0.04335645345767053\n",
            "train loss : 0.00027793326069806194    train_acc : 0.04168979010851076\n",
            "train loss : 0.00020071234532509854    train_acc : 0.04335556788805787\n",
            "train loss : 0.00042673802040009136    train_acc : 0.03835645630287363\n",
            "train loss : 0.00030429746233443146    train_acc : 0.04002045677669487\n",
            "train loss : 0.00043185899302971637    train_acc : 0.04002134424361424\n",
            "train loss : 0.0003691299880856705    train_acc : 0.041688011383596596\n",
            "train loss : 0.0005022781999442479    train_acc : 0.03835556693940458\n",
            "train loss : 0.0002981904611540653    train_acc : 0.043353789635701016\n",
            "train loss : 0.00039996073423309076    train_acc : 0.03835645535447237\n",
            "train loss : 0.0002786695870599666    train_acc : 0.04168712344285572\n",
            "train loss : 0.0003866391913049932    train_acc : 0.04002223313250285\n",
            "train loss : 0.00043638422423129364    train_acc : 0.038354678524337334\n",
            "train loss : 0.0002733941813274676    train_acc : 0.04335378916187965\n",
            "train loss : 0.0003855240438217568    train_acc : 0.03835645535421967\n",
            "train loss : 0.0005134971307168449    train_acc : 0.03335379010952225\n",
            "train loss : 0.0005520635303585503    train_acc : 0.030017788688058412\n",
            "train loss : 0.0006713555682212369    train_acc : 0.031682676153966964\n",
            "train loss : 0.000597217225309399    train_acc : 0.031683564093948784\n",
            "train loss : 0.0003811384723150869    train_acc : 0.03501689790085011\n",
            "train loss : 0.00036280967211646845    train_acc : 0.04168534234554712\n",
            "train loss : 0.0003233037610446926    train_acc : 0.03835556551591762\n",
            "train loss : 0.000386900238578563    train_acc : 0.03668712296827516\n",
            "train loss : 0.0006580512081423786    train_acc : 0.028352899798916413\n",
            "train loss : 0.0003097548887652736    train_acc : 0.04334845487989276\n",
            "train loss : 0.0003683760269379893    train_acc : 0.04002311917593594\n",
            "train loss : 0.00023605137901856616    train_acc : 0.0450213456635605\n",
            "train loss : 0.00047060392014099415    train_acc : 0.03669067805102056\n",
            "train loss : 0.0004409516469198434    train_acc : 0.03668623502829388\n",
            "train loss : 0.0002601867339142459    train_acc : 0.04335289932534843\n",
            "train loss : 0.0007924705889713038    train_acc : 0.030023121546306854\n",
            "train loss : 0.00042118099372370456    train_acc : 0.038349345664824695\n",
            "train loss : 0.00036813784422896055    train_acc : 0.04168711965102124\n",
            "train loss : 0.0004496958201131949    train_acc : 0.036688899797147206\n",
            "train loss : 0.000329986529156632    train_acc : 0.04168623407989181\n",
            "train loss : 0.00016487886922444506    train_acc : 0.050022232658175944\n",
            "train loss : 0.00019335475630451247    train_acc : 0.04502667852408436\n",
            "train loss : 0.0003034045522852309    train_acc : 0.04002401422854618\n",
            "train loss : 0.0002961966922667527    train_acc : 0.045021346140921896\n",
            "train loss : 0.00039297260779967767    train_acc : 0.03502401138460849\n",
            "train loss : 0.0005260743454717998    train_acc : 0.03335201280607179\n",
            "train loss : 0.0003960796376204821    train_acc : 0.03835112107349657\n",
            "train loss : 0.0003793871279917895    train_acc : 0.04168712059790586\n",
            "train loss : 0.0003233279565109869    train_acc : 0.040022233130985546\n",
            "train loss : 0.0004471810195593905    train_acc : 0.03835467852433652\n",
            "train loss : 0.00023562189587136102    train_acc : 0.04668712249521298\n",
            "train loss : 0.0004475571975530422    train_acc : 0.03669156646533078\n",
            "train loss : 0.00018717394713642444    train_acc : 0.04501956883544818\n",
            "train loss : 0.0004083622060693321    train_acc : 0.033357343770045574\n",
            "train loss : 0.00028633690455555794    train_acc : 0.041684457250010694\n",
            "train loss : 0.000505478952283748    train_acc : 0.03502223171053334\n",
            "train loss : 0.0006129225581349029    train_acc : 0.025018678523578954\n",
            "train loss : 0.0004679981910117182    train_acc : 0.038346676628545906\n",
            "train loss : 0.0005836365288651215    train_acc : 0.03168711822753523\n",
            "train loss : 0.00023125558840784264    train_acc : 0.04668356646305469\n",
            "train loss : 0.0005447779281920728    train_acc : 0.03835823123544697\n",
            "train loss : 0.000396350887838151    train_acc : 0.038353791056658905\n",
            "train loss : 0.00019520249889555142    train_acc : 0.05002045535523022\n",
            "train loss : 0.00038546517545343117    train_acc : 0.040026677576189454\n",
            "train loss : 0.00033959875210155046    train_acc : 0.03668801422804064\n",
            "train loss : 0.0004803698705551573    train_acc : 0.036686233607588284\n",
            "train loss : 0.0003162431484565161    train_acc : 0.04335289932459071\n",
            "train loss : 0.000521414237963682    train_acc : 0.03668978821297311\n",
            "train loss : 0.0003038049023722265    train_acc : 0.04335290122038025\n",
            "train loss : 0.0004012322743882802    train_acc : 0.04002312154731753\n",
            "train loss : 0.0006518778979153183    train_acc : 0.028354678998158567\n",
            "train loss : 0.0006215618354892397    train_acc : 0.04001512249546568\n",
            "train loss : 0.00028388201640991716    train_acc : 0.04502134139866425\n",
            "train loss : 0.00045304164091452995    train_acc : 0.03169067804874596\n",
            "train loss : 0.000279945779719881    train_acc : 0.04335023502829267\n",
            "train loss : 0.0006418033825815407    train_acc : 0.030023120125348423\n",
            "train loss : 0.0005332431053790761    train_acc : 0.03501601233073352\n",
            "train loss : 0.00024910211037824806    train_acc : 0.041685341873243055\n",
            "train loss : 0.00031569300460535275    train_acc : 0.0400222321823324\n",
            "train loss : 0.0005271516942891424    train_acc : 0.02668801185716391\n",
            "train loss : 0.0002911920273412023    train_acc : 0.04501423360632382\n",
            "train loss : 0.0002624283120327053    train_acc : 0.04502400759125671\n",
            "train loss : 0.00038206824672938947    train_acc : 0.03669067947071533\n",
            "train loss : 0.00033270785145766323    train_acc : 0.04001956836238438\n",
            "train loss : 0.0003138388287699636    train_acc : 0.041688010436459935\n",
            "train loss : 0.0003496597716591982    train_acc : 0.038355566938899445\n",
            "train loss : 0.0005263015848005815    train_acc : 0.036687122969034085\n",
            "train loss : 0.00048048546712777906    train_acc : 0.03168623313225015\n",
            "train loss : 0.00032623091406716863    train_acc : 0.04168356599100387\n",
            "train loss : 0.0005361351409438827    train_acc : 0.031688897901861864\n",
            "train loss : 0.00023926882283098876    train_acc : 0.04501690074554766\n",
            "train loss : 0.0005722168688685958    train_acc : 0.03335734234706429\n",
            "train loss : 0.000263431794039658    train_acc : 0.04335112391591843\n",
            "train loss : 0.0003021396526371597    train_acc : 0.04502312059942182\n",
            "train loss : 0.000578983849277956    train_acc : 0.043357345664319695\n",
            "train loss : 0.00043768430509884666    train_acc : 0.03835645725102097\n",
            "train loss : 0.00042781766650283983    train_acc : 0.04002045677720055\n",
            "train loss : 0.0003352208006985834    train_acc : 0.04335467757694784\n",
            "train loss : 0.0004226974192231818    train_acc : 0.04168978916137437\n",
            "train loss : 0.00033410996392265797    train_acc : 0.04335556788755274\n",
            "train loss : 0.00028053402694169625    train_acc : 0.04335645630287337\n",
            "train loss : 0.0003742911841063611    train_acc : 0.0416897901100282\n",
            "train loss : 0.00045304424171583327    train_acc : 0.03502223455472535\n",
            "train loss : 0.0004315166734409594    train_acc : 0.04168534519176252\n",
            "train loss : 0.00030285363669198105    train_acc : 0.04335556551743561\n",
            "train loss : 0.0005410572382888529    train_acc : 0.03502312296827596\n",
            "train loss : 0.0003127004443535848    train_acc : 0.043352012332249744\n",
            "train loss : 0.0003800920563665459    train_acc : 0.040023121073243864\n",
            "train loss : 0.0005045114799032067    train_acc : 0.0400213456645724\n",
            "train loss : 0.00033241841945879665    train_acc : 0.043354678051021105\n",
            "train loss : 0.0003165767051577771    train_acc : 0.038356455828293876\n",
            "train loss : 0.0002941966780269305    train_acc : 0.04335379010977509\n",
            "train loss : 0.0003677436129637953    train_acc : 0.03835645535472521\n",
            "train loss : 0.0003546205429721614    train_acc : 0.03668712344285586\n",
            "train loss : 0.0003408630226595396    train_acc : 0.038352899799169525\n",
            "train loss : 0.0003983788087386313    train_acc : 0.040020454879892885\n",
            "train loss : 0.0003467198590381632    train_acc : 0.03835467757593594\n",
            "train loss : 0.0005100793200800124    train_acc : 0.0350204558280405\n",
            "train loss : 0.00025310604448096325    train_acc : 0.04168534424310829\n",
            "train loss : 0.0004611737383136483    train_acc : 0.040022232183596324\n",
            "train loss : 0.00032122221665131936    train_acc : 0.04002134519049792\n",
            "train loss : 0.0002878012725357297    train_acc : 0.043354678050768265\n",
            "train loss : 0.00016293476435707115    train_acc : 0.05168978916162707\n",
            "train loss : 0.000401710274751521    train_acc : 0.04002756788755287\n",
            "train loss : 0.00031561605390068454    train_acc : 0.04168801470287336\n",
            "train loss : 0.0003755950660295283    train_acc : 0.04002223360784153\n",
            "train loss : 0.0004735891799784124    train_acc : 0.03668801185792418\n",
            "train loss : 0.00039510369326171117    train_acc : 0.04168623360632422\n",
            "train loss : 0.0003285454667410612    train_acc : 0.041688899324590044\n",
            "train loss : 0.0003048977240963407    train_acc : 0.038355567412973114\n",
            "train loss : 0.0005235743311238312    train_acc : 0.03835378963595359\n",
            "train loss : 0.0003561954854216677    train_acc : 0.0400204553544725\n",
            "train loss : 0.00034709482093166173    train_acc : 0.04168801090952238\n",
            "train loss : 0.0005303647131850958    train_acc : 0.03668890027248508\n",
            "train loss : 0.000433394274500271    train_acc : 0.036686234080145326\n",
            "train loss : 0.00033951089062234416    train_acc : 0.040019565991509416\n",
            "train loss : 0.000293227197251203    train_acc : 0.04335467710186214\n",
            "train loss : 0.00037076347783487185    train_acc : 0.03668978916112099\n",
            "train loss : 0.00041288850717393705    train_acc : 0.03835290122088593\n",
            "train loss : 0.0004129877764278658    train_acc : 0.036687121547317805\n",
            "train loss : 0.00044928136778292624    train_acc : 0.03835289979815857\n",
            "train loss : 0.00022352950383987648    train_acc : 0.04668712154655902\n",
            "train loss : 0.0006701870029057914    train_acc : 0.030024899798158165\n",
            "train loss : 0.000305651152042012    train_acc : 0.04168267994655902\n",
            "train loss : 0.0004074390403556334    train_acc : 0.04002223076263817\n",
            "train loss : 0.0003451687079361877    train_acc : 0.045021345189740075\n",
            "train loss : 0.00033581320349278406    train_acc : 0.04335734471743453\n",
            "train loss : 0.0005774714791186857    train_acc : 0.03335645725051597\n",
            "train loss : 0.0008111523425972943    train_acc : 0.02335112344386694\n",
            "train loss : 0.00046286344140563523    train_acc : 0.038345787265836734\n",
            "train loss : 0.00019612411053768683    train_acc : 0.04835378441987512\n",
            "train loss : 0.00038833546373257    train_acc : 0.0366924553516906\n",
            "train loss : 0.0004775750855749608    train_acc : 0.03335290264285423\n",
            "train loss : 0.0003101288280603698    train_acc : 0.043351121548076195\n",
            "train loss : 0.0002476460011271221    train_acc : 0.0433564539314923\n",
            "train loss : 0.0003140312134568674    train_acc : 0.04168979010876346\n",
            "train loss : 0.0003621374311287851    train_acc : 0.04002223455472467\n",
            "train loss : 0.0003110955447484249    train_acc : 0.041688011858429186\n",
            "train loss : 0.00038229485566300967    train_acc : 0.038355566939657824\n",
            "train loss : 0.0002973693797864643    train_acc : 0.04335378963570115\n",
            "train loss : 0.0003877517074049779    train_acc : 0.03668978868780571\n",
            "train loss : 0.0003232058014599324    train_acc : 0.04001956788730016\n",
            "train loss : 0.0005675400572949583    train_acc : 0.035021343769539894\n",
            "train loss : 0.00045221277052974354    train_acc : 0.035018678050010424\n",
            "train loss : 0.0003530372896616979    train_acc : 0.04335200996162667\n",
            "train loss : 0.0005328438117620058    train_acc : 0.03335645440531287\n",
            "train loss : 0.0005125039938521129    train_acc : 0.035017790109016164\n",
            "train loss : 0.00025105544377027256    train_acc : 0.0450186761547248\n",
            "train loss : 0.0003952898664922113    train_acc : 0.038357343293949187\n",
            "train loss : 0.000510622655978926    train_acc : 0.033353790583090105\n",
            "train loss : 0.0004881080890208515    train_acc : 0.035017788688310986\n",
            "train loss : 0.0005221943885434739    train_acc : 0.035018676153967095\n",
            "train loss : 0.00022785394884906074    train_acc : 0.04335200996061545\n",
            "train loss : 0.00046763021961581844    train_acc : 0.033356454405312326\n",
            "train loss : 0.00042256004121152946    train_acc : 0.04001779010901616\n",
            "train loss : 0.00032090998907357453    train_acc : 0.04335467615472481\n",
            "train loss : 0.00019613440755854433    train_acc : 0.045023122493949186\n",
            "train loss : 0.00031951922064846807    train_acc : 0.04002401233199677\n",
            "train loss : 0.00023299777167577653    train_acc : 0.0450213461399104\n",
            "train loss : 0.0002790836669267948    train_acc : 0.04169067805127462\n",
            "train loss : 0.0006044532218603167    train_acc : 0.038355568361627346\n",
            "train loss : 0.0003038750344042793    train_acc : 0.0450204563031262\n",
            "train loss : 0.0002718959546753314    train_acc : 0.04335734424336166\n",
            "train loss : 0.0004309259305687305    train_acc : 0.03502312391692979\n",
            "train loss : 0.00036427535176013795    train_acc : 0.04335201233275569\n",
            "train loss : 0.0004992979534867591    train_acc : 0.03668978773991081\n",
            "train loss : 0.000398907497075844    train_acc : 0.03668623455346129\n",
            "train loss : 0.0003209073879636911    train_acc : 0.040019565991761846\n",
            "train loss : 0.00040889647587343415    train_acc : 0.040021343768528936\n",
            "train loss : 0.0004952827817439196    train_acc : 0.03668801138334322\n",
            "train loss : 0.00029070982825630187    train_acc : 0.043352900272737784\n",
            "train loss : 0.0005205195204919808    train_acc : 0.031689788213478794\n",
            "train loss : 0.00030594736316768363    train_acc : 0.04168356788704719\n",
            "train loss : 0.00045029338026035613    train_acc : 0.036688897902873094\n",
            "train loss : 0.00023749812982254022    train_acc : 0.04501956741221487\n",
            "train loss : 0.0001637468674158614    train_acc : 0.05002401043595318\n",
            "train loss : 0.0004096981435319252    train_acc : 0.03669334613889917\n",
            "train loss : 0.00024533548654711516    train_acc : 0.04668623645127408\n",
            "train loss : 0.0004108132377131865    train_acc : 0.04335823265944068\n",
            "train loss : 0.0003212396399950909    train_acc : 0.04168979105741837\n",
            "train loss : 0.0004780525050744362    train_acc : 0.03835556788856396\n",
            "train loss : 0.0005679652020180059    train_acc : 0.033353789636207234\n",
            "train loss : 0.0003985464056293087    train_acc : 0.03668445535447264\n",
            "train loss : 0.0004949995199563747    train_acc : 0.038352898376189055\n",
            "train loss : 0.00046727351232739143    train_acc : 0.04002045487913397\n",
            "train loss : 0.0005459094857739574    train_acc : 0.03168801090926887\n",
            "train loss : 0.00029254474135587306    train_acc : 0.04335023360581827\n",
            "train loss : 0.0003739188221713826    train_acc : 0.03835645345792311\n",
            "train loss : 0.0006834287426263821    train_acc : 0.026687123441844224\n",
            "train loss : 0.0003651552210374086    train_acc : 0.038347566465835654\n",
            "train loss : 0.0002935387385055819    train_acc : 0.043353785368781775\n",
            "train loss : 0.0003750602328879126    train_acc : 0.03835645535219668\n",
            "train loss : 0.00025050269656309035    train_acc : 0.04668712344285451\n",
            "train loss : 0.00036686098536964487    train_acc : 0.04169156646583619\n",
            "train loss : 0.00030637928345977525    train_acc : 0.04168890216878178\n",
            "train loss : 0.000494854647045661    train_acc : 0.036688900747823355\n",
            "train loss : 0.0003091510544872948    train_acc : 0.041686234080398836\n",
            "train loss : 0.00020798860213018294    train_acc : 0.048355565991509544\n",
            "train loss : 0.00031593012713443364    train_acc : 0.04169245630186214\n",
            "train loss : 0.0004833212988530426    train_acc : 0.03502223597669433\n",
            "train loss : 0.000409624092923109    train_acc : 0.0366853451925209\n",
            "train loss : 0.0003086882385217595    train_acc : 0.04001956551743601\n",
            "train loss : 0.0004214397090039043    train_acc : 0.040021343768275965\n",
            "train loss : 0.0003613100237855573    train_acc : 0.04002134471667641\n",
            "train loss : 0.00032893990759251963    train_acc : 0.043354678050515565\n",
            "train loss : 0.00036652007725654286    train_acc : 0.040023122494960275\n",
            "train loss : 0.00041928743319525626    train_acc : 0.04168801233199731\n",
            "train loss : 0.0004305666439003246    train_acc : 0.035022233606577066\n",
            "train loss : 0.0004099032309718803    train_acc : 0.04001867852459018\n",
            "train loss : 0.00044500417820756    train_acc : 0.03835467662854645\n",
            "train loss : 0.0003591922102361899    train_acc : 0.04335378916086856\n",
            "train loss : 0.00028584954723665396    train_acc : 0.041689788687552465\n",
            "train loss : 0.00029794409305564535    train_acc : 0.04168890122063336\n",
            "train loss : 0.00042308644207017    train_acc : 0.04002223408065101\n",
            "train loss : 0.0005862812745610963    train_acc : 0.03502134519150968\n",
            "train loss : 0.0003676747057463349    train_acc : 0.04335201138410214\n",
            "train loss : 0.00046678900902926264    train_acc : 0.03502312107273819\n",
            "train loss : 0.0004234030857096008    train_acc : 0.03668534566457213\n",
            "train loss : 0.000434281874498495    train_acc : 0.04168623218435444\n",
            "train loss : 0.00032495397061708275    train_acc : 0.04502223265716499\n",
            "train loss : 0.0003350958132093779    train_acc : 0.038357345190750486\n",
            "train loss : 0.0004470080049781452    train_acc : 0.0400204572507684\n",
            "train loss : 0.00033104027270804235    train_acc : 0.03835467757720041\n",
            "train loss : 0.00020553747603606928    train_acc : 0.048353789161374505\n",
            "train loss : 0.0002362013927024699    train_acc : 0.0466924553542194\n",
            "train loss : 0.0002563174184730806    train_acc : 0.043358235976188916\n",
            "train loss : 0.00028898189203562056    train_acc : 0.04502312439252063\n",
            "train loss : 0.00041117179589661825    train_acc : 0.038357345666342677\n",
            "train loss : 0.0004258778355291006    train_acc : 0.041687123917688716\n",
            "train loss : 0.00043037179452375677    train_acc : 0.040022233132756095\n",
            "train loss : 0.0006061734657464673    train_acc : 0.031688011857670804\n",
            "train loss : 0.0005635513274882093    train_acc : 0.03668356693965742\n",
            "train loss : 0.00040068431925291555    train_acc : 0.04335289790236782\n",
            "train loss : 0.0004415822781681035    train_acc : 0.03502312154554793\n",
            "train loss : 0.00036350305942734395    train_acc : 0.04001867899815763\n",
            "train loss : 0.0003355357668986217    train_acc : 0.04168800996213235\n",
            "train loss : 0.0005015699499777788    train_acc : 0.03502223360531313\n",
            "train loss : 0.00024404601883538203    train_acc : 0.04335201185792283\n",
            "train loss : 0.00031182897608349934    train_acc : 0.04335645440632423\n",
            "train loss : 0.00036737606118796066    train_acc : 0.03835645677568337\n",
            "train loss : 0.00030994858892859713    train_acc : 0.0416871234436137\n",
            "train loss : 0.00047577212888162457    train_acc : 0.041688899799169926\n",
            "train loss : 0.0003317652426329621    train_acc : 0.04168890074655956\n",
            "train loss : 0.000817162389664235    train_acc : 0.030022234080398162\n",
            "train loss : 0.00037742283216853057    train_acc : 0.04001601185817621\n",
            "train loss : 0.0004730727546812256    train_acc : 0.035021341872991024\n",
            "train loss : 0.00043116384645427075    train_acc : 0.03835201138233226\n",
            "train loss : 0.0003262203130398863    train_acc : 0.04002045440607058\n",
            "train loss : 0.0004239577315220978    train_acc : 0.03168801090901657\n",
            "train loss : 0.0003935107813020267    train_acc : 0.03835023360581814\n",
            "train loss : 0.0007326829915166293    train_acc : 0.02668712012458977\n",
            "train loss : 0.0003909521859743818    train_acc : 0.04334756646406645\n",
            "train loss : 0.0003692506500647788    train_acc : 0.0433564520354475\n",
            "train loss : 0.00046532386137371944    train_acc : 0.04168979010775224\n",
            "train loss : 0.00041087582394066564    train_acc : 0.0416889012213908\n",
            "train loss : 0.0002746838363789695    train_acc : 0.046688900747318074\n",
            "train loss : 0.00037629278278524523    train_acc : 0.03835823408039857\n",
            "train loss : 0.0002865159080917368    train_acc : 0.043353791058176216\n",
            "train loss : 0.00038774086441286374    train_acc : 0.03835645535523103\n",
            "train loss : 0.0003027622926150729    train_acc : 0.04335379010952279\n",
            "train loss : 0.0004905889672718003    train_acc : 0.040023122021391744\n",
            "train loss : 0.00046139735276130476    train_acc : 0.03668801233174474\n",
            "train loss : 0.0002742755878146205    train_acc : 0.04168623360657693\n",
            "train loss : 0.00037144150990741384    train_acc : 0.041688899324590176\n",
            "train loss : 0.0004425704877994246    train_acc : 0.038355567412973114\n",
            "train loss : 0.0005099004141702345    train_acc : 0.03335378963595358\n",
            "train loss : 0.0003960422741377202    train_acc : 0.04168445535447251\n",
            "train loss : 0.00030750479656150627    train_acc : 0.04335556504285572\n",
            "train loss : 0.0003388569219714148    train_acc : 0.038356456301356194\n",
            "train loss : 0.0003116001153237982    train_acc : 0.04335379011002739\n",
            "train loss : 0.0005239527315279774    train_acc : 0.030023122021392012\n",
            "train loss : 0.0002690992504167758    train_acc : 0.04334934566507807\n",
            "train loss : 0.00039376874721049244    train_acc : 0.04168978631768804\n",
            "train loss : 0.00026430534818414053    train_acc : 0.04168890121936943\n",
            "train loss : 0.00047041569734943516    train_acc : 0.041688900747317\n",
            "train loss : 0.000438713304702893    train_acc : 0.0383555674137319\n",
            "train loss : 0.0005991137309988363    train_acc : 0.030020456302620655\n",
            "train loss : 0.0004532946415098042    train_acc : 0.041682677576694735\n",
            "train loss : 0.00045319372883199856    train_acc : 0.03835556409470757\n",
            "train loss : 0.0004832587028846251    train_acc : 0.03668712296751717\n",
            "train loss : 0.0003738008732999857    train_acc : 0.03835289979891601\n",
            "train loss : 0.000372824298159038    train_acc : 0.04502045487989276\n",
            "train loss : 0.00037212711823082805    train_acc : 0.03669067757593594\n",
            "train loss : 0.00040815754753882144    train_acc : 0.03835290169470716\n",
            "train loss : 0.0004045027016746867    train_acc : 0.03668712154757051\n",
            "train loss : 0.0005083597001774654    train_acc : 0.033352899798158704\n",
            "Looked at 38400/60000 samples\n",
            "train loss : 0.0004339304561772529    train_acc : 0.03835112154655902\n",
            "train loss : 0.00028317439222311225    train_acc : 0.0450204539314915\n",
            "train loss : 0.00029062966912409455    train_acc : 0.04502401090876346\n",
            "train loss : 0.00022343611579688083    train_acc : 0.046690679472484675\n",
            "train loss : 0.00036022995745267633    train_acc : 0.038358235029051994\n",
            "train loss : 0.0003057501047708465    train_acc : 0.040020457725348825\n",
            "train loss : 0.0004924633546050315    train_acc : 0.035021344244120185\n",
            "train loss : 0.00017583311920395828    train_acc : 0.04668534471693019\n",
            "train loss : 0.00046880623468465295    train_acc : 0.03835823218384903\n",
            "train loss : 0.00041291558785315995    train_acc : 0.03335379105716472\n",
            "train loss : 0.0003493933860145308    train_acc : 0.04001778868856382\n",
            "train loss : 0.00013194388861039916    train_acc : 0.048354676153967234\n",
            "train loss : 0.000404147931770296    train_acc : 0.03835912249394878\n",
            "train loss : 0.0002680770072030151    train_acc : 0.04335379153199677\n",
            "train loss : 0.0006232843267394536    train_acc : 0.035023122022150396\n",
            "train loss : 0.00035377159041656244    train_acc : 0.040018678998411814\n",
            "train loss : 0.0005455268638098106    train_acc : 0.03835467662879915\n",
            "train loss : 0.0003532530600874564    train_acc : 0.03835378916086869\n",
            "train loss : 0.00028082013457272046    train_acc : 0.0466871220208858\n",
            "train loss : 0.0003658905785895944    train_acc : 0.04335823313174447\n",
            "train loss : 0.00041601049503546913    train_acc : 0.036689791057670264\n",
            "train loss : 0.00047085153255251174    train_acc : 0.04001956788856409\n",
            "train loss : 0.00027751568405995575    train_acc : 0.0433546771028739\n",
            "train loss : 0.0002853798529489791    train_acc : 0.04168978916112153\n",
            "train loss : 0.0004954023004988026    train_acc : 0.03502223455421927\n",
            "train loss : 0.00045967062223441966    train_acc : 0.03668534519176225\n",
            "train loss : 0.00040124390898762006    train_acc : 0.0400195655174356\n",
            "train loss : 0.0004227267823451287    train_acc : 0.0383546771016093\n",
            "train loss : 0.00032383194537278457    train_acc : 0.04502045582778753\n",
            "train loss : 0.0002999310492990881    train_acc : 0.04335734424310815\n",
            "train loss : 0.00021576573079424533    train_acc : 0.045023123916929655\n",
            "train loss : 0.00033275136783189564    train_acc : 0.03835734566608903\n",
            "train loss : 0.00035224900695713073    train_acc : 0.040020457251021914\n",
            "train loss : 0.00035662062192521113    train_acc : 0.041688010910533875\n",
            "train loss : 0.0005075459830348998    train_acc : 0.03668890027248562\n",
            "train loss : 0.00031155758567133935    train_acc : 0.041686234080145323\n",
            "train loss : 0.0003182932875605026    train_acc : 0.04002223265817608\n",
            "train loss : 0.0004516325974539678    train_acc : 0.040021345190751026\n",
            "train loss : 0.00037286313812383417    train_acc : 0.043354678050768404\n",
            "train loss : 0.00031720716493628    train_acc : 0.04168978916162708\n",
            "train loss : 0.00026984446376555394    train_acc : 0.04502223455421953\n",
            "train loss : 0.0002615510698590278    train_acc : 0.04169067852509559\n",
            "train loss : 0.0003675712298963565    train_acc : 0.03835556836188005\n",
            "train loss : 0.00030386040439349355    train_acc : 0.041687122969793\n",
            "train loss : 0.0005353712060177598    train_acc : 0.03502223313225056\n",
            "train loss : 0.00048196032963954735    train_acc : 0.03668534519100387\n",
            "train loss : 0.0003309294367648377    train_acc : 0.0400195655174352\n",
            "train loss : 0.0004077294691289699    train_acc : 0.0383546771016093\n",
            "train loss : 0.00047397653469636095    train_acc : 0.035020455827787525\n",
            "train loss : 0.0003392627718194813    train_acc : 0.04001867757644149\n",
            "train loss : 0.0003450681331889971    train_acc : 0.0366880099613741\n",
            "train loss : 0.0003927703629368219    train_acc : 0.04001956693864607\n",
            "train loss : 0.00038316264448856954    train_acc : 0.041688010435700605\n",
            "train loss : 0.0002389370251480648    train_acc : 0.04335556693889904\n",
            "train loss : 0.00034179438584904057    train_acc : 0.04168978963570075\n",
            "train loss : 0.00015425360546871746    train_acc : 0.04502223455447237\n",
            "train loss : 0.0003718007388451172    train_acc : 0.036690678525095714\n",
            "train loss : 0.00042994981706315883    train_acc : 0.03501956836188005\n",
            "train loss : 0.0003524016177161544    train_acc : 0.040018677103126336\n",
            "train loss : 0.00039291471856623246    train_acc : 0.04335467662778833\n",
            "train loss : 0.0004004441901875485    train_acc : 0.03502312249420149\n",
            "train loss : 0.0003242104955887706    train_acc : 0.04001867899866357\n",
            "train loss : 0.0005093079360171623    train_acc : 0.03668800996213262\n",
            "train loss : 0.00039700170419225926    train_acc : 0.041686233605313136\n",
            "train loss : 0.0004284661160578283    train_acc : 0.036688899324589505\n",
            "train loss : 0.000543526473635921    train_acc : 0.04168623407963978\n",
            "train loss : 0.00026346989825725266    train_acc : 0.04668889932484247\n",
            "train loss : 0.0003589541527872899    train_acc : 0.045024900746306586\n",
            "train loss : 0.0004923753500415615    train_acc : 0.038357346613731365\n",
            "train loss : 0.0003122546096674831    train_acc : 0.04502045725152733\n",
            "train loss : 0.00033873757562743464    train_acc : 0.04335734424386748\n",
            "train loss : 0.0003987618151815293    train_acc : 0.03668979058359673\n",
            "train loss : 0.00047024368009903754    train_acc : 0.038352901221644586\n",
            "train loss : 0.000192077426268058    train_acc : 0.045020454880651546\n",
            "train loss : 0.0001884161480099113    train_acc : 0.045024010909269684\n",
            "train loss : 0.00033126723284444967    train_acc : 0.04169067947248495\n",
            "train loss : 0.00037313854668678137    train_acc : 0.04002223502905199\n",
            "train loss : 0.00027540407899207084    train_acc : 0.04335467852534883\n",
            "train loss : 0.00042068053924943703    train_acc : 0.038356455828546854\n",
            "train loss : 0.00031895255560156454    train_acc : 0.04002045677644189\n",
            "train loss : 0.00025995884373725505    train_acc : 0.045021344243614106\n",
            "train loss : 0.00038459753626735976    train_acc : 0.03835734471692993\n",
            "train loss : 0.0002413499665558823    train_acc : 0.04668712391718236\n",
            "train loss : 0.0002729465542005062    train_acc : 0.04169156646608916\n",
            "train loss : 0.00029341638194072336    train_acc : 0.04168890216878191\n",
            "train loss : 0.0003097530272807362    train_acc : 0.04168890074782335\n",
            "train loss : 0.00037510368962114155    train_acc : 0.038355567413732174\n",
            "train loss : 0.0004949460409680421    train_acc : 0.035020456302620656\n",
            "train loss : 0.0003959498665058665    train_acc : 0.0416853442433614\n",
            "train loss : 0.000279937265282807    train_acc : 0.043355565516929796\n",
            "train loss : 0.0004187918525685024    train_acc : 0.0400231229682757\n",
            "train loss : 0.0003395983429390457    train_acc : 0.03668801233224975\n",
            "train loss : 0.00021330299424339075    train_acc : 0.0466862336065772\n",
            "train loss : 0.0003993424933881082    train_acc : 0.038358232657923506\n",
            "train loss : 0.0003805144899771866    train_acc : 0.03835379105741756\n",
            "train loss : 0.0002856579772246513    train_acc : 0.04168712202189729\n",
            "train loss : 0.0003500501488236116    train_acc : 0.040022233131745015\n",
            "train loss : 0.0005177549319889591    train_acc : 0.03335467852433693\n",
            "train loss : 0.00037236215230080423    train_acc : 0.038351122495212976\n",
            "train loss : 0.00030334578197032216    train_acc : 0.04168712059866411\n",
            "train loss : 0.0003075238004591244    train_acc : 0.04168889979765262\n",
            "train loss : 0.00037670384703399165    train_acc : 0.03835556741322541\n",
            "train loss : 0.0004435417824366799    train_acc : 0.03668712296928705\n",
            "train loss : 0.00032726616529747535    train_acc : 0.04001956646558362\n",
            "train loss : 0.0003394450701123774    train_acc : 0.04335467710211498\n",
            "train loss : 0.00034260984876192127    train_acc : 0.04168978916112113\n",
            "train loss : 0.00036563097761269905    train_acc : 0.0383555678875526\n",
            "train loss : 0.00044843296273192404    train_acc : 0.03668712296954003\n",
            "train loss : 0.00029535453976558916    train_acc : 0.04001956646558375\n",
            "train loss : 0.00042500464842828837    train_acc : 0.03835467710211498\n",
            "train loss : 0.00023111471795360836    train_acc : 0.04835378916112113\n",
            "train loss : 0.00034339769559046067    train_acc : 0.0400257886875526\n",
            "train loss : 0.00027044115884503755    train_acc : 0.04502134708730003\n",
            "train loss : 0.0002208897352135097    train_acc : 0.04335734471844656\n",
            "train loss : 0.00028635825839995894    train_acc : 0.043356457250516504\n",
            "train loss : 0.0005086685592128296    train_acc : 0.0366897901105336\n",
            "train loss : 0.0002563080324265265    train_acc : 0.04501956788805895\n",
            "train loss : 0.0003196220123043872    train_acc : 0.04502401043620697\n",
            "train loss : 0.00028602779285236314    train_acc : 0.04002401280556598\n",
            "train loss : 0.0002741699775079946    train_acc : 0.041688012806829634\n",
            "train loss : 0.00035190403648800424    train_acc : 0.03835556694016364\n",
            "train loss : 0.00029159628454944074    train_acc : 0.041687122969034755\n",
            "train loss : 0.0003379725193611998    train_acc : 0.04002223313225015\n",
            "train loss : 0.00039667766055931684    train_acc : 0.040021345191003865\n",
            "train loss : 0.0008149196353528842    train_acc : 0.03168801138410187\n",
            "train loss : 0.00024440312648598214    train_acc : 0.04501690027273819\n",
            "train loss : 0.0005353752900455515    train_acc : 0.035024009013478796\n",
            "train loss : 0.00033087082248948915    train_acc : 0.040018679471473854\n",
            "train loss : 0.00044788697850726457    train_acc : 0.04335467662905145\n",
            "train loss : 0.00033131368308328006    train_acc : 0.04002312249420216\n",
            "train loss : 0.00044456626641670367    train_acc : 0.03668801233199691\n",
            "train loss : 0.0003439220614203767    train_acc : 0.04335290027324373\n",
            "train loss : 0.0004530214009193298    train_acc : 0.03668978821347907\n",
            "train loss : 0.0005432025412844291    train_acc : 0.033352901220380524\n",
            "train loss : 0.00041763055885209654    train_acc : 0.040017788213984204\n",
            "train loss : 0.00024012462864221135    train_acc : 0.043354676153714126\n",
            "train loss : 0.00018718368602630776    train_acc : 0.046689789160615315\n",
            "train loss : 0.0005831823482873326    train_acc : 0.030024901220885664\n",
            "train loss : 0.0005227012690297637    train_acc : 0.0366826799473178\n",
            "train loss : 0.00022057448934218046    train_acc : 0.04668623076263857\n",
            "train loss : 0.00041174534561957623    train_acc : 0.03335823265640674\n",
            "train loss : 0.00021431404560726336    train_acc : 0.04501779105741675\n",
            "train loss : 0.0004497152517011956    train_acc : 0.04002400948856396\n",
            "train loss : 0.00037034740352364493    train_acc : 0.036688012805060566\n",
            "train loss : 0.0004341388511143988    train_acc : 0.03668623360682936\n",
            "train loss : 0.0003355934029032115    train_acc : 0.04001956599125697\n",
            "train loss : 0.0006994089865051486    train_acc : 0.03002134376852867\n",
            "train loss : 0.00037006458077872397    train_acc : 0.03501601138334321\n",
            "train loss : 0.000474416467456998    train_acc : 0.04001867520607112\n",
            "train loss : 0.0004469821735238547    train_acc : 0.03502134329344324\n",
            "train loss : 0.0005239364661049321    train_acc : 0.038352011383089836\n",
            "train loss : 0.0003025006674214083    train_acc : 0.045020454406070984\n",
            "train loss : 0.000487042875895102    train_acc : 0.0333573442423499\n",
            "train loss : 0.0005425859840243706    train_acc : 0.03335112391692925\n",
            "train loss : 0.0002551000462294435    train_acc : 0.04501778726608903\n",
            "train loss : 0.00022596946934309644    train_acc : 0.04669067615320858\n",
            "train loss : 0.0004412366911124094    train_acc : 0.036691568360615046\n",
            "train loss : 0.0007093483201113992    train_acc : 0.025019568836458995\n",
            "train loss : 0.00030638153998011736    train_acc : 0.036680010436712776\n",
            "train loss : 0.00025714805060306103    train_acc : 0.04168622933889958\n",
            "train loss : 0.0004411168488944037    train_acc : 0.03502223265564741\n",
            "train loss : 0.00038239837368822543    train_acc : 0.03668534519074968\n",
            "train loss : 0.0003230765527125491    train_acc : 0.04001956551743507\n",
            "train loss : 0.0003196254833574102    train_acc : 0.0383546771016093\n",
            "train loss : 0.0002511557078607366    train_acc : 0.04668712249445419\n",
            "train loss : 0.00037912688870622035    train_acc : 0.040024899798663714\n",
            "train loss : 0.0004301896951581075    train_acc : 0.03668801327989262\n",
            "train loss : 0.00041968413624562077    train_acc : 0.03835290027374928\n",
            "train loss : 0.0002538684727797688    train_acc : 0.043353788213479336\n",
            "train loss : 0.00031027695993434323    train_acc : 0.04502312202038052\n",
            "train loss : 0.0005248939699674175    train_acc : 0.040024012331744206\n",
            "train loss : 0.00023747046024040353    train_acc : 0.04168801280657693\n",
            "train loss : 0.0002702968627041204    train_acc : 0.04502223360683017\n",
            "train loss : 0.0003071237509679702    train_acc : 0.04002401185792364\n",
            "train loss : 0.0002917327143830846    train_acc : 0.041688012806324226\n",
            "train loss : 0.00035017542720678556    train_acc : 0.041688900273496705\n",
            "train loss : 0.0003973880122114862    train_acc : 0.040022234080145866\n",
            "train loss : 0.00033151121344619376    train_acc : 0.040021345191509405\n",
            "train loss : 0.0004291682826752475    train_acc : 0.0383546780507688\n",
            "train loss : 0.00019351818558307622    train_acc : 0.048353789161627074\n",
            "train loss : 0.000320594746157311    train_acc : 0.04669245535421953\n",
            "train loss : 0.00023781192103401014    train_acc : 0.04669156930952225\n",
            "train loss : 0.0005844285272587637    train_acc : 0.03502490217029841\n",
            "train loss : 0.0003344849930966181    train_acc : 0.040018679947824154\n",
            "train loss : 0.00037873686876226874    train_acc : 0.041688009962638835\n",
            "train loss : 0.00028361142954045574    train_acc : 0.045022233605313405\n",
            "train loss : 0.00030956335735999647    train_acc : 0.04335734519125616\n",
            "train loss : 0.0003153971548497554    train_acc : 0.04002312391743534\n",
            "train loss : 0.00045717215494132674    train_acc : 0.03668801233275597\n",
            "train loss : 0.0004607083662666523    train_acc : 0.033352900273244133\n",
            "train loss : 0.00036697757732944054    train_acc : 0.03668445488014573\n",
            "train loss : 0.00032327880420945525    train_acc : 0.03835289837593608\n",
            "train loss : 0.0002846449444556695    train_acc : 0.043353788212467166\n",
            "train loss : 0.0004026078398932279    train_acc : 0.03668978868704665\n",
            "train loss : 0.00040312536998409214    train_acc : 0.04501956788729976\n",
            "train loss : 0.00027169444788002344    train_acc : 0.04502401043620656\n",
            "train loss : 0.0003948218115627951    train_acc : 0.03835734613889931\n",
            "train loss : 0.0002018342657348029    train_acc : 0.04835379058460741\n",
            "train loss : 0.0003969531764053483    train_acc : 0.04169245535497846\n",
            "train loss : 0.00025052365522964604    train_acc : 0.043355569309522654\n",
            "train loss : 0.00036934093123998314    train_acc : 0.03835645630363175\n",
            "train loss : 0.00036248024344701936    train_acc : 0.04002045677669527\n",
            "train loss : 0.0002554848006356571    train_acc : 0.043354677576947574\n",
            "train loss : 0.0003045425517935747    train_acc : 0.0400231224947077\n",
            "train loss : 0.0004350671569879097    train_acc : 0.03668801233199717\n",
            "train loss : 0.00039377041548624967    train_acc : 0.0400195669399104\n",
            "train loss : 0.00043512291499185634    train_acc : 0.03668801043570128\n",
            "train loss : 0.0003150388129911857    train_acc : 0.04335290027223237\n",
            "train loss : 0.0004225157252697607    train_acc : 0.03502312154681185\n",
            "train loss : 0.0004014100595342715    train_acc : 0.0400186789981583\n",
            "train loss : 0.00039882540840382194    train_acc : 0.03835467662879902\n",
            "train loss : 0.00028613850847953416    train_acc : 0.04168712249420203\n",
            "train loss : 0.00026186601662835053    train_acc : 0.041688899798663574\n",
            "train loss : 0.0003905698897909811    train_acc : 0.03668890074655928\n",
            "train loss : 0.0003262198980777779    train_acc : 0.04168623408039816\n",
            "train loss : 0.0005036444365721877    train_acc : 0.04002223265817621\n",
            "train loss : 0.0003700604995568456    train_acc : 0.040021345190751026\n",
            "train loss : 0.0004241106855053626    train_acc : 0.0383546780507684\n",
            "train loss : 0.0003141410317922793    train_acc : 0.043353789161627077\n",
            "train loss : 0.00029017611859464144    train_acc : 0.04335645535421954\n",
            "train loss : 0.0004451447279427752    train_acc : 0.04168979010952225\n",
            "train loss : 0.00035889575606193406    train_acc : 0.040022234554725075\n",
            "train loss : 0.000358081317635248    train_acc : 0.03668801185842919\n",
            "train loss : 0.00022181954725937944    train_acc : 0.043352900272991164\n",
            "train loss : 0.0001539774581418798    train_acc : 0.050023121546812266\n",
            "train loss : 0.0004824889918350975    train_acc : 0.03669334566482497\n",
            "train loss : 0.000345350641315258    train_acc : 0.04001956978435458\n",
            "train loss : 0.00040474325886742543    train_acc : 0.043354677103884985\n",
            "train loss : 0.00040753575849782823    train_acc : 0.038356455827788745\n",
            "train loss : 0.00037679612844619887    train_acc : 0.03835379010977483\n",
            "train loss : 0.00023004292138528525    train_acc : 0.045020455354725215\n",
            "train loss : 0.00026598852411210863    train_acc : 0.04335734424285586\n",
            "train loss : 0.00043942561298759933    train_acc : 0.036689790583596185\n",
            "train loss : 0.00045537841601133426    train_acc : 0.038352901221644586\n",
            "train loss : 0.00019784059467465592    train_acc : 0.04668712154731821\n",
            "train loss : 0.00028946988469411293    train_acc : 0.043358233131491904\n",
            "train loss : 0.00029723801608978935    train_acc : 0.045023124391003456\n",
            "train loss : 0.0003184224391428423    train_acc : 0.03835734566634187\n",
            "train loss : 0.00045015799973935274    train_acc : 0.03668712391768872\n",
            "train loss : 0.0005109571265537184    train_acc : 0.03501956646608943\n",
            "train loss : 0.00040071701806637227    train_acc : 0.03835201043544858\n",
            "train loss : 0.0003661430332173339    train_acc : 0.03835378773889891\n",
            "train loss : 0.00012407619510508162    train_acc : 0.04502045535346075\n",
            "train loss : 0.0005381871431072787    train_acc : 0.03335734424285518\n",
            "train loss : 0.00031710492076709535    train_acc : 0.043351123916929524\n",
            "train loss : 0.0003876335505724072    train_acc : 0.0383564539327557\n",
            "train loss : 0.0004766293338714218    train_acc : 0.038353790108764135\n",
            "train loss : 0.00025992511155720704    train_acc : 0.045020455354724674\n",
            "train loss : 0.0004792813879566326    train_acc : 0.03502401090952252\n",
            "train loss : 0.00042844368640170184    train_acc : 0.03835201280581841\n",
            "train loss : 0.00044199565544988625    train_acc : 0.038353787740163105\n",
            "train loss : 0.0005696131851381474    train_acc : 0.03502045535346142\n",
            "train loss : 0.00035720941300212576    train_acc : 0.04335201090952184\n",
            "train loss : 0.00037406816417309984    train_acc : 0.04168978773915174\n",
            "train loss : 0.00029808008900925493    train_acc : 0.04002223455346088\n",
            "train loss : 0.00041431916408661546    train_acc : 0.036688011858428515\n",
            "train loss : 0.00050086428712555    train_acc : 0.03501956693965783\n",
            "train loss : 0.0005053204570875737    train_acc : 0.043352010435701145\n",
            "train loss : 0.0001537680266843725    train_acc : 0.046689787738899036\n",
            "train loss : 0.00030550396599444    train_acc : 0.04002490122012741\n",
            "train loss : 0.0003513334117261997    train_acc : 0.040021346613984064\n",
            "train loss : 0.00023825231859005443    train_acc : 0.04668801138486079\n",
            "train loss : 0.0002770276136592702    train_acc : 0.04669156693940526\n",
            "train loss : 0.0003762024482762758    train_acc : 0.04002490216903435\n",
            "train loss : 0.00036472573692749216    train_acc : 0.04002134661449015\n",
            "train loss : 0.0004208869006623965    train_acc : 0.041688011384861064\n",
            "train loss : 0.0003092895288608546    train_acc : 0.040022233606071926\n",
            "train loss : 0.00043543208494924667    train_acc : 0.0383546785245899\n",
            "train loss : 0.00033249846579686877    train_acc : 0.04002045582854644\n",
            "train loss : 0.0003466720389868527    train_acc : 0.04168801090977522\n",
            "train loss : 0.0003307294588622806    train_acc : 0.03835556693915188\n",
            "train loss : 0.00046312197747449543    train_acc : 0.041687122969034214\n",
            "train loss : 0.0005336924202426414    train_acc : 0.03668889979891682\n",
            "train loss : 0.0006270444193843833    train_acc : 0.03001956741322609\n",
            "train loss : 0.0003550625184270734    train_acc : 0.041682677102620386\n",
            "train loss : 0.0002751282043636689    train_acc : 0.04335556409445473\n",
            "train loss : 0.00035983307791749006    train_acc : 0.043356456300850374\n",
            "train loss : 0.000245186853322098    train_acc : 0.04502312344336046\n",
            "train loss : 0.0003735391156768336    train_acc : 0.04002401233250313\n",
            "train loss : 0.0004578496915953369    train_acc : 0.04335467947324401\n",
            "train loss : 0.0004391641439810253    train_acc : 0.03668978916238573\n",
            "train loss : 0.000387568599437174    train_acc : 0.041686234554219936\n",
            "train loss : 0.0004442666791950091    train_acc : 0.036688899325095586\n",
            "train loss : 0.0005036034443549619    train_acc : 0.035019567412973386\n",
            "train loss : 0.000427588973482904    train_acc : 0.03668534376928692\n",
            "train loss : 0.0004035951535723159    train_acc : 0.036686232183343624\n",
            "train loss : 0.0005067399407408408    train_acc : 0.035019565990497784\n",
            "train loss : 0.0006239625195007844    train_acc : 0.03168534376852827\n",
            "train loss : 0.0004237118515639486    train_acc : 0.038350232183343214\n",
            "train loss : 0.0004328245705049975    train_acc : 0.04002045345716445\n",
            "train loss : 0.0002696312774438689    train_acc : 0.04335467757517715\n",
            "train loss : 0.00043021344064655406    train_acc : 0.035023122494706765\n",
            "train loss : 0.00042953252105140145    train_acc : 0.043352012331997175\n",
            "train loss : 0.00010676260343820902    train_acc : 0.0516897877399104\n",
            "train loss : 0.00019033704373863875    train_acc : 0.04669423455346129\n",
            "train loss : 0.0004439885878784054    train_acc : 0.03335823692509518\n",
            "train loss : 0.00022213798816809246    train_acc : 0.045017791059693384\n",
            "train loss : 0.00039518130165765446    train_acc : 0.04169067615523184\n",
            "train loss : 0.000337402949080692    train_acc : 0.043355568360616124\n",
            "train loss : 0.00042272054933407353    train_acc : 0.040023122969792324\n",
            "train loss : 0.0005258825007745048    train_acc : 0.03668801233225056\n",
            "train loss : 0.0003340806911565817    train_acc : 0.043352900273243865\n",
            "train loss : 0.00038972472966248076    train_acc : 0.03668978821347907\n",
            "train loss : 0.00043237220433457973    train_acc : 0.03668623455371386\n",
            "train loss : 0.00034199733740058    train_acc : 0.04335289932509531\n",
            "train loss : 0.0005330179139201162    train_acc : 0.03335645487964005\n",
            "train loss : 0.0004250339049235569    train_acc : 0.038351123442602476\n",
            "train loss : 0.0005251801454426519    train_acc : 0.03002045393250272\n",
            "train loss : 0.0005072340984579405    train_acc : 0.035016010908764\n",
            "train loss : 0.00026713307622376744    train_acc : 0.04335200853915134\n",
            "train loss : 0.00029699299333319824    train_acc : 0.04168978773788754\n",
            "train loss : 0.000452653685111988    train_acc : 0.03835556788679354\n",
            "train loss : 0.0004966105228630167    train_acc : 0.03502045630287296\n",
            "train loss : 0.0005248852012001656    train_acc : 0.036685344243361534\n",
            "train loss : 0.0003079540606306629    train_acc : 0.04168623218359646\n",
            "train loss : 0.0002938370899134724    train_acc : 0.04168889932383125\n",
            "train loss : 0.000526715907043657    train_acc : 0.03668890074630604\n",
            "train loss : 0.0003993028585606609    train_acc : 0.04001956741373137\n",
            "train loss : 0.00039376583704336455    train_acc : 0.04502134376928733\n",
            "train loss : 0.0001423873518149534    train_acc : 0.04835734471667696\n",
            "train loss : 0.0003822333610330285    train_acc : 0.03835912391718223\n",
            "train loss : 0.0003800757984053439    train_acc : 0.03668712486608917\n",
            "train loss : 0.0003772015448813826    train_acc : 0.04001956646659525\n",
            "train loss : 0.0005718069201086047    train_acc : 0.03835467710211552\n",
            "train loss : 0.00033083840459852083    train_acc : 0.043353789161121134\n",
            "train loss : 0.0004303315018773093    train_acc : 0.0416897886875526\n",
            "train loss : 0.0006923939947941653    train_acc : 0.031688901220633364\n",
            "train loss : 0.0003659093679939195    train_acc : 0.04168356741398434\n",
            "train loss : 0.00027667369653394246    train_acc : 0.04335556456928746\n",
            "train loss : 0.00031140911783753293    train_acc : 0.04168978963443695\n",
            "train loss : 0.00028719388891965006    train_acc : 0.04335556788780503\n",
            "train loss : 0.00043695228302981963    train_acc : 0.04168978963620683\n",
            "train loss : 0.0003943927714930879    train_acc : 0.04168890122113931\n",
            "train loss : 0.0003023118277459519    train_acc : 0.04335556741398461\n",
            "train loss : 0.000336820505486598    train_acc : 0.04335645630262079\n",
            "train loss : 0.00023500596397316544    train_acc : 0.045023123443361394\n",
            "train loss : 0.0004038843391996659    train_acc : 0.04169067899916979\n",
            "train loss : 0.0004253507631555484    train_acc : 0.040022235028799556\n",
            "train loss : 0.0005144868279021334    train_acc : 0.03668801185868203\n",
            "train loss : 0.0004200450788065869    train_acc : 0.036686233606324635\n",
            "train loss : 0.00043258101739937065    train_acc : 0.0400195659912567\n",
            "train loss : 0.00035971906996302316    train_acc : 0.041688010435195336\n",
            "train loss : 0.00024160622229690834    train_acc : 0.04335556693889877\n",
            "train loss : 0.00031359677250475626    train_acc : 0.04168978963570075\n",
            "train loss : 0.0003603087196600982    train_acc : 0.040022234554472375\n",
            "train loss : 0.0003949691129435355    train_acc : 0.03668801185842905\n",
            "train loss : 0.0002878633334516265    train_acc : 0.043352900272991164\n",
            "train loss : 0.00028990394418474194    train_acc : 0.0433564548801456\n",
            "train loss : 0.0003602334906606941    train_acc : 0.04168979010926941\n",
            "train loss : 0.0004079660642968257    train_acc : 0.03835556788805828\n",
            "train loss : 0.0003441857190062643    train_acc : 0.04002045630287363\n",
            "train loss : 0.0002886790109682682    train_acc : 0.04335467757669487\n",
            "train loss : 0.0003641605768242417    train_acc : 0.04168978916137424\n",
            "train loss : 0.0004245399348149638    train_acc : 0.04335556788755273\n",
            "train loss : 0.00045089815769790067    train_acc : 0.03502312296954003\n",
            "train loss : 0.00030074157129242264    train_acc : 0.04501867899891709\n",
            "train loss : 0.00040284750004773945    train_acc : 0.03669067662879942\n",
            "train loss : 0.00046497986029063416    train_acc : 0.03501956836086869\n",
            "train loss : 0.00034970077093606774    train_acc : 0.03835201043645913\n",
            "train loss : 0.00025876512544534557    train_acc : 0.041687121072232776\n",
            "train loss : 0.0002467040652721044    train_acc : 0.04668889979790519\n",
            "train loss : 0.0004526319506133762    train_acc : 0.036691567413225545\n",
            "train loss : 0.0004120182006837191    train_acc : 0.04168623550262039\n",
            "train loss : 0.00040160749811645515    train_acc : 0.03335556599226806\n",
            "train loss : 0.0002770134562092819    train_acc : 0.04501778963519588\n",
            "train loss : 0.00031390538262016706    train_acc : 0.04335734282113877\n",
            "train loss : 0.0003943238721977165    train_acc : 0.04002312391617127\n",
            "train loss : 0.00039559890927562137    train_acc : 0.03835467899942196\n",
            "train loss : 0.00038021628215631747    train_acc : 0.03835378916213303\n",
            "train loss : 0.00040752569741225094    train_acc : 0.0400204553542198\n",
            "train loss : 0.00027938878207686853    train_acc : 0.041688010909522254\n",
            "train loss : 0.00036823200753354645    train_acc : 0.04335556693915175\n",
            "train loss : 0.00015676841443152442    train_acc : 0.05002312296903422\n",
            "train loss : 0.0005349220955388035    train_acc : 0.036693345665583486\n",
            "train loss : 0.00023217811595726098    train_acc : 0.04668623645102164\n",
            "train loss : 0.00038439341552740374    train_acc : 0.04002489932610721\n",
            "train loss : 0.00032600682739770514    train_acc : 0.04002134661297393\n",
            "train loss : 0.0002686908876724474    train_acc : 0.046688011384860256\n",
            "train loss : 0.00041470695766003014    train_acc : 0.04002490027273859\n",
            "train loss : 0.00028604286919292973    train_acc : 0.04335467994681213\n",
            "train loss : 0.0007061813037554966    train_acc : 0.028356455829304964\n",
            "train loss : 0.0002545388144260492    train_acc : 0.046681790109775634\n",
            "train loss : 0.00038067844558302595    train_acc : 0.03669156362139188\n",
            "train loss : 0.00045780991632169375    train_acc : 0.03835290216726474\n",
            "train loss : 0.0003677530535333338    train_acc : 0.04335378821448921\n",
            "train loss : 0.00045481706849459925    train_acc : 0.04002312202038106\n",
            "train loss : 0.00031972158953980193    train_acc : 0.043354678998410875\n",
            "train loss : 0.00014641596597568423    train_acc : 0.05002312249546582\n",
            "train loss : 0.0002931717698506362    train_acc : 0.04502667899866424\n",
            "train loss : 0.0003744155440682693    train_acc : 0.038357347562132625\n",
            "train loss : 0.000510394879991423    train_acc : 0.03335379058536647\n",
            "train loss : 0.0004702094715483001    train_acc : 0.03335112202164553\n",
            "train loss : 0.0004270843762038362    train_acc : 0.04001778726507821\n",
            "train loss : 0.00045012042639509196    train_acc : 0.03835467615320804\n",
            "train loss : 0.00026860198235892114    train_acc : 0.041687122493948375\n",
            "train loss : 0.0003657302027853668    train_acc : 0.036688899798663445\n",
            "train loss : 0.00041954088003189895    train_acc : 0.04001956741322595\n",
            "train loss : 0.0003258881482431134    train_acc : 0.04002134376928705\n",
            "train loss : 0.00040746191329494837    train_acc : 0.04168801138334362\n",
            "train loss : 0.0002937584265559871    train_acc : 0.04335556693940445\n",
            "Looked at 51200/60000 samples\n",
            "train loss : 0.0002991295003437726    train_acc : 0.04168978963570102\n",
            "train loss : 0.0003412385482864138    train_acc : 0.040022234554472375\n",
            "train loss : 0.0001730279615030965    train_acc : 0.050021345191762386\n",
            "train loss : 0.00036850703326810763    train_acc : 0.03836001138410228\n",
            "train loss : 0.00031907261282903854    train_acc : 0.04168712533940485\n",
            "train loss : 0.000370722171470169    train_acc : 0.036688899800181016\n",
            "train loss : 0.00033420339221785047    train_acc : 0.04168623407989343\n",
            "train loss : 0.000577132384061787    train_acc : 0.03835556599150927\n",
            "train loss : 0.00021882062513567605    train_acc : 0.0466871229685288\n",
            "train loss : 0.000247953071622631    train_acc : 0.04502489979891655\n",
            "train loss : 0.00026578578828940314    train_acc : 0.04335734661322609\n",
            "train loss : 0.0004795052574659939    train_acc : 0.03502312391819372\n",
            "train loss : 0.0003041208509851342    train_acc : 0.04001867899942304\n",
            "train loss : 0.00017375905676546356    train_acc : 0.04835467662879969\n",
            "train loss : 0.000397537301166131    train_acc : 0.03835912249420203\n",
            "train loss : 0.0005420950434738707    train_acc : 0.03168712486533024\n",
            "train loss : 0.00021438246872065514    train_acc : 0.04668356646659484\n",
            "train loss : 0.0004069707788144537    train_acc : 0.038358231235448854\n",
            "train loss : 0.00047705281056282866    train_acc : 0.03668712438999224\n",
            "train loss : 0.00031886948324986206    train_acc : 0.04335289979967467\n",
            "train loss : 0.0005051882343990419    train_acc : 0.03502312154655982\n",
            "train loss : 0.00026143638776554015    train_acc : 0.04501867899815817\n",
            "train loss : 0.00036683734813524254    train_acc : 0.03835734329546569\n",
            "train loss : 0.0004911859038488558    train_acc : 0.03502045724975758\n",
            "train loss : 0.0002645853116354951    train_acc : 0.04168534424386654\n",
            "train loss : 0.00041636327123399533    train_acc : 0.03835556551693007\n",
            "train loss : 0.0005010102086555468    train_acc : 0.03502045630160903\n",
            "train loss : 0.0005568310278730017    train_acc : 0.03501867757669419\n",
            "train loss : 0.0005578334196227432    train_acc : 0.030018676628040905\n",
            "train loss : 0.00038690115266714194    train_acc : 0.04001600996086829\n",
            "train loss : 0.00029107358055778965    train_acc : 0.04502134187197913\n",
            "train loss : 0.0004432596017307571    train_acc : 0.03835734471566505\n",
            "train loss : 0.00027457054971320824    train_acc : 0.043353790583848355\n",
            "train loss : 0.0004032480408166179    train_acc : 0.03502312202164472\n",
            "train loss : 0.0003217674693896725    train_acc : 0.04001867899841154\n",
            "train loss : 0.00027107115943704365    train_acc : 0.04502134329546582\n",
            "train loss : 0.00018747165423689668    train_acc : 0.04835734471642424\n",
            "train loss : 0.0004320077340582851    train_acc : 0.04002579058384876\n",
            "train loss : 0.00036619357703742875    train_acc : 0.043354680421644726\n",
            "train loss : 0.00025424415934053327    train_acc : 0.04335645582955821\n",
            "train loss : 0.000292831977500216    train_acc : 0.045023123443109096\n",
            "train loss : 0.0002116508453794552    train_acc : 0.04669067899916966\n",
            "train loss : 0.00028977009175457997    train_acc : 0.04169156836213289\n",
            "train loss : 0.00034813221408270856    train_acc : 0.04168890216979314\n",
            "train loss : 0.00045862339939601335    train_acc : 0.040022234081157224\n",
            "train loss : 0.0002531319276121242    train_acc : 0.041688011858176624\n",
            "train loss : 0.00043418813861293933    train_acc : 0.03668890027299102\n",
            "train loss : 0.00020044604836102    train_acc : 0.04501956741347893\n",
            "train loss : 0.0004712861673561636    train_acc : 0.03502401043595385\n",
            "train loss : 0.0003921788174256498    train_acc : 0.03668534613889917\n",
            "train loss : 0.00034486505726911466    train_acc : 0.038352898851274075\n",
            "train loss : 0.00034637342693191075    train_acc : 0.041687121546054015\n",
            "train loss : 0.00044440126996994964    train_acc : 0.031688899798157893\n",
            "train loss : 0.0006169998746266989    train_acc : 0.031683567413225686\n",
            "train loss : 0.00031258947182237576    train_acc : 0.041683564569287054\n",
            "train loss : 0.00014417663831990847    train_acc : 0.048355564567770286\n",
            "train loss : 0.000401098279399975    train_acc : 0.045025789634436146\n",
            "train loss : 0.00019112196961945603    train_acc : 0.048357347087805036\n",
            "train loss : 0.0003267109392346271    train_acc : 0.04002579058511349\n",
            "train loss : 0.00046474642117808984    train_acc : 0.040021347088312065\n",
            "train loss : 0.0004726444610307481    train_acc : 0.0350213447184471\n",
            "train loss : 0.0003402006676424    train_acc : 0.04168534471718317\n",
            "train loss : 0.00036582525952925303    train_acc : 0.04002223218384916\n",
            "train loss : 0.000545970774631417    train_acc : 0.03502134519049805\n",
            "train loss : 0.0004374738333897806    train_acc : 0.03501867805076827\n",
            "train loss : 0.0004575531512259524    train_acc : 0.036685343294960406\n",
            "train loss : 0.0003678722530380757    train_acc : 0.04001956551642398\n",
            "train loss : 0.00043792755992411376    train_acc : 0.04002134376827543\n",
            "train loss : 0.0001770410602971776    train_acc : 0.04668801138334308\n",
            "train loss : 0.0004767843645766409    train_acc : 0.036691566939404445\n",
            "train loss : 0.0003417784409290926    train_acc : 0.041686235502367684\n",
            "train loss : 0.00028932625741092717    train_acc : 0.04168889932560126\n",
            "train loss : 0.0003059018163399714    train_acc : 0.04502223407964032\n",
            "train loss : 0.00031706321019641645    train_acc : 0.04169067852484248\n",
            "train loss : 0.0005002012709923782    train_acc : 0.03502223502854658\n",
            "train loss : 0.0002691152249897929    train_acc : 0.046685345192015226\n",
            "train loss : 0.00039964596443998156    train_acc : 0.038358232184102406\n",
            "train loss : 0.0002664452376896203    train_acc : 0.04502045772383152\n",
            "train loss : 0.00048801575372393576    train_acc : 0.033357344244119375\n",
            "train loss : 0.00036690793182727905    train_acc : 0.04168445725026353\n",
            "train loss : 0.0006603125792701843    train_acc : 0.025022231710533475\n",
            "train loss : 0.00038349425655269414    train_acc : 0.03501334519024562\n",
            "train loss : 0.0002542444536201777    train_acc : 0.0433520071174348\n",
            "train loss : 0.00021046834036875367    train_acc : 0.045023121070462635\n",
            "train loss : 0.00032922255429609703    train_acc : 0.03835734566457091\n",
            "train loss : 0.00043485445778864215    train_acc : 0.03835379058435443\n",
            "train loss : 0.0004222431070698701    train_acc : 0.03502045535497832\n",
            "train loss : 0.0002923111300090733    train_acc : 0.04168534424285599\n",
            "train loss : 0.00030108204418714417    train_acc : 0.041688898850262855\n",
            "train loss : 0.00024575423617328166    train_acc : 0.046688900746053474\n",
            "train loss : 0.00044562422663063027    train_acc : 0.03502490074706456\n",
            "train loss : 0.0003708862092941607    train_acc : 0.0400186799470651\n",
            "train loss : 0.000449686223285826    train_acc : 0.036688009962638435\n",
            "train loss : 0.00037133123025577194    train_acc : 0.040019566938646736\n",
            "train loss : 0.000563952188627165    train_acc : 0.036688010435700615\n",
            "train loss : 0.000407086326207551    train_acc : 0.03835290027223237\n",
            "train loss : 0.0002618457722735542    train_acc : 0.04502045488014519\n",
            "train loss : 0.0002168066833203672    train_acc : 0.04502401090926941\n",
            "train loss : 0.00039691436807493555    train_acc : 0.04002401280581828\n",
            "train loss : 0.0005174135563560072    train_acc : 0.041688012806829766\n",
            "train loss : 0.000746768594074132    train_acc : 0.02502223360683031\n",
            "train loss : 0.0003763455612819925    train_acc : 0.040013345191256974\n",
            "train loss : 0.0005173904428727553    train_acc : 0.036688007117435334\n",
            "train loss : 0.000537517241516635    train_acc : 0.0300195669371293\n",
            "train loss : 0.0003183800710952965    train_acc : 0.04168267710236647\n",
            "train loss : 0.00045083216130647216    train_acc : 0.0383555640944546\n",
            "train loss : 0.0003311969664430614    train_acc : 0.040020456300850375\n",
            "train loss : 0.00026657966178399915    train_acc : 0.04668801091002712\n",
            "train loss : 0.0004274279742103245    train_acc : 0.03169156693915201\n",
            "train loss : 0.00027002725764744347    train_acc : 0.04501690216903422\n",
            "train loss : 0.00040677119272955063    train_acc : 0.03669067568115682\n",
            "train loss : 0.0002994717117361387    train_acc : 0.040019568360363286\n",
            "train loss : 0.0005851334985603869    train_acc : 0.03168801043645886\n",
            "train loss : 0.000311818310772393    train_acc : 0.043350233605566106\n",
            "train loss : 0.0003987654826848534    train_acc : 0.04002312012458963\n",
            "train loss : 0.0002714901979648701    train_acc : 0.046688012330733115\n",
            "train loss : 0.0002512089363636379    train_acc : 0.04502490027324306\n",
            "train loss : 0.00029125736829714783    train_acc : 0.043357346613479064\n",
            "train loss : 0.000389343849429433    train_acc : 0.038356457251527185\n",
            "train loss : 0.0004070755038534848    train_acc : 0.03668712344386748\n",
            "train loss : 0.00023837708135148554    train_acc : 0.0466862331325034\n",
            "train loss : 0.0004070372042269161    train_acc : 0.04002489932433734\n",
            "train loss : 0.0004524467134823911    train_acc : 0.038354679946306314\n",
            "train loss : 0.000306484215934772    train_acc : 0.045020455829304695\n",
            "train loss : 0.00036426693445496016    train_acc : 0.03835734424310896\n",
            "train loss : 0.00031344008080294957    train_acc : 0.04335379058359633\n",
            "train loss : 0.0003420471716543416    train_acc : 0.041689788688311254\n",
            "train loss : 0.0003588502438634565    train_acc : 0.04168890122063377\n",
            "train loss : 0.0004521083675401354    train_acc : 0.03835556741398434\n",
            "train loss : 0.0003454794286008717    train_acc : 0.04168712296928746\n",
            "train loss : 0.00032626864697129544    train_acc : 0.04002223313225029\n",
            "train loss : 0.00044861096952960536    train_acc : 0.040021345191003865\n",
            "train loss : 0.00043896711486612216    train_acc : 0.03668801138410187\n",
            "train loss : 0.00021501293646821994    train_acc : 0.045019566939404856\n",
            "train loss : 0.0003663440487695669    train_acc : 0.04169067710236768\n",
            "train loss : 0.00023055202907678354    train_acc : 0.046688901694454596\n",
            "train loss : 0.0005249041246157029    train_acc : 0.03335823408090371\n",
            "train loss : 0.00018317030047982434    train_acc : 0.043351124391509815\n",
            "train loss : 0.0002365315624140401    train_acc : 0.043356453933008805\n",
            "train loss : 0.00035892490835550425    train_acc : 0.04002312344209761\n",
            "train loss : 0.0004264878514835123    train_acc : 0.04002134566583578\n",
            "train loss : 0.0003979998041409084    train_acc : 0.040021344717688444\n",
            "train loss : 0.0002272984450239191    train_acc : 0.0433546780505161\n",
            "train loss : 0.0002469734489157559    train_acc : 0.04502312249496028\n",
            "train loss : 0.0005343191632238618    train_acc : 0.03502401233199731\n",
            "train loss : 0.0006549755855564864    train_acc : 0.030018679473243732\n",
            "train loss : 0.00032785071267679873    train_acc : 0.03501600996238573\n",
            "train loss : 0.00037881321659794914    train_acc : 0.04001867520531328\n",
            "train loss : 0.0003385078249345944    train_acc : 0.04002134329344283\n",
            "train loss : 0.0004971259822494703    train_acc : 0.035021344716423174\n",
            "train loss : 0.0003562496945058    train_acc : 0.04001867805051543\n",
            "train loss : 0.0003679600676878184    train_acc : 0.03835467662829361\n",
            "train loss : 0.0003347542872598957    train_acc : 0.04335378916086842\n",
            "train loss : 0.0004277723072364898    train_acc : 0.03668978868755247\n",
            "train loss : 0.00033949791044862504    train_acc : 0.041686234553966695\n",
            "train loss : 0.0003489318475651787    train_acc : 0.04002223265842878\n",
            "train loss : 0.00039930792188567734    train_acc : 0.038354678524084494\n",
            "train loss : 0.00040997750839411376    train_acc : 0.038353789161879516\n",
            "train loss : 0.00035513179401440524    train_acc : 0.04502045535421967\n",
            "train loss : 0.0004594682096740928    train_acc : 0.03335734424285559\n",
            "train loss : 0.0002892531266975033    train_acc : 0.04168445725026286\n",
            "train loss : 0.00027924356022470087    train_acc : 0.04335556504386681\n",
            "train loss : 0.0003645143168616771    train_acc : 0.04335645630135673\n",
            "train loss : 0.0004243466547466622    train_acc : 0.03668979011002739\n",
            "train loss : 0.0004877513998503864    train_acc : 0.035019567888058684\n",
            "train loss : 0.00040680904739982576    train_acc : 0.038352010436206965\n",
            "train loss : 0.0003164867122414583    train_acc : 0.043353787738899315\n",
            "train loss : 0.0002607587263085812    train_acc : 0.04335645535346074\n",
            "train loss : 0.0002493994174154652    train_acc : 0.04502312344285518\n",
            "train loss : 0.0003300026012387674    train_acc : 0.04169067899916953\n",
            "train loss : 0.0003018098362261945    train_acc : 0.04335556836213289\n",
            "train loss : 0.000455993891678279    train_acc : 0.03502312296979314\n",
            "train loss : 0.0002630697666173098    train_acc : 0.04668534566558389\n",
            "train loss : 0.0003918799902056887    train_acc : 0.038358232184354975\n",
            "train loss : 0.000290232424902654    train_acc : 0.04168712439049832\n",
            "train loss : 0.0004263929142024611    train_acc : 0.04168889979967493\n",
            "train loss : 0.0003527680297222914    train_acc : 0.04168890074655983\n",
            "train loss : 0.00047227237909420474    train_acc : 0.03502223408039817\n",
            "train loss : 0.00022396638920195572    train_acc : 0.045018678524842884\n",
            "train loss : 0.0006402119170026264    train_acc : 0.03335734329521325\n",
            "train loss : 0.00045634056643223213    train_acc : 0.03501779058309078\n",
            "train loss : 0.00031522407926726643    train_acc : 0.03835200948831099\n",
            "train loss : 0.00047855239176642955    train_acc : 0.0366871210717271\n",
            "train loss : 0.0002752885429650619    train_acc : 0.043352899797904916\n",
            "train loss : 0.0005630181813350567    train_acc : 0.033356454879892215\n",
            "train loss : 0.0003358312885805825    train_acc : 0.04001779010926928\n",
            "train loss : 0.0004489105324448927    train_acc : 0.038354676154724945\n",
            "train loss : 0.0002315739303896428    train_acc : 0.048353789160615855\n",
            "train loss : 0.00025394197037538263    train_acc : 0.045025788687552325\n",
            "train loss : 0.0002734460016660199    train_acc : 0.043357347087300026\n",
            "train loss : 0.00044383149664034164    train_acc : 0.040023123918446556\n",
            "train loss : 0.00040806097211477715    train_acc : 0.03502134566608984\n",
            "train loss : 0.0003143839240732301    train_acc : 0.04335201138435525\n",
            "train loss : 0.0005145881171862636    train_acc : 0.033356454406071656\n",
            "train loss : 0.00038740836321490824    train_acc : 0.03668445677568324\n",
            "train loss : 0.0005207622729552543    train_acc : 0.03668623171028037\n",
            "train loss : 0.0005000874182744987    train_acc : 0.03835289932357881\n",
            "train loss : 0.00038405644336176784    train_acc : 0.03668712154630591\n",
            "train loss : 0.00025509085330665816    train_acc : 0.04168623313149136\n",
            "train loss : 0.0005283616293063339    train_acc : 0.03668889932433679\n",
            "train loss : 0.0005030621031818702    train_acc : 0.03668623407963965\n",
            "train loss : 0.00031311209937942813    train_acc : 0.04335289932484247\n",
            "train loss : 0.00040127433615759223    train_acc : 0.04168978821297325\n",
            "train loss : 0.0004416241412548374    train_acc : 0.03835556788704692\n",
            "train loss : 0.0004002310823951927    train_acc : 0.0400204563028731\n",
            "train loss : 0.0005028211200497709    train_acc : 0.03502134424336153\n",
            "train loss : 0.0005464281698398729    train_acc : 0.03168534471692979\n",
            "train loss : 0.0003475739651817922    train_acc : 0.040016898850515695\n",
            "train loss : 0.0003567962749319023    train_acc : 0.040021342346053604\n",
            "train loss : 0.0004956060106968094    train_acc : 0.038354678049251224\n",
            "train loss : 0.0005310535077760174    train_acc : 0.03335378916162627\n",
            "train loss : 0.0002842875035415496    train_acc : 0.0433511220208862\n",
            "train loss : 0.0005039072530381844    train_acc : 0.0366897872650778\n",
            "train loss : 0.0002700415980284433    train_acc : 0.04335290121987471\n",
            "train loss : 0.0006216434652398143    train_acc : 0.030023121547317268\n",
            "train loss : 0.0005081075492442868    train_acc : 0.038349345664825236\n",
            "train loss : 0.0002692681512865989    train_acc : 0.04335378631768791\n",
            "train loss : 0.000327549507456972    train_acc : 0.041689788686036094\n",
            "train loss : 0.000201640522284104    train_acc : 0.04502223455396589\n",
            "train loss : 0.0004272493351808139    train_acc : 0.03669067852509545\n",
            "train loss : 0.0002884710647068718    train_acc : 0.041686235028546716\n",
            "train loss : 0.0004116755721207408    train_acc : 0.03835556599201523\n",
            "train loss : 0.0002988251930871135    train_acc : 0.041687122968529076\n",
            "train loss : 0.0004432731137215507    train_acc : 0.041688899798916546\n",
            "train loss : 0.0002835904086041736    train_acc : 0.04335556741322609\n",
            "train loss : 0.00027239210947483956    train_acc : 0.040023122969287055\n",
            "train loss : 0.00040605099936349726    train_acc : 0.03835467899891695\n",
            "train loss : 0.00018814377483759997    train_acc : 0.05002045582879942\n",
            "train loss : 0.0003675892680088522    train_acc : 0.04002667757644203\n",
            "train loss : 0.00033890877165949077    train_acc : 0.0400213475613741\n",
            "train loss : 0.00041811539159350165    train_acc : 0.036688011385366065\n",
            "train loss : 0.00025603573520144426    train_acc : 0.04335290027273886\n",
            "train loss : 0.00023231702063348112    train_acc : 0.04335645488014546\n",
            "train loss : 0.00040625920717988475    train_acc : 0.03502312344260274\n",
            "train loss : 0.0002327829169740946    train_acc : 0.045018678999169386\n",
            "train loss : 0.0004506558561095932    train_acc : 0.03835734329546622\n",
            "train loss : 0.0005800258875096517    train_acc : 0.030020457249757583\n",
            "train loss : 0.0004202508678309231    train_acc : 0.04168267757719987\n",
            "train loss : 0.00041113435871072726    train_acc : 0.041688897428041174\n",
            "train loss : 0.00035986721324733455    train_acc : 0.038355567411961625\n",
            "train loss : 0.0003668162844965607    train_acc : 0.04168712296928637\n",
            "train loss : 0.0004170283049277406    train_acc : 0.03668889979891696\n",
            "train loss : 0.00039664124088128046    train_acc : 0.03835290074655942\n",
            "train loss : 0.0003525880030241894    train_acc : 0.04002045488039817\n",
            "train loss : 0.0005265977683094579    train_acc : 0.03835467757593621\n",
            "train loss : 0.0003494167914232904    train_acc : 0.041687122494707164\n",
            "train loss : 0.0003023689346626198    train_acc : 0.041688899798663845\n",
            "train loss : 0.00048409283666383504    train_acc : 0.03502223407989262\n",
            "train loss : 0.00042374095058710613    train_acc : 0.04168534519150927\n",
            "train loss : 0.00044075931528269593    train_acc : 0.03668889885076881\n",
            "train loss : 0.00034370211646513965    train_acc : 0.03835290074605374\n",
            "train loss : 0.0003942712899731825    train_acc : 0.03835378821373123\n",
            "train loss : 0.00030008575994474023    train_acc : 0.04335378868704732\n",
            "train loss : 0.0006374375853097635    train_acc : 0.03168978868729976\n",
            "train loss : 0.0005232748803474191    train_acc : 0.03335023455396656\n",
            "train loss : 0.00033786586429897827    train_acc : 0.041684453458428784\n",
            "train loss : 0.0004864548825767635    train_acc : 0.040022231708511165\n",
            "train loss : 0.0006227083002293665    train_acc : 0.033354678523577874\n",
            "train loss : 0.00045396545129324406    train_acc : 0.03668445582854591\n",
            "train loss : 0.00022371080005750287    train_acc : 0.046686231709775226\n",
            "train loss : 0.0003616067646158249    train_acc : 0.03835823265691188\n",
            "train loss : 0.0003308296761448231    train_acc : 0.03835379105741702\n",
            "train loss : 0.0003895752398470526    train_acc : 0.040020455355230626\n",
            "train loss : 0.0002044933633151743    train_acc : 0.04335467757618946\n",
            "train loss : 0.00038372350785976746    train_acc : 0.04168978916137397\n",
            "train loss : 0.00038204504051766845    train_acc : 0.040022234554219396\n",
            "train loss : 0.0003149531253867624    train_acc : 0.041688011858428915\n",
            "train loss : 0.00040103680409159627    train_acc : 0.038355566939657824\n",
            "train loss : 0.0004260331243224524    train_acc : 0.03835378963570115\n",
            "train loss : 0.00044404381592273436    train_acc : 0.03835378868780571\n",
            "train loss : 0.00038112042169612885    train_acc : 0.041687122020633495\n",
            "train loss : 0.00037741950796880107    train_acc : 0.04335556646507767\n",
            "train loss : 0.00025624073500975946    train_acc : 0.04335645630211471\n",
            "train loss : 0.00027857996642301105    train_acc : 0.04502312344336113\n",
            "train loss : 0.0004591102217442179    train_acc : 0.03335734566583646\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023850183933973312    test_acc : 0.22963258785942492\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002060564700514078    test_acc : 0.24035026385897582\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002031908603385091    test_acc : 0.2503685311944376\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016153748147189617    test_acc : 0.2803526151156372\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014803851954638958    test_acc : 0.2704643853518071\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013899413170292974    test_acc : 0.2604487680043189\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015952313551679254    test_acc : 0.2704007947859563\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002412541536614299    test_acc : 0.22051246260315002\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002533992985263467    test_acc : 0.22035307496039347\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023956047371029854    test_acc : 0.27027269353022493\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021555114071816206    test_acc : 0.23049607889306784\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002606368390843272    test_acc : 0.23036899705716635\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016487902030348778    test_acc : 0.25033664216312196\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018410008633509278    test_acc : 0.2504004365564317\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003335622139275074    test_acc : 0.2204485636950685\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018301132367923856    test_acc : 0.2602889730469491\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020117522217333317    test_acc : 0.23046418202251423\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022067539393901825    test_acc : 0.22038486959112624\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020321107003837824    test_acc : 0.2503047439923039\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002719152718782425    test_acc : 0.2404163090862374\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018852886278182268    test_acc : 0.27033679331976435\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002807482611387968    test_acc : 0.22051225812562228\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023084089625626802    test_acc : 0.24032112542532147\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002300393534824252    test_acc : 0.24038441254129497\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0010657976381480694    test_acc : 0.2903047425320808\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002109532244503498    test_acc : 0.23056007904962328\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017558198887854815    test_acc : 0.25033725264872086\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00214093504473567    test_acc : 0.2603844640659704\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002258812543004751    test_acc : 0.23046448710564205\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015479421708732843    test_acc : 0.2603209728022544\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0034209927543997765    test_acc : 0.21049623313994328\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026775896549224854    test_acc : 0.2003530231090733\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002244552131742239    test_acc : 0.23027269336456574\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016973611200228333    test_acc : 0.2503363344835929\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024109873920679092    test_acc : 0.24041641001432457\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017704799538478255    test_acc : 0.2503687425240074\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018497157143428922    test_acc : 0.27036859023170606\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014288841048255563    test_acc : 0.28041651306783294\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025047804228961468    test_acc : 0.23052848726219755\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017589735798537731    test_acc : 0.28028922839380893\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002900957828387618    test_acc : 0.23052808060189717\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002786625875160098    test_acc : 0.24035312485815302\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018642870709300041    test_acc : 0.2503685403350101\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022621541284024715    test_acc : 0.24041651290841853\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.000995682436041534    test_acc : 0.2803208195300589\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023405381944030523    test_acc : 0.24051220709115034\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002825920470058918    test_acc : 0.19046489523032317\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023720311000943184    test_acc : 0.2601931785790106\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016752510564401746    test_acc : 0.2604159526472173\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019495281158015132    test_acc : 0.250432638826349\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002784561365842819    test_acc : 0.21046464101861453\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024060020223259926    test_acc : 0.2203209732939892\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002064734697341919    test_acc : 0.25030453985077955\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001861394732259214    test_acc : 0.25040033399313344\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002252548933029175    test_acc : 0.2603846656038119\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00156783580314368    test_acc : 0.2704005899859547\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025918916799128056    test_acc : 0.2504645386261532\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00316688627935946    test_acc : 0.21046474293490783\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001176059595309198    test_acc : 0.2802251269742329\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001357291010208428    test_acc : 0.2604799524823458\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019055231241509318    test_acc : 0.2704008944168765\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003561629680916667    test_acc : 0.19056038624414337\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024148246739059687    test_acc : 0.23024140698480558\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029821451753377914    test_acc : 0.21040013229068627\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030702140647917986    test_acc : 0.21033674163671146\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017239940352737904    test_acc : 0.270240692465293\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001715171500109136    test_acc : 0.2504640277714546\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024429848417639732    test_acc : 0.2404168179801005\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030174520798027515    test_acc : 0.19046459047277986\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013765126932412386    test_acc : 0.2701772031644498\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001561355427838862    test_acc : 0.26044785048934327\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017913993215188384    test_acc : 0.23046468961817682\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002899648156017065    test_acc : 0.22038487121283762\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025323303416371346    test_acc : 0.23033669287927425\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002379263285547495    test_acc : 0.23036848783667502\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022896837908774614    test_acc : 0.23036858941800853\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017265252536162734    test_acc : 0.2703046919789713\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002412657253444195    test_acc : 0.220512155565428\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031642357353121042    test_acc : 0.20038502286123142\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019000128377228975    test_acc : 0.2602248722775119\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015748519217595458    test_acc : 0.2903681305823563\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002070471877232194    test_acc : 0.26051235824467206\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024146793875843287    test_acc : 0.24044892127234718\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002484824974089861    test_acc : 0.24038482083473592\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024109475780278444    test_acc : 0.25036864160011096\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028336092364042997    test_acc : 0.23043248767284377\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002098719123750925    test_acc : 0.240352819449434\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002101488411426544    test_acc : 0.2403845138001579\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027765899430960417    test_acc : 0.1804804617054318\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028612534515559673    test_acc : 0.21024115163484164\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00398612767457962    test_acc : 0.19036818259308255\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002800086745992303    test_acc : 0.2202567673565274\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015637862961739302    test_acc : 0.2702723858381998\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002367281587794423    test_acc : 0.24048010346913162\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002716942224651575    test_acc : 0.22041686934015695\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002858403604477644    test_acc : 0.23033679510971294\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025334921665489674    test_acc : 0.21040043704507894\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026666747871786356    test_acc : 0.2502728448467894\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017291634576395154    test_acc : 0.26038425829024536\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002396693453192711    test_acc : 0.24044851200731707\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021694209426641464    test_acc : 0.2603528706453908\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0034341022837907076    test_acc : 0.21049633504998527\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025769700296223164    test_acc : 0.2003530234346645\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002725825412198901    test_acc : 0.24025671892471137\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021921799052506685    test_acc : 0.2104321300924112\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0011960439151152968    test_acc : 0.2702409972207425\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021843696013092995    test_acc : 0.23049597762690333\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024416917003691196    test_acc : 0.22038497117452685\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.004150942899286747    test_acc : 0.20038461652132436\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019901238847523928    test_acc : 0.2402568198610905\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026786320377141237    test_acc : 0.24038420709220795\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001997346756979823    test_acc : 0.26035266519837763\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024053589440882206    test_acc : 0.24044841107092135\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015068775974214077    test_acc : 0.2703368958820157\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002374447649344802    test_acc : 0.240480309571508\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025529353879392147    test_acc : 0.23040089555773646\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002147225197404623    test_acc : 0.2603207696343698\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021028011105954647    test_acc : 0.24044830916816096\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024471560027450323    test_acc : 0.23040079332002605\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025672807823866606    test_acc : 0.23036869263041543\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002236434491351247    test_acc : 0.24035261563140708\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019172349711880088    test_acc : 0.24038451314898213\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032791406847536564    test_acc : 0.19046448726245682\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0035293567925691605    test_acc : 0.21027304948007175\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002537789521738887    test_acc : 0.25027243785776376\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002166553633287549    test_acc : 0.24041620587175003\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003301590448245406    test_acc : 0.23040069075358388\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003139423904940486    test_acc : 0.22038466674362167\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001301517360843718    test_acc : 0.2702727944624397\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00429909722879529    test_acc : 0.1805759514200078\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020245774649083614    test_acc : 0.24019353339111824\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025334516540169716    test_acc : 0.21043192822169687\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022450159303843975    test_acc : 0.220320868780261\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015944549813866615    test_acc : 0.2802566161941861\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002544574672356248    test_acc : 0.23052797640956607\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013693420914933085    test_acc : 0.2802892267616919\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016453844727948308    test_acc : 0.26048015727399904\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002640758641064167    test_acc : 0.2204807672756358\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001944004907272756    test_acc : 0.25030505037468254\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017267693765461445    test_acc : 0.2404163100650948\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026658016722649336    test_acc : 0.21043263996825912\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.000770353595726192    test_acc : 0.30019307552705515\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001899004215374589    test_acc : 0.2605437478451344\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016024145297706127    test_acc : 0.27040109823592695\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025024276692420244    test_acc : 0.22051246357263873\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030867503955960274    test_acc : 0.20038502384528\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0035755347926169634    test_acc : 0.1803526678078124\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021694402676075697    test_acc : 0.24019282002494508\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032657997217029333    test_acc : 0.19046387482436083\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017756116576492786    test_acc : 0.26019317531892766\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0036121767479926348    test_acc : 0.22047985040037998\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024293954484164715    test_acc : 0.23033699632715776\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001960618421435356    test_acc : 0.23036848880615704\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028041366022080183    test_acc : 0.20041651274378963\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032983217388391495    test_acc : 0.20032081952953287\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022064698860049248    test_acc : 0.23027259047773016\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031295446678996086    test_acc : 0.2104002319184592\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00291666598059237    test_acc : 0.21033674195501104\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003783163148909807    test_acc : 0.17040043687525563\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00216066581197083    test_acc : 0.25014504931909026\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002653713570907712    test_acc : 0.2503998244387191\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029446252156049013    test_acc : 0.23043258729852625\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003077232511714101    test_acc : 0.22038476864951606\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015096631832420826    test_acc : 0.2702727947880176\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00341772916726768    test_acc : 0.2205120536574697\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002170207444578409    test_acc : 0.22035307365385773\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022926710080355406    test_acc : 0.2503046424078398\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014412726741284132    test_acc : 0.25040033432079184\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001584220677614212    test_acc : 0.27036869116396417\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002215505810454488    test_acc : 0.23049638559477306\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002182962140068412    test_acc : 0.2503370491552549\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018685210961848497    test_acc : 0.2404164122976206\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00412118760868907    test_acc : 0.19046458917666972\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002240528119727969    test_acc : 0.24022512648299255\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.004104252904653549    test_acc : 0.20044800359898718\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002990687731653452    test_acc : 0.22028897125750477\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027007723692804575    test_acc : 0.19040028425321887\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0034685744903981686    test_acc : 0.17033674212221478\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002135553164407611    test_acc : 0.2401608202623713\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022812997922301292    test_acc : 0.2503679259433303\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025547666009515524    test_acc : 0.24041651094550587\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002388698048889637    test_acc : 0.24038471728736585\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002200059127062559    test_acc : 0.2503686412692887\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0009303940460085869    test_acc : 0.2903366410264195\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001699112239293754    test_acc : 0.2505282320799566\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028184608090668917    test_acc : 0.2204489719874759\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002790539525449276    test_acc : 0.19040079543765967\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021294800098985434    test_acc : 0.2402249226691299\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020430509466677904    test_acc : 0.23040007962514097\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016055427258834243    test_acc : 0.26032076702755635\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00257695815525949    test_acc : 0.230464283600727\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020570377819240093    test_acc : 0.25033694659297356\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027741536032408476    test_acc : 0.21046433529262928\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00264268321916461    test_acc : 0.22032097231722883\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018974717240780592    test_acc : 0.24032051428855344\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002160965697839856    test_acc : 0.22041635947057048\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002936074510216713    test_acc : 0.23033679348073666\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027774302288889885    test_acc : 0.20041641148076914\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022603601682931185    test_acc : 0.25024094700153604\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002026522997766733    test_acc : 0.22044805414377486\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026090724859386683    test_acc : 0.21036884362346253\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002001633169129491    test_acc : 0.26025676946844556\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025751767680048943    test_acc : 0.23046407913568193\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021575887221843004    test_acc : 0.23036889482151976\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002348820911720395    test_acc : 0.2004165140409633\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029229819774627686    test_acc : 0.21030484509278263\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020090078469365835    test_acc : 0.2802246161185073\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025483097415417433    test_acc : 0.23052787417290257\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024242443032562733    test_acc : 0.2403531241986355\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003552019828930497    test_acc : 0.20044841253737583\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022301292046904564    test_acc : 0.23027299812312257\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017125406302511692    test_acc : 0.2603203610163678\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002274391707032919    test_acc : 0.23046428230356666\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002148284809663892    test_acc : 0.22038486991151301\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025623226538300514    test_acc : 0.23033669287511666\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030094117391854525    test_acc : 0.20041641115934541\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023337416350841522    test_acc : 0.24025692144140368\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001081231515854597    test_acc : 0.29030433521227283\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019466865342110395    test_acc : 0.2704961799847037\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002573179779574275    test_acc : 0.22051276734819394\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018738762009888887    test_acc : 0.24032112705223066\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00235139112919569    test_acc : 0.2304003869873873\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002209174446761608    test_acc : 0.22038466577312263\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020514687057584524    test_acc : 0.23033669222291733\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025924888905137777    test_acc : 0.23036848783457803\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002142223995178938    test_acc : 0.23036858941800187\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020493047777563334    test_acc : 0.23036858974254953\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002396909287199378    test_acc : 0.23036858974358643\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020358653273433447    test_acc : 0.23036858974358976\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003072859952226281    test_acc : 0.21040053862537889\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031345533207058907    test_acc : 0.21033674293490537\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018674680031836033    test_acc : 0.27024069246944055\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002524521667510271    test_acc : 0.230495976653257\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015046946937218308    test_acc : 0.2603210734078379\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022329245693981647    test_acc : 0.2504323356977886\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001955299871042371    test_acc : 0.2603847678456799\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031035179272294044    test_acc : 0.2104964369579734\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029726524371653795    test_acc : 0.22032107487845995\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002987174317240715    test_acc : 0.23033648905711968\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019166427664458752    test_acc : 0.2503365383036969\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001867939718067646    test_acc : 0.22044835954729616\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026796767488121986    test_acc : 0.22035287015829805\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014762275386601686    test_acc : 0.26028866731679967\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016712972428649664    test_acc : 0.25043223216395144\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0036370293237268925    test_acc : 0.19049658860116278\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020222323946654797    test_acc : 0.23024120315847016\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00352324265986681    test_acc : 0.22038415719858936\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001860915799625218    test_acc : 0.2702727928345003\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002405210630968213    test_acc : 0.2604481558876502\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027067717164754868    test_acc : 0.2204806650347848\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022279939148575068    test_acc : 0.22035297337071816\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021610360126942396    test_acc : 0.26028866764655184\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002808032324537635    test_acc : 0.20051210436947778\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020637502893805504    test_acc : 0.23027320161140408\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002958484459668398    test_acc : 0.20041620831185752\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014252440305426717    test_acc : 0.2602249719115395\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003457998624071479    test_acc : 0.2404480031051487\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016766804037615657    test_acc : 0.2603528690195053\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021521004382520914    test_acc : 0.22048036060389617\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002532059093937278    test_acc : 0.22035297239809554\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001472551841288805    test_acc : 0.2702726932025498\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017841473454609513    test_acc : 0.25046413001023177\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016981551889330149    test_acc : 0.2603848694249528\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015731330495327711    test_acc : 0.2504325395189296\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002032437827438116    test_acc : 0.25040074293776016\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001695679733529687    test_acc : 0.25040064135123885\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018701831577345729    test_acc : 0.25040064102668125\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021429036278277636    test_acc : 0.2703686921438552\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002524727024137974    test_acc : 0.25046443671611457\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027942003216594458    test_acc : 0.2204487681684221\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017674601403996348    test_acc : 0.2602889737002186\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018611527048051357    test_acc : 0.2304641820246013\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032200333662331104    test_acc : 0.2104008440320275\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0034842933528125286    test_acc : 0.1903686927924346\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002726713428273797    test_acc : 0.2202567689865573\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001691369921900332    test_acc : 0.2503043347251967\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020522214472293854    test_acc : 0.24041630777867476\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002216870430856943    test_acc : 0.22041666552005965\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023775312583893538    test_acc : 0.240320820017636\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031725948210805655    test_acc : 0.20044830932912983\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001610994921065867    test_acc : 0.2702091000298055\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001551130786538124    test_acc : 0.2704319779553668\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015683546662330627    test_acc : 0.24048061334810023\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001773556345142424    test_acc : 0.25036894764647954\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002137033035978675    test_acc : 0.2504005397688386\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023883592803031206    test_acc : 0.23043258958392598\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003162298584356904    test_acc : 0.2004167175386068\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002567020710557699    test_acc : 0.25024094797935653\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002383120823651552    test_acc : 0.2204480541468989\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003396349959075451    test_acc : 0.18041676694615622\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002719918731600046    test_acc : 0.21024094813720814\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019864195492118597    test_acc : 0.250272335297563\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002816088031977415    test_acc : 0.23043217998497625\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023753459099680185    test_acc : 0.21040074178908938\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0008712373673915863    test_acc : 0.29020894805683417\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002421427983790636    test_acc : 0.22057574743788125\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026013199239969254    test_acc : 0.25030535382568014\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002474177861586213    test_acc : 0.24041631103458683\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002252912847325206    test_acc : 0.24038471664867284\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020667831413447857    test_acc : 0.25036864126724817\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018585470970720053    test_acc : 0.2603845643490966\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002167178550735116    test_acc : 0.2404485129851409\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002247299998998642    test_acc : 0.23040079397119853\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001314538880251348    test_acc : 0.280288820428023\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013752756640315056    test_acc : 0.26048015597580837\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014853391330689192    test_acc : 0.29036894618522624\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002700161188840866    test_acc : 0.23056028417311575\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0012590393889695406    test_acc : 0.26032127886317286\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028622066602110863    test_acc : 0.2104962341177737\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023285814095288515    test_acc : 0.24028912534861907\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003291371511295438    test_acc : 0.2104322336273119\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021669112611562014    test_acc : 0.22032086975599782\n",
            "Looked at 29984/10000 samples\n",
            "test loss : 0.0012779716635122895    test_acc : 0.2802566161973035\n",
            "Epoch: 2\n",
            "---\n",
            "Looked at 0/60000 samples\n",
            "train loss : 0.0002702125549316406    train_acc : 0.045\n",
            "train loss : 0.00011813130106608073    train_acc : 0.050024\n",
            "train loss : 0.0003583491298152398    train_acc : 0.040026679466666666\n",
            "train loss : 0.000304979327543714    train_acc : 0.04002134756238222\n",
            "train loss : 0.0003327162012753085    train_acc : 0.04002134471869994\n",
            "train loss : 0.00027294047837497375    train_acc : 0.045021344717183304\n",
            "train loss : 0.0002762670256729393    train_acc : 0.04502401138384916\n",
            "train loss : 0.0003069662091434286    train_acc : 0.04002401280607138\n",
            "train loss : 0.0003494956028746128    train_acc : 0.03835467947349657\n",
            "train loss : 0.0005980419020087076    train_acc : 0.0316871224957192\n",
            "train loss : 0.00024226193358634472    train_acc : 0.040016899798664386\n",
            "train loss : 0.0002647874349208945    train_acc : 0.04668800901322595\n",
            "train loss : 0.0002643751111345213    train_acc : 0.04669156693814039\n",
            "train loss : 0.0002922818335960556    train_acc : 0.045024902169033675\n",
            "train loss : 0.00043020784617917116    train_acc : 0.03669067994782348\n",
            "train loss : 0.0004853941818860449    train_acc : 0.03835290169597217\n",
            "train loss : 0.00032406401834598054    train_acc : 0.043353788214237854\n",
            "train loss : 0.00040823637301233985    train_acc : 0.03835645535371426\n",
            "train loss : 0.0004338284196792947    train_acc : 0.03835379010952199\n",
            "train loss : 0.0003752624331507333    train_acc : 0.038353788688058416\n",
            "train loss : 0.00035795643675796686    train_acc : 0.03668712202063363\n",
            "train loss : 0.0003619177981204376    train_acc : 0.03668623313174434\n",
            "train loss : 0.00023668829514418636    train_acc : 0.045019565991003596\n",
            "train loss : 0.00033008356001066545    train_acc : 0.04169067710186187\n",
            "train loss : 0.0002843761089065662    train_acc : 0.04502223502778766\n",
            "train loss : 0.0003279231083998478    train_acc : 0.03835734519201482\n",
            "train loss : 0.0004736370607001309    train_acc : 0.040020457250769075\n",
            "train loss : 0.0003016370236483565    train_acc : 0.04502134424386708\n",
            "train loss : 0.0002702671886920721    train_acc : 0.045024011383596726\n",
            "train loss : 0.00046437019764589946    train_acc : 0.04169067947273792\n",
            "train loss : 0.00036470644757302184    train_acc : 0.041688901695718796\n",
            "train loss : 0.00038490301001381955    train_acc : 0.03502223408090439\n",
            "train loss : 0.0003461151314079937    train_acc : 0.03835201185817648\n",
            "train loss : 0.0006671060831807613    train_acc : 0.03168712107299103\n",
            "train loss : 0.00029374676332004666    train_acc : 0.04335023313123893\n",
            "train loss : 0.0002800252676931229    train_acc : 0.04502312012433666\n",
            "train loss : 0.000164491368751575    train_acc : 0.045024012330732985\n",
            "train loss : 0.0003459439184577841    train_acc : 0.04169067947324306\n",
            "train loss : 0.00024752244872550496    train_acc : 0.0450222350290524\n",
            "train loss : 0.00014378454645751687    train_acc : 0.046690678525348826\n",
            "train loss : 0.00023214037591277377    train_acc : 0.04169156836188019\n",
            "train loss : 0.000383205880140591    train_acc : 0.041688902169793006\n",
            "train loss : 0.0005352054318689851    train_acc : 0.03835556741449056\n",
            "train loss : 0.00039039758863877743    train_acc : 0.040020456302621056\n",
            "train loss : 0.0005630376747344485    train_acc : 0.03168801091002806\n",
            "train loss : 0.0004320636814335237    train_acc : 0.036683566939152014\n",
            "train loss : 0.0001980087542759312    train_acc : 0.045019564569034214\n",
            "train loss : 0.00025438238893001487    train_acc : 0.04335734376777015\n",
            "train loss : 0.00027728617440178155    train_acc : 0.04168979058334281\n",
            "train loss : 0.0003562317876829069    train_acc : 0.03835556788831112\n",
            "train loss : 0.0003899917150385546    train_acc : 0.0383537896362071\n",
            "train loss : 0.00039083178759388644    train_acc : 0.03835378868780598\n",
            "train loss : 0.0002841353120587708    train_acc : 0.04502045535396683\n",
            "train loss : 0.00034377295174610256    train_acc : 0.04169067757618878\n",
            "train loss : 0.00033610806633842477    train_acc : 0.038355568361373965\n",
            "train loss : 0.000295875611547734    train_acc : 0.043353789636459396\n",
            "train loss : 0.00037490746081810216    train_acc : 0.04168978868780611\n",
            "train loss : 0.0002947116185331395    train_acc : 0.0416889012206335\n",
            "train loss : 0.0002292672933095881    train_acc : 0.04502223408065101\n",
            "train loss : 0.00045559083515490185    train_acc : 0.03335734519150968\n",
            "train loss : 0.0003938064748716041    train_acc : 0.0416844572507688\n",
            "train loss : 0.0004987371755056738    train_acc : 0.03168889837720041\n",
            "train loss : 0.00045229334581122995    train_acc : 0.03835023407913451\n",
            "train loss : 0.00040843706840381743    train_acc : 0.03835378679150887\n",
            "train loss : 0.000243308844150512    train_acc : 0.04502045535295547\n",
            "train loss : 0.0003873829538037943    train_acc : 0.03669067757618824\n",
            "train loss : 0.000500481993595463    train_acc : 0.03668623502804064\n",
            "train loss : 0.000283075863345803    train_acc : 0.040019565992014956\n",
            "train loss : 0.000384439570900718    train_acc : 0.04168801043519574\n",
            "train loss : 0.0004533316874732955    train_acc : 0.03835556693889877\n",
            "train loss : 0.0002781545207862976    train_acc : 0.043353789635700746\n",
            "train loss : 0.00032703547981505736    train_acc : 0.04002312202113904\n",
            "train loss : 0.00029785152260095347    train_acc : 0.04168801233174461\n",
            "train loss : 0.00033728275678414045    train_acc : 0.041688900273243595\n",
            "train loss : 0.0001888331148040658    train_acc : 0.05002223408014573\n",
            "train loss : 0.0005887569457357732    train_acc : 0.030026678524842743\n",
            "train loss : 0.000574811480917934    train_acc : 0.03334934756187991\n",
            "train loss : 0.00032022605693736526    train_acc : 0.04501778631869967\n",
            "train loss : 0.000418617900267476    train_acc : 0.04002400948603664\n",
            "train loss : 0.0005026669897376459    train_acc : 0.03502134613839255\n",
            "train loss : 0.00052310569612703    train_acc : 0.033352011384607146\n",
            "train loss : 0.0002022004209281525    train_acc : 0.04668445440607179\n",
            "train loss : 0.0003280209020022456    train_acc : 0.04335823170901657\n",
            "train loss : 0.00038584773627997734    train_acc : 0.04335645772357814\n",
            "train loss : 0.0004877105220417875    train_acc : 0.03835645677745257\n",
            "train loss : 0.0005084536369569704    train_acc : 0.040020456776947974\n",
            "train loss : 0.0005154108186242807    train_acc : 0.03502134424361437\n",
            "train loss : 0.0003575488776726022    train_acc : 0.04001867805026326\n",
            "train loss : 0.00039922951618039673    train_acc : 0.04502134329496014\n",
            "train loss : 0.00041234044900978516    train_acc : 0.036690678049757314\n",
            "train loss : 0.0005465674694308781    train_acc : 0.035019568361626535\n",
            "train loss : 0.00028240349707581885    train_acc : 0.0450186771031262\n",
            "train loss : 0.00034987067999743137    train_acc : 0.04169067662778834\n",
            "train loss : 0.0003307685488373561    train_acc : 0.04502223502753482\n",
            "train loss : 0.0003737254897877328    train_acc : 0.040024011858681346\n",
            "train loss : 0.0003619116806433002    train_acc : 0.04502134613965796\n",
            "train loss : 0.00027443017547731637    train_acc : 0.046690678051274484\n",
            "train loss : 0.00022345444915551822    train_acc : 0.04669156836162735\n",
            "train loss : 0.00033235393980533736    train_acc : 0.04002490216979287\n",
            "train loss : 0.00032351048704263577    train_acc : 0.04502134661449055\n",
            "train loss : 0.00030554391877871763    train_acc : 0.04502401138486106\n",
            "train loss : 0.0002988264139913825    train_acc : 0.04335734613940526\n",
            "train loss : 0.0004101171644622179    train_acc : 0.040023123917941016\n",
            "train loss : 0.00031966087418408367    train_acc : 0.041688012332756236\n",
            "train loss : 0.00038067487142985455    train_acc : 0.0433555669399108\n",
            "train loss : 0.0006834112378611167    train_acc : 0.031689789635701286\n",
            "train loss : 0.00013895844580187556    train_acc : 0.04835023455447238\n",
            "train loss : 0.00044609131291263083    train_acc : 0.03502578679176239\n",
            "train loss : 0.0004949239467084395    train_acc : 0.03335201375295561\n",
            "train loss : 0.00045224509067400296    train_acc : 0.04001778774066825\n",
            "train loss : 0.00032852724118458994    train_acc : 0.040021342820128356\n",
            "train loss : 0.00025804798674542866    train_acc : 0.04335467804950407\n",
            "train loss : 0.0003516252939482369    train_acc : 0.041689789161626405\n",
            "train loss : 0.00025046204864208327    train_acc : 0.04502223455421953\n",
            "train loss : 0.000351356349178628    train_acc : 0.040024011858428916\n",
            "train loss : 0.0003350328058040346    train_acc : 0.04335467947299117\n",
            "train loss : 0.00033522081113656553    train_acc : 0.041689789162385596\n",
            "train loss : 0.0005229626042669974    train_acc : 0.03335556788755327\n",
            "train loss : 0.00023238836468288771    train_acc : 0.046684456302873364\n",
            "train loss : 0.0005849430348011707    train_acc : 0.03502489837669486\n",
            "train loss : 0.0004172610192503152    train_acc : 0.04168534661246757\n",
            "train loss : 0.0003253504587252408    train_acc : 0.04168889885152665\n",
            "train loss : 0.00036503465890635594    train_acc : 0.040022234079387486\n",
            "train loss : 0.00039736936141077545    train_acc : 0.03835467852484234\n",
            "train loss : 0.00028082665607478366    train_acc : 0.04502045582854658\n",
            "train loss : 0.00029660112029608165    train_acc : 0.040024010909775226\n",
            "train loss : 0.0005192377135296201    train_acc : 0.03835467947248521\n",
            "train loss : 0.0003690288659183258    train_acc : 0.040020455829052\n",
            "train loss : 0.00036678467677414404    train_acc : 0.041688010909775496\n",
            "train loss : 0.00029885271790020397    train_acc : 0.04002223360581855\n",
            "train loss : 0.0002928527487627792    train_acc : 0.041688011857923105\n",
            "train loss : 0.00034865208478998143    train_acc : 0.041688900272990895\n",
            "train loss : 0.00032231634176048824    train_acc : 0.040022234080145595\n",
            "train loss : 0.00029823324110940443    train_acc : 0.040021345191509405\n",
            "train loss : 0.00034699877177888467    train_acc : 0.041688011384102136\n",
            "train loss : 0.0003036922033101994    train_acc : 0.04335556693940485\n",
            "train loss : 0.00032305355420724397    train_acc : 0.04168978963570102\n",
            "train loss : 0.0005091446221433799    train_acc : 0.03668890122113904\n",
            "train loss : 0.0004285507847245832    train_acc : 0.036686234080651275\n",
            "train loss : 0.00033703708296083746    train_acc : 0.04168623265817634\n",
            "train loss : 0.00029980089009414815    train_acc : 0.040022232657417695\n",
            "train loss : 0.00036249923147487964    train_acc : 0.04002134519075062\n",
            "train loss : 0.00037924466857612257    train_acc : 0.036688011384101736\n",
            "train loss : 0.000407665687234292    train_acc : 0.03668623360607152\n",
            "train loss : 0.0003200193244708544    train_acc : 0.04668623265792324\n",
            "train loss : 0.000428725084728585    train_acc : 0.03669156599075089\n",
            "train loss : 0.00045975913408124    train_acc : 0.0383529021685284\n",
            "train loss : 0.00036672476063729774    train_acc : 0.04335378821448988\n",
            "train loss : 0.0007236227868563894    train_acc : 0.03168978868704773\n",
            "train loss : 0.0003456898172949985    train_acc : 0.04168356788729976\n",
            "train loss : 0.00035695691784966017    train_acc : 0.04168889790287323\n",
            "train loss : 0.00031636901675267086    train_acc : 0.0416889007455482\n",
            "train loss : 0.00028497364652410405    train_acc : 0.04002223408039763\n",
            "train loss : 0.0003391895632560108    train_acc : 0.04335467852484288\n",
            "train loss : 0.00032113376161472607    train_acc : 0.04335645582854659\n",
            "train loss : 0.000316991447781992    train_acc : 0.04335645677644189\n",
            "train loss : 0.00021632455933855664    train_acc : 0.0450231234436141\n",
            "train loss : 0.00026826071179256845    train_acc : 0.04335734566583659\n",
            "train loss : 0.0003946964476237633    train_acc : 0.03835645725102178\n",
            "train loss : 0.000580451835236747    train_acc : 0.030020456777200544\n",
            "train loss : 0.00041429800560688535    train_acc : 0.038349344243614504\n",
            "train loss : 0.00032190007423173384    train_acc : 0.03835378631692993\n",
            "train loss : 0.00024595094003918336    train_acc : 0.04668712201936903\n",
            "train loss : 0.0006637038667057813    train_acc : 0.03002489979841033\n",
            "train loss : 0.00036855109860521183    train_acc : 0.04001601327989248\n",
            "train loss : 0.0002455244942152766    train_acc : 0.04335467520708261\n",
            "train loss : 0.00022093272900494703    train_acc : 0.04668978916011045\n",
            "train loss : 0.00028719999428164115    train_acc : 0.04502490122088539\n",
            "train loss : 0.0002960416926173929    train_acc : 0.0416906799473178\n",
            "train loss : 0.0003252246778940216    train_acc : 0.04335556836263857\n",
            "train loss : 0.00030959327585441455    train_acc : 0.04335645630312674\n",
            "train loss : 0.0003769900993748079    train_acc : 0.036689790110028334\n",
            "train loss : 0.0004820952365979218    train_acc : 0.03835290122139202\n",
            "train loss : 0.000549852123766485    train_acc : 0.036687121547318076\n",
            "train loss : 0.00027223847668484015    train_acc : 0.04335289979815857\n",
            "train loss : 0.0002810605177474611    train_acc : 0.043356454879892356\n",
            "train loss : 0.00032916537644117106    train_acc : 0.038356456775935945\n",
            "train loss : 0.00037789804072762407    train_acc : 0.041687123443613826\n",
            "train loss : 0.0003040253345824961    train_acc : 0.04335556646583659\n",
            "train loss : 0.0003583697223252539    train_acc : 0.036689789635448444\n",
            "train loss : 0.00031843263137306565    train_acc : 0.03835290122113891\n",
            "train loss : 0.00037490763387800836    train_acc : 0.04168712154731794\n",
            "train loss : 0.00029568601946365424    train_acc : 0.04168889979815857\n",
            "train loss : 0.0002965008350441208    train_acc : 0.04335556741322569\n",
            "train loss : 0.0003081545021868933    train_acc : 0.04502312296928706\n",
            "train loss : 0.0004963902147497992    train_acc : 0.036690678998916954\n",
            "train loss : 0.0004597961122466133    train_acc : 0.036686235028799426\n",
            "train loss : 0.00035628032998056804    train_acc : 0.04168623265868202\n",
            "train loss : 0.00021126322403121426    train_acc : 0.0483555659907513\n",
            "train loss : 0.00023553633399780362    train_acc : 0.0483591229685284\n",
            "train loss : 0.00021898487866564844    train_acc : 0.045025791532249886\n",
            "train loss : 0.0005487775153036152    train_acc : 0.03669068042215053\n",
            "train loss : 0.00031238021719964956    train_acc : 0.04168623502955848\n",
            "train loss : 0.00019320899233430498    train_acc : 0.04502223265868243\n",
            "train loss : 0.00017344627408973488    train_acc : 0.04669067852408463\n",
            "train loss : 0.0004401525162253316    train_acc : 0.03502490169521284\n",
            "train loss : 0.0004365161429671496    train_acc : 0.03835201328090411\n",
            "train loss : 0.000451089812139449    train_acc : 0.036687121073749815\n",
            "train loss : 0.00031890506843610327    train_acc : 0.043352899797906005\n",
            "train loss : 0.00021578316177421085    train_acc : 0.041689788213225554\n",
            "train loss : 0.00034192667772656274    train_acc : 0.04335556788704705\n",
            "train loss : 0.0002766320094340128    train_acc : 0.041689789636206426\n",
            "train loss : 0.0005951193210275901    train_acc : 0.03668890122113931\n",
            "train loss : 0.00021266374414426973    train_acc : 0.04668623408065128\n",
            "train loss : 0.0003055938054902038    train_acc : 0.04169156599150968\n",
            "train loss : 0.0002398436709493946    train_acc : 0.04335556883519547\n",
            "train loss : 0.0005674316377345324    train_acc : 0.03335645630337877\n",
            "train loss : 0.0003170649186182501    train_acc : 0.0383511234433618\n",
            "train loss : 0.0003970587878744024    train_acc : 0.04168712059916979\n",
            "train loss : 0.0004929811331764497    train_acc : 0.03502223313098622\n",
            "train loss : 0.0003979251814192208    train_acc : 0.036685345191003194\n",
            "train loss : 0.0002999068920364542    train_acc : 0.0400195655174352\n",
            "train loss : 0.00040621056883200277    train_acc : 0.03668801043494264\n",
            "train loss : 0.00033692109052196434    train_acc : 0.03835290027223197\n",
            "train loss : 0.00016883700999929563    train_acc : 0.04835378821347852\n",
            "train loss : 0.0002122828996971624    train_acc : 0.04502578868704719\n",
            "train loss : 0.0003697585905730351    train_acc : 0.03835734708729976\n",
            "train loss : 0.0002939906921304671    train_acc : 0.04168712391844656\n",
            "train loss : 0.00023532882834573945    train_acc : 0.04502223313275651\n",
            "train loss : 0.0003520682704475461    train_acc : 0.0400240118576708\n",
            "train loss : 0.00034507982652727905    train_acc : 0.03668801280632409\n",
            "train loss : 0.00027841159435230544    train_acc : 0.045019566940163375\n",
            "train loss : 0.00018078522591448955    train_acc : 0.04669067710236809\n",
            "train loss : 0.00043463532361300076    train_acc : 0.03835823502778793\n",
            "train loss : 0.00020716164813694906    train_acc : 0.050020457725348154\n",
            "train loss : 0.000435527248246438    train_acc : 0.036693344244120185\n",
            "train loss : 0.0001980185805866787    train_acc : 0.04668623645026353\n",
            "train loss : 0.0002333270772456651    train_acc : 0.043358232659440145\n",
            "train loss : 0.00036449450107490535    train_acc : 0.0400231243907517\n",
            "train loss : 0.0002554969216338903    train_acc : 0.045021345666341736\n",
            "train loss : 0.0003732116230977881    train_acc : 0.04335734471768871\n",
            "train loss : 0.00020227312889144967    train_acc : 0.046689790583849436\n",
            "train loss : 0.0006080296700787356    train_acc : 0.038358234554978055\n",
            "train loss : 0.00032839746618618397    train_acc : 0.041687124391762656\n",
            "train loss : 0.0004161170512906411    train_acc : 0.040022233133008935\n",
            "train loss : 0.00026475808733783675    train_acc : 0.040021345191004275\n",
            "train loss : 0.00034838062615772926    train_acc : 0.04668801138410187\n",
            "train loss : 0.0005406767163917307    train_acc : 0.03169156693940485\n",
            "train loss : 0.0002690798457481726    train_acc : 0.04335023550236768\n",
            "train loss : 0.00034805833214738733    train_acc : 0.04168978679226793\n",
            "train loss : 0.000186061460171351    train_acc : 0.050022234552955874\n",
            "train loss : 0.0003459124435840152    train_acc : 0.04169334519176158\n",
            "train loss : 0.0002146830542177549    train_acc : 0.045022236450768936\n",
            "train loss : 0.00033630917180290705    train_acc : 0.041690678526107076\n",
            "train loss : 0.00030042121669826886    train_acc : 0.04335556836188059\n",
            "train loss : 0.00028367917479335563    train_acc : 0.04335645630312633\n",
            "train loss : 0.00017540544976521206    train_acc : 0.048356456776695\n",
            "train loss : 0.0002911789517440252    train_acc : 0.04169245677694757\n",
            "train loss : 0.0004940509532981893    train_acc : 0.03668890264361437\n",
            "train loss : 0.00034209091820902794    train_acc : 0.038352900748076596\n",
            "train loss : 0.00037310617234631956    train_acc : 0.03835378821373231\n",
            "train loss : 0.0003077178854257071    train_acc : 0.040020455353713985\n",
            "train loss : 0.0003289610115506125    train_acc : 0.04002134424285531\n",
            "train loss : 0.0002677398061735269    train_acc : 0.04335467805026286\n",
            "train loss : 0.0005385075864334097    train_acc : 0.033356455828293476\n",
            "train loss : 0.0002491471976628377    train_acc : 0.04668445677644176\n",
            "train loss : 0.0003433316865396813    train_acc : 0.04002489837694744\n",
            "train loss : 0.0003236555147128016    train_acc : 0.04168801327913437\n",
            "train loss : 0.00034277763489023614    train_acc : 0.043355566940415535\n",
            "train loss : 0.0004076988491827813    train_acc : 0.04168978963570156\n",
            "train loss : 0.00025557151004968295    train_acc : 0.046688901221139036\n",
            "train loss : 0.0002683676331338104    train_acc : 0.04169156741398461\n",
            "train loss : 0.0004384309279679773    train_acc : 0.03668890216928746\n",
            "train loss : 0.00041587326209957125    train_acc : 0.04001956741449029\n",
            "train loss : 0.0004761773807381588    train_acc : 0.0366880104359544\n",
            "train loss : 0.0003338506676601079    train_acc : 0.04335290027223251\n",
            "train loss : 0.0007491789208972637    train_acc : 0.035023121546811864\n",
            "train loss : 0.00035776707765511004    train_acc : 0.041685345664824965\n",
            "train loss : 0.00035722110543376633    train_acc : 0.036688898851021244\n",
            "train loss : 0.0003212187822865048    train_acc : 0.041686234079387215\n",
            "train loss : 0.000422620619357226    train_acc : 0.036688899324842345\n",
            "train loss : 0.00026915335329109864    train_acc : 0.04501956741297325\n",
            "train loss : 0.000427852873609711    train_acc : 0.03502401043595359\n",
            "train loss : 0.0005446055787428783    train_acc : 0.03668534613889917\n",
            "train loss : 0.00019355018823717036    train_acc : 0.04835289885127408\n",
            "train loss : 0.00043375937885649666    train_acc : 0.03835912154605402\n",
            "train loss : 0.0003898516742322    train_acc : 0.040020458198157895\n",
            "train loss : 0.0002561589439947305    train_acc : 0.04168801091103902\n",
            "train loss : 0.0003149482928572888    train_acc : 0.04502223360581922\n",
            "train loss : 0.0004059080745550187    train_acc : 0.038357345191256435\n",
            "train loss : 0.0003401884717128421    train_acc : 0.041687123917435336\n",
            "train loss : 0.0004823829465062026    train_acc : 0.0383555664660893\n",
            "train loss : 0.0002347876244509459    train_acc : 0.04668712296878192\n",
            "train loss : 0.00042373489006962905    train_acc : 0.038358233132250015\n",
            "train loss : 0.00031592779451298505    train_acc : 0.04168712439100386\n",
            "train loss : 0.00030047471964673826    train_acc : 0.03835556646634187\n",
            "train loss : 0.0004138694168495831    train_acc : 0.04168712296878205\n",
            "train loss : 0.0003821682159615288    train_acc : 0.03835556646558335\n",
            "train loss : 0.0004049668222652283    train_acc : 0.03835378963544831\n",
            "train loss : 0.0004554486661227537    train_acc : 0.03502045535447224\n",
            "train loss : 0.00018935205652390966    train_acc : 0.04835201090952238\n",
            "train loss : 0.0004150966898712268    train_acc : 0.03669245440581841\n",
            "train loss : 0.00021992356512790854    train_acc : 0.0466862359756831\n",
            "train loss : 0.00022395864185192237    train_acc : 0.04669156599252036\n",
            "train loss : 0.000313307202736836    train_acc : 0.04169156883519601\n",
            "train loss : 0.00024239042353994597    train_acc : 0.04502223550337877\n",
            "train loss : 0.0002640065554457366    train_acc : 0.04335734519226847\n",
            "train loss : 0.0002182144894601863    train_acc : 0.045023123917435875\n",
            "train loss : 0.0003803130795677186    train_acc : 0.03669067899942263\n",
            "train loss : 0.00043622816704697715    train_acc : 0.03668623502879969\n",
            "train loss : 0.00028239322325972975    train_acc : 0.04335289932534869\n",
            "train loss : 0.000518823362099443    train_acc : 0.036689788212973515\n",
            "train loss : 0.0002476505092497441    train_acc : 0.04335290122038025\n",
            "train loss : 0.0002321994904227636    train_acc : 0.0466897882139842\n",
            "train loss : 0.00039287788585045855    train_acc : 0.04002490122038079\n",
            "train loss : 0.00022299316397198158    train_acc : 0.04668801328065087\n",
            "train loss : 0.0004141278331908697    train_acc : 0.03669156694041635\n",
            "train loss : 0.0003507536224399902    train_acc : 0.040019568835701555\n",
            "train loss : 0.0005187653436433937    train_acc : 0.036688010436712375\n",
            "train loss : 0.00035701899332121593    train_acc : 0.03668623360556624\n",
            "train loss : 0.0003834359736368514    train_acc : 0.038352899324589636\n",
            "train loss : 0.0004758835152992209    train_acc : 0.03668712154630645\n",
            "train loss : 0.000451853434261545    train_acc : 0.03835289979815803\n",
            "train loss : 0.0002286420295291566    train_acc : 0.04502045487989235\n",
            "train loss : 0.0005458098722049742    train_acc : 0.03335734424260261\n",
            "train loss : 0.00038382155641304385    train_acc : 0.03835112391692939\n",
            "train loss : 0.0003003524289816983    train_acc : 0.045020453932755694\n",
            "train loss : 0.0002674451852664042    train_acc : 0.0466906775754308\n",
            "train loss : 0.0003358377769991994    train_acc : 0.04169156836137356\n",
            "train loss : 0.00017334127181134782    train_acc : 0.048355568836459396\n",
            "train loss : 0.00026043843218354843    train_acc : 0.04335912297004611\n",
            "train loss : 0.0002866244135485155    train_acc : 0.043356458198917355\n",
            "train loss : 0.00028628126016289323    train_acc : 0.04168979011103942\n",
            "train loss : 0.0003996435166212243    train_acc : 0.04002223455472589\n",
            "train loss : 0.00046989993748987053    train_acc : 0.03668801185842919\n",
            "train loss : 0.0005186781760136177    train_acc : 0.036686233606324496\n",
            "train loss : 0.00039355589471796245    train_acc : 0.03835289932459004\n",
            "train loss : 0.00031756301791679883    train_acc : 0.04168712154630645\n",
            "train loss : 0.0003579828841954931    train_acc : 0.040022233131491364\n",
            "train loss : 0.0002970217055057669    train_acc : 0.04502134519100347\n",
            "train loss : 0.0004933388772745119    train_acc : 0.03669067805076853\n",
            "train loss : 0.0002592181953717941    train_acc : 0.041686235028293744\n",
            "train loss : 0.00030325123340781157    train_acc : 0.04335556599201509\n",
            "train loss : 0.00031801779147609547    train_acc : 0.04002312296852908\n",
            "train loss : 0.00025616679042750795    train_acc : 0.04502134566558322\n",
            "train loss : 0.00035275770077943896    train_acc : 0.04169067805102165\n",
            "train loss : 0.00038624122276756416    train_acc : 0.04168890169496055\n",
            "train loss : 0.0005283824619427514    train_acc : 0.033355567414237314\n",
            "train loss : 0.00039034648852967024    train_acc : 0.03835112296928759\n",
            "train loss : 0.00032255064415464093    train_acc : 0.04335378726558362\n",
            "train loss : 0.00033739972120373795    train_acc : 0.04002312201987498\n",
            "train loss : 0.0004635266185760808    train_acc : 0.0333546789984106\n",
            "train loss : 0.00037976133620585124    train_acc : 0.038351122495465816\n",
            "train loss : 0.00025498046359907707    train_acc : 0.05002045393199758\n",
            "train loss : 0.0002844952221652216    train_acc : 0.041693344242097065\n",
            "train loss : 0.00027387056470849465    train_acc : 0.045022236450262446\n",
            "train loss : 0.00045426161402448514    train_acc : 0.04002401185944014\n",
            "train loss : 0.0004039156038036028    train_acc : 0.036688012806325034\n",
            "train loss : 0.00031488538040576883    train_acc : 0.04001956694016337\n",
            "train loss : 0.0004023593870812197    train_acc : 0.03835467710236808\n",
            "train loss : 0.00033915552998691205    train_acc : 0.041687122494454595\n",
            "train loss : 0.0004538616843409279    train_acc : 0.04002223313199704\n",
            "train loss : 0.0003882391464078366    train_acc : 0.04002134519100373\n",
            "train loss : 0.00039752141863565253    train_acc : 0.0400213447174352\n",
            "train loss : 0.00020542425482130295    train_acc : 0.046688011383849294\n",
            "train loss : 0.00029534369936290667    train_acc : 0.043358233606071384\n",
            "train loss : 0.0003742551817906205    train_acc : 0.04168979105792324\n",
            "train loss : 0.00025188102595087754    train_acc : 0.046688901221897554\n",
            "train loss : 0.000373569134469944    train_acc : 0.04002490074731835\n",
            "train loss : 0.00029927032629839377    train_acc : 0.04168801328039857\n",
            "train loss : 0.0003686156248380883    train_acc : 0.03668890027374954\n",
            "train loss : 0.00024530534248221346    train_acc : 0.04501956741347933\n",
            "train loss : 0.0004175223522414137    train_acc : 0.038357343769287185\n",
            "train loss : 0.00033976217469178466    train_acc : 0.041687123916676956\n",
            "train loss : 0.0004351963444834847    train_acc : 0.04002223313275556\n",
            "train loss : 0.00036606853431300513    train_acc : 0.040021345191004136\n",
            "train loss : 0.0004744077558794331    train_acc : 0.03168801138410187\n",
            "train loss : 0.0002528127429133245    train_acc : 0.04168356693940486\n",
            "train loss : 0.0004025818981804978    train_acc : 0.04168889790236768\n",
            "train loss : 0.0003626665497927178    train_acc : 0.04002223407888126\n",
            "train loss : 0.0002687643539124122    train_acc : 0.0416880118581754\n",
            "train loss : 0.00031435480491697593    train_acc : 0.03835556693965769\n",
            "train loss : 0.00028437147135616076    train_acc : 0.040020456302367814\n",
            "train loss : 0.00036338717020667965    train_acc : 0.0383546775766946\n",
            "train loss : 0.00024418742259490285    train_acc : 0.04335378916137424\n",
            "train loss : 0.00034367423963970687    train_acc : 0.040023122020886065\n",
            "train loss : 0.00025326211919632184    train_acc : 0.04668801233174447\n",
            "train loss : 0.0004481813441365043    train_acc : 0.03669156693991026\n",
            "train loss : 0.0003981748337814887    train_acc : 0.04168623550236795\n",
            "train loss : 0.0003113635783256568    train_acc : 0.03835556599226793\n",
            "train loss : 0.00024301307404861778    train_acc : 0.04168712296852921\n",
            "train loss : 0.0005267955488470122    train_acc : 0.03668889979891655\n",
            "train loss : 0.0004521991463569274    train_acc : 0.03668623407989276\n",
            "train loss : 0.0002430490062317354    train_acc : 0.046686232658175945\n",
            "train loss : 0.0005134029606398307    train_acc : 0.03502489932408436\n",
            "train loss : 0.00038093479386538487    train_acc : 0.04001867994630618\n",
            "train loss : 0.0004483786590995342    train_acc : 0.0333546766293047\n",
            "train loss : 0.0004361812764998596    train_acc : 0.03835112249420229\n",
            "train loss : 0.000393905223448785    train_acc : 0.03502045393199691\n",
            "train loss : 0.0003433754168515945    train_acc : 0.04335201090876373\n",
            "train loss : 0.00030766483572443673    train_acc : 0.04168978773915134\n",
            "train loss : 0.00033205482989643584    train_acc : 0.04002223455346088\n",
            "train loss : 0.00022140118139918696    train_acc : 0.04502134519176185\n",
            "train loss : 0.0005521876810532566    train_acc : 0.03169067805076894\n",
            "train loss : 0.0003088416472116334    train_acc : 0.04335023502829374\n",
            "train loss : 0.0002046796481776258    train_acc : 0.045023120125348426\n",
            "train loss : 0.0005101353313022702    train_acc : 0.04002401233073352\n",
            "train loss : 0.0002297309128647642    train_acc : 0.045021346139909726\n",
            "train loss : 0.0003577630438851359    train_acc : 0.04002401138460795\n",
            "train loss : 0.00036061720110143274    train_acc : 0.038354679472738455\n",
            "Looked at 12800/60000 samples\n",
            "train loss : 0.0006425436068431917    train_acc : 0.0316871224957188\n",
            "train loss : 0.00038359861669163474    train_acc : 0.04168356646533106\n",
            "train loss : 0.00047944595708368737    train_acc : 0.03502223123544818\n",
            "train loss : 0.00035360898235264844    train_acc : 0.03668534518999224\n",
            "train loss : 0.00021007326680108613    train_acc : 0.04501956551743466\n",
            "train loss : 0.0003594803633554124    train_acc : 0.038357343768275966\n",
            "train loss : 0.0005375542666023181    train_acc : 0.03668712391667642\n",
            "train loss : 0.0004186018201205733    train_acc : 0.040019566466088896\n",
            "train loss : 0.0003353753081370721    train_acc : 0.038354677102115244\n",
            "train loss : 0.00040505859578405005    train_acc : 0.03835378916112113\n",
            "train loss : 0.0003776816608287216    train_acc : 0.0383537886875526\n",
            "train loss : 0.0004175661864853847    train_acc : 0.0400204553539667\n",
            "train loss : 0.00037234727853472906    train_acc : 0.03668801090952212\n",
            "train loss : 0.0004294863367309249    train_acc : 0.04168623360581841\n",
            "train loss : 0.0002078253137283447    train_acc : 0.04668889932458977\n",
            "train loss : 0.0003149925464051147    train_acc : 0.04002490074630645\n",
            "train loss : 0.0004967055860516049    train_acc : 0.03502134661373136\n",
            "train loss : 0.0003333940295189411    train_acc : 0.040018678051527326\n",
            "train loss : 0.000573458398934884    train_acc : 0.03168800996162748\n",
            "train loss : 0.0004281554233368538    train_acc : 0.03335023360531287\n",
            "train loss : 0.0003386512837733871    train_acc : 0.041684453457922835\n",
            "train loss : 0.00041619868678209127    train_acc : 0.03502223170851089\n",
            "train loss : 0.0002721033304617265    train_acc : 0.045018678523577875\n",
            "train loss : 0.0004032177968454194    train_acc : 0.03835734329521257\n",
            "train loss : 0.00027559400787584683    train_acc : 0.04335379058309078\n",
            "train loss : 0.00025796964010233684    train_acc : 0.04502312202164432\n",
            "train loss : 0.00035161984800890096    train_acc : 0.04335734566507821\n",
            "train loss : 0.0001770905400002776    train_acc : 0.04502312391768804\n",
            "train loss : 0.00036152042027286343    train_acc : 0.04169067899942277\n",
            "train loss : 0.0004262729424215739    train_acc : 0.038355568362133025\n",
            "train loss : 0.00045224017058149855    train_acc : 0.030020456303126473\n",
            "train loss : 0.0002528208519943785    train_acc : 0.04334934424336166\n",
            "train loss : 0.0002515642843260197    train_acc : 0.04502311965026313\n",
            "train loss : 0.00019647239202931797    train_acc : 0.04835734566381347\n",
            "train loss : 0.00016187264950304388    train_acc : 0.046692457251020704\n",
            "train loss : 0.00024254785449998238    train_acc : 0.04669156931053388\n",
            "train loss : 0.0003699540544428915    train_acc : 0.038358235503632286\n",
            "train loss : 0.000491301625540462    train_acc : 0.03502045772560194\n",
            "train loss : 0.0005084948435940057    train_acc : 0.03668534424412032\n",
            "train loss : 0.0003735813817286766    train_acc : 0.041686232183596864\n",
            "train loss : 0.00043553392586818826    train_acc : 0.03502223265716458\n",
            "train loss : 0.00039792347102958736    train_acc : 0.04001867852408383\n",
            "train loss : 0.0002721176162727653    train_acc : 0.043354676628546175\n",
            "train loss : 0.0003223327756254073    train_acc : 0.04168978916086856\n",
            "train loss : 0.0003323597859255647    train_acc : 0.04502223455421913\n",
            "train loss : 0.0002794674896214227    train_acc : 0.04502401185842892\n",
            "train loss : 0.0003860222398897284    train_acc : 0.03835734613965783\n",
            "train loss : 0.00033204957257904793    train_acc : 0.04168712391794115\n",
            "train loss : 0.00031987768387482535    train_acc : 0.0416888997994229\n",
            "train loss : 0.0003187248120942417    train_acc : 0.04168890074655969\n",
            "train loss : 0.0005554196457868278    train_acc : 0.03502223408039817\n",
            "train loss : 0.00024496437375920644    train_acc : 0.04835201185817621\n",
            "train loss : 0.00029946943347329655    train_acc : 0.04335912107299102\n",
            "train loss : 0.0003363782966413745    train_acc : 0.04002312486457226\n",
            "train loss : 0.00042965048126667225    train_acc : 0.038354678999927776\n",
            "train loss : 0.0003880137551285017    train_acc : 0.03668712249546663\n",
            "train loss : 0.00043815758357304774    train_acc : 0.03835289979866425\n",
            "train loss : 0.0003707915326570397    train_acc : 0.04168712154655929\n",
            "train loss : 0.0003319795242829119    train_acc : 0.041688899798158166\n",
            "train loss : 0.00044216279005007653    train_acc : 0.03668890074655902\n",
            "train loss : 0.0001964304847919411    train_acc : 0.04835290074706483\n",
            "train loss : 0.0005294020511840928    train_acc : 0.036692454880398434\n",
            "train loss : 0.0002696560189744173    train_acc : 0.04335290264260288\n",
            "train loss : 0.0002901191735991171    train_acc : 0.041689788214742715\n",
            "train loss : 0.0003307322626762776    train_acc : 0.0466889012203812\n",
            "train loss : 0.00028894039047905887    train_acc : 0.04335823408065087\n",
            "train loss : 0.0002964204665928584    train_acc : 0.045023124391509683\n",
            "train loss : 0.000374802275028065    train_acc : 0.040024012333008806\n",
            "train loss : 0.0003491999988150736    train_acc : 0.045021346139910934\n",
            "train loss : 0.00029736946753843057    train_acc : 0.04169067805127462\n",
            "train loss : 0.0004928470303786182    train_acc : 0.030022235028294014\n",
            "train loss : 0.00035711983366278397    train_acc : 0.041682678525348425\n",
            "train loss : 0.00026037737454055927    train_acc : 0.04335556409521352\n",
            "train loss : 0.00036764524483535066    train_acc : 0.03835645630085078\n",
            "train loss : 0.00042005167209892193    train_acc : 0.03835379011002712\n",
            "train loss : 0.00020416837684739318    train_acc : 0.04668712202139201\n",
            "train loss : 0.0002851425608621832    train_acc : 0.04002489979841141\n",
            "train loss : 0.00047549352322687714    train_acc : 0.03668801327989248\n",
            "train loss : 0.0003258352107221533    train_acc : 0.0416862336070826\n",
            "train loss : 0.0004104569628305655    train_acc : 0.040022232657923776\n",
            "train loss : 0.00041302767833468476    train_acc : 0.04168801185741756\n",
            "train loss : 0.00036728145137066525    train_acc : 0.04335556693965729\n",
            "train loss : 0.0004178323097306073    train_acc : 0.04002312296903448\n",
            "train loss : 0.0001930657247833293    train_acc : 0.04335467899891682\n",
            "train loss : 0.0003198941264269157    train_acc : 0.04002312249546609\n",
            "train loss : 0.0003922901450111452    train_acc : 0.03502134566533092\n",
            "train loss : 0.0003095601888178992    train_acc : 0.04001867805102151\n",
            "train loss : 0.0002952821908999379    train_acc : 0.043354676628293884\n",
            "train loss : 0.00040883118353200537    train_acc : 0.038356455827535094\n",
            "train loss : 0.00034386018273269815    train_acc : 0.04002045677644135\n",
            "train loss : 0.0002630670870640915    train_acc : 0.04168801091028077\n",
            "train loss : 0.0003192794651458157    train_acc : 0.04335556693915215\n",
            "train loss : 0.00026677766229474113    train_acc : 0.04668978963570088\n",
            "train loss : 0.0003342841398386731    train_acc : 0.038358234554472376\n",
            "train loss : 0.0003582223052094602    train_acc : 0.040020457725095715\n",
            "train loss : 0.0003172453553832536    train_acc : 0.041688010910786714\n",
            "train loss : 0.0003603232858610872    train_acc : 0.03835556693915242\n",
            "train loss : 0.0002199700108082047    train_acc : 0.04668712296903421\n",
            "train loss : 0.00032313233961692975    train_acc : 0.04335823313225015\n",
            "train loss : 0.0002884159806905854    train_acc : 0.04002312439100387\n",
            "train loss : 0.00025529185349801054    train_acc : 0.04502134566634187\n",
            "train loss : 0.00019564619339527053    train_acc : 0.04335734471768871\n",
            "train loss : 0.00029264317825464155    train_acc : 0.0433564572505161\n",
            "train loss : 0.0004627229607204598    train_acc : 0.03502312344386694\n",
            "train loss : 0.00030342201242638167    train_acc : 0.043352012332503395\n",
            "train loss : 0.00039352186764531554    train_acc : 0.03835645440657734\n",
            "train loss : 0.00042355865482884115    train_acc : 0.03835379010901684\n",
            "train loss : 0.0002085664474182362    train_acc : 0.046687122021391476\n",
            "train loss : 0.00038409122353676763    train_acc : 0.04169156646507808\n",
            "train loss : 0.00031429637214091556    train_acc : 0.04168890216878137\n",
            "train loss : 0.0003008184120853241    train_acc : 0.04168890074782335\n",
            "train loss : 0.000343207069888952    train_acc : 0.04335556741373218\n",
            "train loss : 0.0005626127261843444    train_acc : 0.031689789635953994\n",
            "train loss : 0.0002753192607635353    train_acc : 0.045016901221139174\n",
            "train loss : 0.0003545596151576611    train_acc : 0.04002400901398461\n",
            "train loss : 0.0005986697681188068    train_acc : 0.03168801280480746\n",
            "train loss : 0.0004485343619032669    train_acc : 0.03835023360682923\n",
            "train loss : 0.00028596977099968826    train_acc : 0.04002045345792365\n",
            "train loss : 0.0004206798164563984    train_acc : 0.041688010908510896\n",
            "train loss : 0.000240495819605113    train_acc : 0.04502223360581788\n",
            "train loss : 0.00030372027874986683    train_acc : 0.0450240118579231\n",
            "train loss : 0.00041245646733348106    train_acc : 0.04002401280632422\n",
            "train loss : 0.0004063129702009535    train_acc : 0.03835467947349671\n",
            "train loss : 0.0006169830576726489    train_acc : 0.0366871224957192\n",
            "train loss : 0.00019671523596531278    train_acc : 0.04501956646533105\n",
            "train loss : 0.0002836756494523456    train_acc : 0.04335734376878151\n",
            "train loss : 0.00037836371382537847    train_acc : 0.038356457250010016\n",
            "train loss : 0.00037631975459268604    train_acc : 0.0400204567772\n",
            "train loss : 0.0003655141702590901    train_acc : 0.03668801091028117\n",
            "train loss : 0.0003262411851941903    train_acc : 0.041686233605818815\n",
            "train loss : 0.00031703444858653066    train_acc : 0.0400222326579231\n",
            "train loss : 0.0003131710356211147    train_acc : 0.04168801185741756\n",
            "train loss : 0.0005199128830626666    train_acc : 0.035022233606323956\n",
            "train loss : 0.0004433042427934114    train_acc : 0.03501867852459004\n",
            "train loss : 0.0005218154719999651    train_acc : 0.03501867662854645\n",
            "train loss : 0.0004277521327618244    train_acc : 0.04001867662753523\n",
            "train loss : 0.0003806345141705133    train_acc : 0.04168800996086802\n",
            "train loss : 0.0003902400648581598    train_acc : 0.04168890027197913\n",
            "train loss : 0.0004874206270173384    train_acc : 0.033355567413478386\n",
            "train loss : 0.00031632606382822436    train_acc : 0.04335112296928719\n",
            "train loss : 0.00043915860082941935    train_acc : 0.033356453932250285\n",
            "train loss : 0.0003157763118332028    train_acc : 0.041684456775430534\n",
            "train loss : 0.0002601684203908065    train_acc : 0.04168889837694689\n",
            "train loss : 0.00034962561613036076    train_acc : 0.038355567412467706\n",
            "train loss : 0.00038308228634137956    train_acc : 0.04002045630261998\n",
            "train loss : 0.00020840781702244197    train_acc : 0.048354677576694725\n",
            "train loss : 0.0005180030028966991    train_acc : 0.03335912249470757\n",
            "train loss : 0.0003889593966251452    train_acc : 0.04168445819866384\n",
            "train loss : 0.0004309573699891539    train_acc : 0.035022231711039284\n",
            "train loss : 0.0005540481651586883    train_acc : 0.03001867852357922\n",
            "train loss : 0.0005280774874922839    train_acc : 0.03334934329521258\n",
            "train loss : 0.0004100328658444848    train_acc : 0.038351119649757445\n",
            "train loss : 0.0003666425771699706    train_acc : 0.03835378726381321\n",
            "train loss : 0.00020865623924611336    train_acc : 0.0483537886865407\n",
            "train loss : 0.0004152879292423115    train_acc : 0.03502578868729949\n",
            "train loss : 0.00028656464816065124    train_acc : 0.04501868042063322\n",
            "train loss : 0.0005641946079572092    train_acc : 0.03169067662955767\n",
            "train loss : 0.0005345356500881111    train_acc : 0.033350235027535764\n",
            "train loss : 0.00026373562876578105    train_acc : 0.04668445345868136\n",
            "train loss : 0.0004147895097608789    train_acc : 0.035024898375177964\n",
            "train loss : 0.00021956949275969798    train_acc : 0.04335201327913343\n",
            "train loss : 0.00020761160102866617    train_acc : 0.0483564544070822\n",
            "train loss : 0.00023322554094551607    train_acc : 0.04502579010901711\n",
            "train loss : 0.0003650567015100212    train_acc : 0.04169068042139148\n",
            "train loss : 0.00026303670041190726    train_acc : 0.04335556836289141\n",
            "train loss : 0.00024927194988055986    train_acc : 0.04502312296979354\n",
            "train loss : 0.00045759729033616024    train_acc : 0.03502401233225056\n",
            "train loss : 0.00035330734915990777    train_acc : 0.04001867947324386\n",
            "train loss : 0.0004934637233006555    train_acc : 0.036688009962385734\n",
            "train loss : 0.00030682401490128764    train_acc : 0.04001956693864661\n",
            "train loss : 0.000342411379902267    train_acc : 0.038354677102367285\n",
            "train loss : 0.00016373023063918682    train_acc : 0.05002045582778793\n",
            "train loss : 0.00035279636184200173    train_acc : 0.04336001090977482\n",
            "train loss : 0.0004389722487375462    train_acc : 0.03502312533915188\n",
            "train loss : 0.000249157531491441    train_acc : 0.04501867900018088\n",
            "train loss : 0.0003444204066506659    train_acc : 0.0416906766288001\n",
            "train loss : 0.00029625760445776576    train_acc : 0.04668890169420203\n",
            "train loss : 0.00032383455841768997    train_acc : 0.04169156741423691\n",
            "train loss : 0.0003230310771412798    train_acc : 0.04168890216928759\n",
            "train loss : 0.0004473235626906049    train_acc : 0.03335556741449029\n",
            "train loss : 0.0001679690788279581    train_acc : 0.04835112296928773\n",
            "train loss : 0.00034064057838175516    train_acc : 0.040025787265583625\n",
            "train loss : 0.000362600199399372    train_acc : 0.04002134708654164\n",
            "train loss : 0.0004165816759067207    train_acc : 0.03835467805177949\n",
            "train loss : 0.00044155245083994324    train_acc : 0.03668712249496095\n",
            "train loss : 0.000491255347596584    train_acc : 0.04168623313199732\n",
            "train loss : 0.0003271716085140795    train_acc : 0.041688899324337066\n",
            "train loss : 0.0004272391328412726    train_acc : 0.03668890074630632\n",
            "train loss : 0.00035738378789314685    train_acc : 0.0383529007470647\n",
            "train loss : 0.0005089817970595163    train_acc : 0.03502045488039844\n",
            "train loss : 0.0002956372037744311    train_acc : 0.043352010909269545\n",
            "train loss : 0.0004197148949058965    train_acc : 0.04335645440581828\n",
            "train loss : 0.0004538691090726933    train_acc : 0.03668979010901644\n",
            "train loss : 0.0003785715312982763    train_acc : 0.04001956788805815\n",
            "train loss : 0.00047497547074483367    train_acc : 0.03835467710287363\n",
            "train loss : 0.0004081079090775613    train_acc : 0.041687122494454866\n",
            "train loss : 0.00021771397318973233    train_acc : 0.04502223313199704\n",
            "train loss : 0.0003384129090103919    train_acc : 0.038357345191003735\n",
            "train loss : 0.00034428681139855876    train_acc : 0.04335379058410187\n",
            "train loss : 0.00039486720321021984    train_acc : 0.040023122021644854\n",
            "train loss : 0.0005918475714479947    train_acc : 0.03668801233174488\n",
            "train loss : 0.0003248604950760288    train_acc : 0.04168623360657693\n",
            "train loss : 0.0003325478634737574    train_acc : 0.041688899324590176\n",
            "train loss : 0.0004277352723228403    train_acc : 0.036688900746306444\n",
            "train loss : 0.00030300563834681764    train_acc : 0.04001956741373137\n",
            "train loss : 0.00023356663675447347    train_acc : 0.04502134376928733\n",
            "train loss : 0.0004406747372282417    train_acc : 0.038357344716676955\n",
            "train loss : 0.0005073522661607177    train_acc : 0.03168712391718223\n",
            "train loss : 0.00048175671763602793    train_acc : 0.0350168997994225\n",
            "train loss : 0.0005104959976358805    train_acc : 0.03835200901322636\n",
            "train loss : 0.00033860459968628793    train_acc : 0.04502045440480705\n",
            "train loss : 0.0002914960811140547    train_acc : 0.04335734424234923\n",
            "train loss : 0.0003378684836297354    train_acc : 0.04335645725026259\n",
            "train loss : 0.00038187890029047167    train_acc : 0.041689790110533476\n",
            "train loss : 0.00035808229016609245    train_acc : 0.04335556788805896\n",
            "train loss : 0.00026217572288283954    train_acc : 0.046689789636206964\n",
            "train loss : 0.0004887469793552641    train_acc : 0.036691567887805976\n",
            "train loss : 0.0004888464232648847    train_acc : 0.03668623550287349\n",
            "train loss : 0.00043109501145788644    train_acc : 0.0400195659922682\n",
            "train loss : 0.00030349825458920006    train_acc : 0.040021343768529206\n",
            "train loss : 0.0006631283396330792    train_acc : 0.03668801138334322\n",
            "train loss : 0.0005428951723540543    train_acc : 0.03501956693940445\n",
            "train loss : 0.0004574378417196891    train_acc : 0.03668534376903435\n",
            "train loss : 0.00031014873293168084    train_acc : 0.04501956551667682\n",
            "train loss : 0.00022671530826201506    train_acc : 0.04502401043494223\n",
            "train loss : 0.00031702091381382045    train_acc : 0.04169067947223197\n",
            "train loss : 0.00028404365311773843    train_acc : 0.04168890169571852\n",
            "train loss : 0.0005871728090197812    train_acc : 0.03835556741423772\n",
            "train loss : 0.00020246800184396907    train_acc : 0.0466871229692876\n",
            "train loss : 0.00040260940708796263    train_acc : 0.040024899798916956\n",
            "train loss : 0.0003181269221301507    train_acc : 0.04168801327989276\n",
            "train loss : 0.0005357675324785866    train_acc : 0.03668890027374928\n",
            "train loss : 0.00045997755329189877    train_acc : 0.03835290074681266\n",
            "train loss : 0.00035949058986558055    train_acc : 0.038353788213731636\n",
            "train loss : 0.0003356110970584915    train_acc : 0.041687122020380656\n",
            "train loss : 0.000340478344595189    train_acc : 0.04335556646507754\n",
            "train loss : 0.00036836831661003734    train_acc : 0.03835645630211471\n",
            "train loss : 0.00041369238519081507    train_acc : 0.03668712344336113\n",
            "train loss : 0.00027588687281620986    train_acc : 0.04501956646583646\n",
            "train loss : 0.00022010474566322332    train_acc : 0.046690677102115115\n",
            "train loss : 0.0004000519989287255    train_acc : 0.0383582350277878\n",
            "train loss : 0.000357909449221206    train_acc : 0.04168712439201482\n",
            "train loss : 0.00035547366854788543    train_acc : 0.03835556646634241\n",
            "train loss : 0.00038465483533603483    train_acc : 0.04002045630211538\n",
            "train loss : 0.00020390419760570137    train_acc : 0.0466880109100278\n",
            "train loss : 0.000461479534936477    train_acc : 0.04169156693915202\n",
            "train loss : 0.0004150721340763476    train_acc : 0.031688902169034215\n",
            "train loss : 0.00031588884210106467    train_acc : 0.040016900747823485\n",
            "train loss : 0.0002888814206230138    train_acc : 0.04502134234706551\n",
            "train loss : 0.0003081781904218095    train_acc : 0.043357344715918435\n",
            "train loss : 0.0004574511646292113    train_acc : 0.04168979058384849\n",
            "train loss : 0.00014310300372436636    train_acc : 0.048355567888311385\n",
            "train loss : 0.0004550246833166673    train_acc : 0.03835912296954043\n",
            "train loss : 0.00026368858170182976    train_acc : 0.046687124865583754\n",
            "train loss : 0.0001681533006125115    train_acc : 0.048358233133261645\n",
            "train loss : 0.0003091861452816483    train_acc : 0.038359124391004404\n",
            "train loss : 0.00045611693963189433    train_acc : 0.03835379153300854\n",
            "train loss : 0.00041066104165450617    train_acc : 0.04002045535548427\n",
            "train loss : 0.0002846091508547027    train_acc : 0.046688010909522924\n",
            "train loss : 0.0002690481169764031    train_acc : 0.045024900272485076\n",
            "train loss : 0.00022198121955785454    train_acc : 0.043357346613478655\n",
            "train loss : 0.0004180844379065344    train_acc : 0.038356457251527185\n",
            "train loss : 0.0003277986619606335    train_acc : 0.040020456777200814\n",
            "train loss : 0.0002824453210498881    train_acc : 0.04502134424361451\n",
            "train loss : 0.00046852943567147077    train_acc : 0.03502401138359659\n",
            "train loss : 0.0003217711009834486    train_acc : 0.04001867947273792\n",
            "train loss : 0.00018817229026997113    train_acc : 0.04668800996238546\n",
            "train loss : 0.0003701122604102794    train_acc : 0.040024900271979934\n",
            "train loss : 0.0004967891519335795    train_acc : 0.03835467994681172\n",
            "train loss : 0.0005185321724862559    train_acc : 0.0333537891626383\n",
            "train loss : 0.0004336450108293136    train_acc : 0.035017788687553404\n",
            "train loss : 0.0004763990833286923    train_acc : 0.038352009487300026\n",
            "train loss : 0.00029174310844177267    train_acc : 0.041687121071726556\n",
            "train loss : 0.00035834530630578487    train_acc : 0.03835556646457159\n",
            "train loss : 0.00020146141168614955    train_acc : 0.043353789635447774\n",
            "train loss : 0.0001072266407527772    train_acc : 0.050023122021138906\n",
            "train loss : 0.00025501856691083477    train_acc : 0.04336001233174461\n",
            "train loss : 0.0003989816507206304    train_acc : 0.038356458673243596\n",
            "train loss : 0.0003712191200680471    train_acc : 0.0383537901112924\n",
            "train loss : 0.00040516144508872707    train_acc : 0.03835378868805935\n",
            "train loss : 0.0004414913012529731    train_acc : 0.04002045535396696\n",
            "train loss : 0.00033253952646011484    train_acc : 0.03668801090952212\n",
            "train loss : 0.00031353887200240955    train_acc : 0.04168623360581841\n",
            "train loss : 0.0003674430627625777    train_acc : 0.0400222326579231\n",
            "train loss : 0.00035605006763558925    train_acc : 0.04002134519075089\n",
            "train loss : 0.0003616320142393601    train_acc : 0.04002134471743507\n",
            "train loss : 0.00021539598078458813    train_acc : 0.04502134471718263\n",
            "train loss : 0.0004901351211832413    train_acc : 0.038357344717182495\n",
            "train loss : 0.00046758388210287977    train_acc : 0.03502045725051583\n",
            "train loss : 0.00048779413718503823    train_acc : 0.038352010910533606\n",
            "train loss : 0.00028479149588276823    train_acc : 0.04668712107248561\n",
            "train loss : 0.0005038414322039728    train_acc : 0.031691566464571996\n",
            "train loss : 0.00028427531104208105    train_acc : 0.03835023550211444\n",
            "train loss : 0.00039007000440905316    train_acc : 0.038353786792267794\n",
            "train loss : 0.00035909138643643355    train_acc : 0.04335378868628921\n",
            "train loss : 0.0005042220927478149    train_acc : 0.03668978868729936\n",
            "train loss : 0.000282364255363528    train_acc : 0.041686234553966556\n",
            "train loss : 0.00031437763487825117    train_acc : 0.041688899325095445\n",
            "train loss : 0.00038888233125959783    train_acc : 0.038355567412973385\n",
            "train loss : 0.0003221203283586542    train_acc : 0.04168712296928692\n",
            "train loss : 0.0003965034892504013    train_acc : 0.04002223313225029\n",
            "train loss : 0.0002981780441501588    train_acc : 0.04668801185767053\n",
            "train loss : 0.0003160481007084263    train_acc : 0.04169156693965742\n",
            "train loss : 0.00027320500714785175    train_acc : 0.04168890216903449\n",
            "train loss : 0.00032941317937905634    train_acc : 0.041688900747823485\n",
            "train loss : 0.0003291059696217756    train_acc : 0.038355567413732174\n",
            "train loss : 0.0002125032896022553    train_acc : 0.04502045630262066\n",
            "train loss : 0.00030224829399078264    train_acc : 0.0433573442433614\n",
            "train loss : 0.000289362077233439    train_acc : 0.045023123916929794\n",
            "train loss : 0.0002759556036018357    train_acc : 0.041690678999422366\n",
            "train loss : 0.0005985781277582816    train_acc : 0.03335556836213303\n",
            "train loss : 0.00040776712018783176    train_acc : 0.03835112296979314\n",
            "train loss : 0.0003471953454670299    train_acc : 0.04168712059891723\n",
            "train loss : 0.00045202658879932067    train_acc : 0.03502223313098609\n",
            "train loss : 0.00031034506949482055    train_acc : 0.04168534519100319\n",
            "train loss : 0.00036657005075662767    train_acc : 0.040022232184101864\n",
            "train loss : 0.00024388721863176585    train_acc : 0.04335467852383152\n",
            "train loss : 0.00025642782047737836    train_acc : 0.040023122495212705\n",
            "train loss : 0.00041478087568068694    train_acc : 0.03502134566533078\n",
            "train loss : 0.0004799137073626189    train_acc : 0.038352011384354845\n",
            "train loss : 0.0004132364468870094    train_acc : 0.038353787739404996\n",
            "train loss : 0.00044604428472927395    train_acc : 0.03835378868679435\n",
            "train loss : 0.00026206610620214043    train_acc : 0.04668712202063296\n",
            "train loss : 0.0002740872300361008    train_acc : 0.04169156646507767\n",
            "train loss : 0.0002421311830095023    train_acc : 0.045022235502114705\n",
            "train loss : 0.0003390060516089658    train_acc : 0.04002401185893446\n",
            "train loss : 0.00032157197245766804    train_acc : 0.04335467947299143\n",
            "train loss : 0.0003394826915245523    train_acc : 0.041689789162385596\n",
            "train loss : 0.0002964465323907206    train_acc : 0.04335556788755327\n",
            "train loss : 0.00046402465653439743    train_acc : 0.0366897896362067\n",
            "train loss : 0.0007068636907543184    train_acc : 0.03168623455447264\n",
            "train loss : 0.00048737450297108785    train_acc : 0.03335023265842905\n",
            "train loss : 0.0006205378464797096    train_acc : 0.0333511201240845\n",
            "train loss : 0.0005943112340763907    train_acc : 0.03501778726406617\n",
            "train loss : 0.00034684012374704127    train_acc : 0.03668534281987417\n",
            "train loss : 0.00029871114132161365    train_acc : 0.04335289884950393\n",
            "train loss : 0.00044033942354010136    train_acc : 0.03502312154605307\n",
            "train loss : 0.0005635047807573334    train_acc : 0.03668534566482456\n",
            "train loss : 0.00024672948651071545    train_acc : 0.04335289885102124\n",
            "train loss : 0.00044306839096987276    train_acc : 0.03668978821272055\n",
            "train loss : 0.00041937874012305175    train_acc : 0.03835290122038012\n",
            "train loss : 0.000472784106181744    train_acc : 0.036687121547317535\n",
            "train loss : 0.00035778067910101504    train_acc : 0.04001956646482523\n",
            "train loss : 0.0003591974119737594    train_acc : 0.03835467710211457\n",
            "train loss : 0.0001818206733883961    train_acc : 0.04835378916112113\n",
            "train loss : 0.00018386993232443182    train_acc : 0.048359122020885936\n",
            "train loss : 0.00035140828531807304    train_acc : 0.03669245819841114\n",
            "train loss : 0.000347413942105197    train_acc : 0.04168623597770582\n",
            "train loss : 0.00047626615011686044    train_acc : 0.033355565992521444\n",
            "train loss : 0.0002549961382571538    train_acc : 0.04668445630186268\n",
            "train loss : 0.0003911018093519436    train_acc : 0.040024898376694325\n",
            "train loss : 0.00018014493304994864    train_acc : 0.04835467994580091\n",
            "train loss : 0.0002990940199042347    train_acc : 0.03835912249597109\n",
            "train loss : 0.00024775633700613157    train_acc : 0.04502045819866452\n",
            "train loss : 0.0003302261244958662    train_acc : 0.040024010911039284\n",
            "train loss : 0.0001703174360599362    train_acc : 0.04835467947248589\n",
            "train loss : 0.00025228136229239604    train_acc : 0.043359122495718654\n",
            "train loss : 0.00019892325652147783    train_acc : 0.04502312486533105\n",
            "train loss : 0.00023306111903006503    train_acc : 0.04502401233326151\n",
            "train loss : 0.0003254897154723857    train_acc : 0.0416906794732444\n",
            "train loss : 0.00040451106425165367    train_acc : 0.041688901695719066\n",
            "train loss : 0.0004491793177763411    train_acc : 0.03668890074757105\n",
            "train loss : 0.0003224580757915836    train_acc : 0.04001956741373204\n",
            "train loss : 0.0001877772632849534    train_acc : 0.04668801043595399\n",
            "train loss : 0.0003085828901833223    train_acc : 0.04002490027223251\n",
            "train loss : 0.0001756036333021326    train_acc : 0.048354679946811856\n",
            "train loss : 0.00035032810020721103    train_acc : 0.0400257891626383\n",
            "train loss : 0.00043552247779235986    train_acc : 0.04002134708755341\n",
            "train loss : 0.00031868807012821126    train_acc : 0.0400213447184467\n",
            "train loss : 0.00025578706101065204    train_acc : 0.046688011383849835\n",
            "train loss : 0.00029872399631413084    train_acc : 0.04169156693940472\n",
            "train loss : 0.0004286082508408402    train_acc : 0.03668890216903435\n",
            "train loss : 0.00015323053529115482    train_acc : 0.04668623408115682\n",
            "train loss : 0.0005861110299996813    train_acc : 0.030024899324843285\n",
            "train loss : 0.0005160088320207688    train_acc : 0.036682679946306584\n",
            "train loss : 0.0003860964200220973    train_acc : 0.03668623076263803\n",
            "train loss : 0.00031575195385728    train_acc : 0.040019565989740075\n",
            "train loss : 0.00021579602364642735    train_acc : 0.04668801043519453\n",
            "train loss : 0.0003564754631201635    train_acc : 0.03835823360556544\n",
            "train loss : 0.00037682370077271355    train_acc : 0.040020457724589634\n",
            "train loss : 0.000411575032953319    train_acc : 0.04002134424411978\n",
            "train loss : 0.0004066583659880341    train_acc : 0.03835467805026353\n",
            "train loss : 0.0003217904597995882    train_acc : 0.04335378916162681\n",
            "train loss : 0.0006692582686122512    train_acc : 0.031689788687552865\n",
            "train loss : 0.00021665070185763975    train_acc : 0.0483502345539667\n",
            "train loss : 0.0004234434062837479    train_acc : 0.04169245345842878\n",
            "train loss : 0.0002667810663061459    train_acc : 0.041688902641844494\n",
            "train loss : 0.0003613244324866654    train_acc : 0.03835556741474231\n",
            "train loss : 0.0002985759422953894    train_acc : 0.040020456302621195\n",
            "train loss : 0.00036976633939985576    train_acc : 0.03835467757669474\n",
            "train loss : 0.0005538587458847567    train_acc : 0.033353789161374235\n",
            "train loss : 0.00027215491001806886    train_acc : 0.045017788687552726\n",
            "train loss : 0.0004957569122858313    train_acc : 0.03502400948730003\n",
            "train loss : 0.00044078182922366186    train_acc : 0.033352012805059894\n",
            "train loss : 0.0003054280946082357    train_acc : 0.0450177877401627\n",
            "train loss : 0.00036417637612392776    train_acc : 0.04002400948679476\n",
            "train loss : 0.0005955994881224419    train_acc : 0.03502134613839296\n",
            "train loss : 0.00034523628556765623    train_acc : 0.04001867805127381\n",
            "train loss : 0.00036109911930510224    train_acc : 0.03835467662829401\n",
            "train loss : 0.00030551227690130844    train_acc : 0.04335378916086842\n",
            "Looked at 25600/60000 samples\n",
            "train loss : 0.0002145937489020101    train_acc : 0.043356455354219135\n",
            "train loss : 0.0006099482945021813    train_acc : 0.03168979010952225\n",
            "train loss : 0.00036916409283592526    train_acc : 0.03835023455472508\n",
            "train loss : 0.00022377828107595293    train_acc : 0.04835378679176252\n",
            "train loss : 0.00019745810434207027    train_acc : 0.04669245535295561\n",
            "train loss : 0.00035281870515402146    train_acc : 0.03835823597618824\n",
            "train loss : 0.00031495521277189923    train_acc : 0.04168712439252063\n",
            "train loss : 0.000500262262714227    train_acc : 0.033355566466342676\n",
            "train loss : 0.0005444258692435648    train_acc : 0.035017789635448715\n",
            "train loss : 0.00037776787918104447    train_acc : 0.04001867615447224\n",
            "train loss : 0.00024462887106917327    train_acc : 0.048354676627282385\n",
            "train loss : 0.00039224793724726876    train_acc : 0.04169245582753455\n",
            "train loss : 0.00044718964349011586    train_acc : 0.04002223597644135\n",
            "train loss : 0.0003442849031207338    train_acc : 0.04002134519252076\n",
            "train loss : 0.00040379833037647235    train_acc : 0.04002134471743601\n",
            "train loss : 0.0002842131491282516    train_acc : 0.04002134471718263\n",
            "train loss : 0.00031622506158806375    train_acc : 0.043354678050515835\n",
            "train loss : 0.00028504267934681177    train_acc : 0.041689789161626946\n",
            "train loss : 0.000392033107204457    train_acc : 0.040022234554219535\n",
            "train loss : 0.0002435891540565084    train_acc : 0.04668801185842891\n",
            "train loss : 0.0003035432099613464    train_acc : 0.0433582336063245\n",
            "train loss : 0.000416423085848454    train_acc : 0.03502312439125671\n",
            "train loss : 0.00038981445376264784    train_acc : 0.03501867899967534\n",
            "train loss : 0.00018758142793307933    train_acc : 0.05001867662879983\n",
            "train loss : 0.00041415437361825376    train_acc : 0.03836000996086869\n",
            "train loss : 0.0004168735299868119    train_acc : 0.033353792005312466\n",
            "train loss : 0.0002454933795057065    train_acc : 0.0450177886890695\n",
            "train loss : 0.00022171483661189196    train_acc : 0.045024009487300835\n",
            "train loss : 0.0007428144377932308    train_acc : 0.02669067947172656\n",
            "train loss : 0.000531697221065228    train_acc : 0.035014235029051585\n",
            "train loss : 0.0003105708884914529    train_acc : 0.04668534092534883\n",
            "train loss : 0.00038457398461912574    train_acc : 0.043358232181826856\n",
            "train loss : 0.0004194316889701823    train_acc : 0.03668979105716364\n",
            "train loss : 0.00033295373586155884    train_acc : 0.03835290122189715\n",
            "train loss : 0.0003216715824363885    train_acc : 0.041687121547318344\n",
            "train loss : 0.0002603247564194869    train_acc : 0.0450222331314919\n",
            "train loss : 0.0003661297011270077    train_acc : 0.0416906785243368\n",
            "train loss : 0.00045915087499872286    train_acc : 0.04002223502854631\n",
            "train loss : 0.00037592545424799733    train_acc : 0.03835467852534856\n",
            "train loss : 0.0004863003089442513    train_acc : 0.035020455828546855\n",
            "train loss : 0.00023782443254229306    train_acc : 0.04668534424310856\n",
            "train loss : 0.00038622067870695874    train_acc : 0.04169156551692965\n",
            "train loss : 0.00026343020463785597    train_acc : 0.04168890216827569\n",
            "train loss : 0.00040596725544751907    train_acc : 0.03835556741448975\n",
            "train loss : 0.0003767006238407634    train_acc : 0.04335378963595439\n",
            "train loss : 0.00038781719168769555    train_acc : 0.03835645535447251\n",
            "train loss : 0.0004553083894346065    train_acc : 0.03835379010952238\n",
            "train loss : 0.0003388753193917642    train_acc : 0.04002045535472508\n",
            "train loss : 0.00024351836472926154    train_acc : 0.046688010909522515\n",
            "train loss : 0.0005375931282108634    train_acc : 0.03502490027248508\n",
            "train loss : 0.0004644947470357294    train_acc : 0.035018679946811994\n",
            "train loss : 0.0002781968729878071    train_acc : 0.0433520099626383\n",
            "train loss : 0.00040997531792413844    train_acc : 0.04168978773864674\n",
            "train loss : 0.0004738661525873656    train_acc : 0.038355567886793944\n",
            "train loss : 0.00030618261193583957    train_acc : 0.04335378963620629\n",
            "train loss : 0.0002972436552624986    train_acc : 0.04335645535447264\n",
            "train loss : 0.00034680601504672267    train_acc : 0.045023123442855716\n",
            "train loss : 0.00022645417656658612    train_acc : 0.048357345665836195\n",
            "train loss : 0.00026010526881645077    train_acc : 0.04169245725102178\n",
            "train loss : 0.0002629478563396985    train_acc : 0.04335556931053387\n",
            "train loss : 0.0002308183362637376    train_acc : 0.040023122970298954\n",
            "train loss : 0.0004036778923379344    train_acc : 0.038354678998917494\n",
            "train loss : 0.00035898226392091685    train_acc : 0.04002045582879942\n",
            "train loss : 0.0004399900984122585    train_acc : 0.03835467757644202\n",
            "train loss : 0.00038329053907136675    train_acc : 0.041687122494707435\n",
            "train loss : 0.00026603566958633286    train_acc : 0.04668889979866384\n",
            "train loss : 0.0004535797226553712    train_acc : 0.038358234079892625\n",
            "train loss : 0.00034461421346601513    train_acc : 0.043353791058175946\n",
            "train loss : 0.00042780019973424243    train_acc : 0.04002312202189769\n",
            "train loss : 0.00028465656586824364    train_acc : 0.04002134566507834\n",
            "train loss : 0.00023134618583333936    train_acc : 0.04835467805102138\n",
            "train loss : 0.0006158709635712466    train_acc : 0.03502578916162721\n",
            "train loss : 0.0005168131383318571    train_acc : 0.0400186804208862\n",
            "train loss : 0.00038018740608385186    train_acc : 0.04168800996289114\n",
            "train loss : 0.0003016475514167896    train_acc : 0.04168890027198021\n",
            "train loss : 0.0003855699795445121    train_acc : 0.03835556741347839\n",
            "train loss : 0.00027859973242536974    train_acc : 0.043353789635953856\n",
            "train loss : 0.00041999181518200055    train_acc : 0.040023122021139175\n",
            "train loss : 0.0004171886855253074    train_acc : 0.03668801233174461\n",
            "train loss : 0.0003714809819758674    train_acc : 0.04001956693991027\n",
            "train loss : 0.0003739131424164613    train_acc : 0.040021343769034615\n",
            "train loss : 0.0006009047578670362    train_acc : 0.031688011383343484\n",
            "train loss : 0.0004641666574134243    train_acc : 0.036683566939404444\n",
            "train loss : 0.0004536607241398946    train_acc : 0.04168623123570101\n",
            "train loss : 0.00036261082216770206    train_acc : 0.038355565989992374\n",
            "train loss : 0.0002715655517605923    train_acc : 0.04668712296852799\n",
            "train loss : 0.00044432640437825667    train_acc : 0.04002489979891655\n",
            "train loss : 0.0002708991546650174    train_acc : 0.04335467994655943\n",
            "train loss : 0.0001787714460255951    train_acc : 0.04835645582930483\n",
            "train loss : 0.00047580274858997993    train_acc : 0.028359123443108963\n",
            "train loss : 0.00020252954627629879    train_acc : 0.05001512486583633\n",
            "train loss : 0.00021445200862707327    train_acc : 0.046693341399928444\n",
            "train loss : 0.0003074335068915802    train_acc : 0.04169156978207996\n",
            "train loss : 0.0004275465926599743    train_acc : 0.03335556883721711\n",
            "train loss : 0.00024021314117225395    train_acc : 0.045017789636713185\n",
            "train loss : 0.00037056335155981335    train_acc : 0.040024009487806246\n",
            "train loss : 0.0004180466340825018    train_acc : 0.04168801280506017\n",
            "train loss : 0.00036654871809253933    train_acc : 0.0383555669401627\n",
            "train loss : 0.0001624378098493564    train_acc : 0.04835378963570142\n",
            "train loss : 0.00029873438689621005    train_acc : 0.04335912202113904\n",
            "train loss : 0.00033267580449853213    train_acc : 0.03668979153174461\n",
            "train loss : 0.00033112037575499365    train_acc : 0.038352901222150265\n",
            "train loss : 0.00046339865614783107    train_acc : 0.036687121547318485\n",
            "train loss : 0.0004862051181586044    train_acc : 0.03835289979815857\n",
            "train loss : 0.0002959670438222139    train_acc : 0.043353788213225684\n",
            "train loss : 0.00032948660860135034    train_acc : 0.04168978868704705\n",
            "train loss : 0.0005016743300120548    train_acc : 0.03835556788729976\n",
            "train loss : 0.0006260456943065728    train_acc : 0.03668712296953989\n",
            "train loss : 0.0002935656644134283    train_acc : 0.04001956646558375\n",
            "train loss : 0.0004962539741059    train_acc : 0.03835467710211498\n",
            "train loss : 0.00032736389669146327    train_acc : 0.041687122494454464\n",
            "train loss : 0.00025028164402025204    train_acc : 0.04335556646533038\n",
            "train loss : 0.00028205064230609145    train_acc : 0.04502312296878151\n",
            "train loss : 0.000426733335639601    train_acc : 0.03669067899891668\n",
            "train loss : 0.0001695970184754189    train_acc : 0.04668623502879943\n",
            "train loss : 0.0005345406012793196    train_acc : 0.03502489932534869\n",
            "train loss : 0.000202820256650557    train_acc : 0.04335201327964019\n",
            "train loss : 0.0003678707364163399    train_acc : 0.03835645440708248\n",
            "train loss : 0.0004545296620306948    train_acc : 0.03502045677568377\n",
            "train loss : 0.00041973170414982787    train_acc : 0.038352010910280364\n",
            "train loss : 0.00046869745030284147    train_acc : 0.03335378773915215\n",
            "train loss : 0.00029573340220013876    train_acc : 0.045017788686794215\n",
            "train loss : 0.0004346284415933967    train_acc : 0.03669067615396629\n",
            "train loss : 0.0004618750208296571    train_acc : 0.03835290169394878\n",
            "train loss : 0.0002938648534453539    train_acc : 0.04168712154757011\n",
            "train loss : 0.00038117943554309404    train_acc : 0.038355566464825366\n",
            "train loss : 0.00014812965805450647    train_acc : 0.04835378963544791\n",
            "train loss : 0.00023912823051672573    train_acc : 0.04669245535447224\n",
            "train loss : 0.0003612966189694429    train_acc : 0.041691569309522385\n",
            "train loss : 0.00037967410251034167    train_acc : 0.040022235503631744\n",
            "train loss : 0.0004669215162413096    train_acc : 0.03668801185893527\n",
            "train loss : 0.0004901951676411001    train_acc : 0.0350195669396581\n",
            "train loss : 0.0002782726640767947    train_acc : 0.04168534376903448\n",
            "train loss : 0.0004067545140772537    train_acc : 0.03668889885001015\n",
            "train loss : 0.0004774958358873256    train_acc : 0.03835290074605334\n",
            "train loss : 0.00039000190260498626    train_acc : 0.04002045488039789\n",
            "train loss : 0.00027905712592316993    train_acc : 0.04502134424260287\n",
            "train loss : 0.0003192959086633667    train_acc : 0.04169067805026272\n",
            "train loss : 0.00032123177517024874    train_acc : 0.04002223502829347\n",
            "train loss : 0.00013806997537225388    train_acc : 0.050021345192015086\n",
            "train loss : 0.000196429981543221    train_acc : 0.04669334471743575\n",
            "train loss : 0.0003662052964601271    train_acc : 0.04002490311718263\n",
            "train loss : 0.0003452532420272527    train_acc : 0.0416880132816625\n",
            "train loss : 0.00047442812564265545    train_acc : 0.03502223360708355\n",
            "train loss : 0.000537093594333025    train_acc : 0.03168534519125711\n",
            "train loss : 0.00018048294179421557    train_acc : 0.04501689885076867\n",
            "train loss : 0.00029092254642800635    train_acc : 0.040024009012720414\n",
            "train loss : 0.0003516565917460344    train_acc : 0.040021346138140124\n",
            "train loss : 0.0003450156154186741    train_acc : 0.04335467805127368\n",
            "train loss : 0.00042166641240130273    train_acc : 0.038356455828294014\n",
            "train loss : 0.00023737407965599067    train_acc : 0.04668712344310842\n",
            "train loss : 0.0002544543576624295    train_acc : 0.04335823313250299\n",
            "train loss : 0.00040722026938504045    train_acc : 0.04002312439100401\n",
            "train loss : 0.0002874724395249383    train_acc : 0.0433546789996752\n",
            "train loss : 0.00037032718617795174    train_acc : 0.040023122495466495\n",
            "train loss : 0.0003607711810809192    train_acc : 0.041688012331997586\n",
            "train loss : 0.0003925849512746038    train_acc : 0.035022233606577066\n",
            "train loss : 0.00043082451764824817    train_acc : 0.035018678524590174\n",
            "train loss : 0.00032919782117178214    train_acc : 0.04335200996187978\n",
            "train loss : 0.00035799392137376556    train_acc : 0.04002312107197967\n",
            "train loss : 0.0003432832762828056    train_acc : 0.04168801233123839\n",
            "train loss : 0.00022677298665604224    train_acc : 0.04502223360657666\n",
            "train loss : 0.000350622832596464    train_acc : 0.04169067852459017\n",
            "train loss : 0.0002112051320075036    train_acc : 0.04502223502854645\n",
            "train loss : 0.000327362360449473    train_acc : 0.04002401185868189\n",
            "train loss : 0.000142173249849686    train_acc : 0.046688012806324626\n",
            "train loss : 0.0003069666312955904    train_acc : 0.04335823360683004\n",
            "train loss : 0.0005975990309867236    train_acc : 0.03835645772459031\n",
            "train loss : 0.00029614868316727496    train_acc : 0.045020456777453116\n",
            "train loss : 0.0003550874885058598    train_acc : 0.03835734424361464\n",
            "train loss : 0.000367053870502903    train_acc : 0.0383537905835966\n",
            "train loss : 0.00040357310428838935    train_acc : 0.04002045535497792\n",
            "train loss : 0.00045814732974823116    train_acc : 0.03502134424285599\n",
            "train loss : 0.0005321535885083201    train_acc : 0.03668534471692952\n",
            "train loss : 0.0003230257687791055    train_acc : 0.04001956551718236\n",
            "train loss : 0.00022919982188849048    train_acc : 0.04502134376827583\n",
            "train loss : 0.0003260916498476342    train_acc : 0.04169067805000974\n",
            "train loss : 0.0003637468627308302    train_acc : 0.038355568361626666\n",
            "train loss : 0.0003278899720409825    train_acc : 0.041687122969792864\n",
            "train loss : 0.00025106369911464583    train_acc : 0.04168889979891722\n",
            "train loss : 0.00023812625089994447    train_acc : 0.04335556741322609\n",
            "train loss : 0.00040030670469750143    train_acc : 0.03835645630262039\n",
            "train loss : 0.0003938098281876551    train_acc : 0.03835379011002807\n",
            "train loss : 0.0003194712331311044    train_acc : 0.04335378868805868\n",
            "train loss : 0.0004925655631346393    train_acc : 0.0366897886873003\n",
            "train loss : 0.0003539789161315559    train_acc : 0.04001956788729989\n",
            "train loss : 0.0002910453726684375    train_acc : 0.043354677102873225\n",
            "train loss : 0.00041123342294957033    train_acc : 0.0383564558277882\n",
            "train loss : 0.0003521644068388381    train_acc : 0.03835379010977482\n",
            "train loss : 0.000250035694068942    train_acc : 0.04335378868805855\n",
            "train loss : 0.0003777182866047892    train_acc : 0.036689788687300294\n",
            "train loss : 0.00032798249121607204    train_acc : 0.043352901220633226\n",
            "train loss : 0.0003564253054650418    train_acc : 0.038356454880651\n",
            "train loss : 0.0006115749939072181    train_acc : 0.03335379010926968\n",
            "train loss : 0.0002705139327294979    train_acc : 0.04335112202139161\n",
            "train loss : 0.0005031233619270456    train_acc : 0.03502312059841141\n",
            "train loss : 0.00040719607700722045    train_acc : 0.03835201233098582\n",
            "train loss : 0.000382931347419049    train_acc : 0.0416871210732432\n",
            "train loss : 0.0003067766355491899    train_acc : 0.04168889979790573\n",
            "train loss : 0.0003378509476305123    train_acc : 0.041688900746558885\n",
            "train loss : 0.00021177867884077076    train_acc : 0.04502223408039817\n",
            "train loss : 0.00039298401058957124    train_acc : 0.036690678524842875\n",
            "train loss : 0.00047798719970799155    train_acc : 0.03335290169521325\n",
            "train loss : 0.0002821229392526861    train_acc : 0.04335112154757078\n",
            "train loss : 0.0002799056764440663    train_acc : 0.04168978726482537\n",
            "train loss : 0.00039916194400766147    train_acc : 0.04002223455320791\n",
            "train loss : 0.0002645198766757201    train_acc : 0.04668801185842838\n",
            "train loss : 0.000428921919094953    train_acc : 0.04169156693965782\n",
            "train loss : 0.00037318636944083504    train_acc : 0.04335556883570115\n",
            "train loss : 0.0005078278550570286    train_acc : 0.03335645630337904\n",
            "train loss : 0.0004025600731409425    train_acc : 0.04001779011002847\n",
            "train loss : 0.0003789626705582956    train_acc : 0.04002134282139201\n",
            "train loss : 0.0005097680390224422    train_acc : 0.038354678049504744\n",
            "train loss : 0.0003949652827104116    train_acc : 0.0433537891616264\n",
            "train loss : 0.000344965312253969    train_acc : 0.04335645535421954\n",
            "train loss : 0.00041545046353518776    train_acc : 0.04168979010952225\n",
            "train loss : 0.0002789766662014017    train_acc : 0.04335556788805841\n",
            "train loss : 0.0005096513891788425    train_acc : 0.040023122969540297\n",
            "train loss : 0.00045973970195264664    train_acc : 0.03502134566558376\n",
            "train loss : 0.0004444568046659926    train_acc : 0.03668534471768831\n",
            "train loss : 0.0006768564461967008    train_acc : 0.026686232183849433\n",
            "train loss : 0.00034327420447027954    train_acc : 0.04001423265716472\n",
            "train loss : 0.00027083613667410616    train_acc : 0.04168800759075049\n",
            "train loss : 0.00025083113913108786    train_acc : 0.04168890027071507\n",
            "train loss : 0.0003178022463544441    train_acc : 0.04502223408014438\n",
            "train loss : 0.00034721056101901925    train_acc : 0.038357345191509414\n",
            "train loss : 0.00037718547905539506    train_acc : 0.03835379058410214\n",
            "train loss : 0.000350223068308963    train_acc : 0.03835378868831152\n",
            "train loss : 0.00036316610234785724    train_acc : 0.0400204553539671\n",
            "train loss : 0.0003865800512995074    train_acc : 0.03502134424285545\n",
            "train loss : 0.00027845454969475887    train_acc : 0.045018678050262856\n",
            "train loss : 0.00023176583265070635    train_acc : 0.04669067662829348\n",
            "train loss : 0.00028285615874722164    train_acc : 0.04335823502753509\n",
            "train loss : 0.0004256644102373833    train_acc : 0.040023124392014685\n",
            "train loss : 0.0005155300605695746    train_acc : 0.033354678999675744\n",
            "train loss : 0.0003057243079878702    train_acc : 0.04001778916213316\n",
            "train loss : 0.0003870549584309757    train_acc : 0.03668800948755314\n",
            "train loss : 0.00020730066949976183    train_acc : 0.048352900271726694\n",
            "train loss : 0.0006781246798232124    train_acc : 0.030025788213478254\n",
            "train loss : 0.00042442897301446037    train_acc : 0.036682680420380524\n",
            "train loss : 0.0003323659495212848    train_acc : 0.03668623076289087\n",
            "train loss : 0.00024636889061375187    train_acc : 0.04501956598974021\n",
            "train loss : 0.0003419700108875754    train_acc : 0.040024010435194525\n",
            "train loss : 0.00029234826738796816    train_acc : 0.04168801280556544\n",
            "train loss : 0.0003665103527714155    train_acc : 0.0416889002734963\n",
            "train loss : 0.00046830225065087265    train_acc : 0.03502223408014587\n",
            "train loss : 0.0002885115880334363    train_acc : 0.043352011858176075\n",
            "train loss : 0.00027712597596584766    train_acc : 0.04335645440632436\n",
            "train loss : 0.00033222558982715577    train_acc : 0.03835645677568337\n",
            "train loss : 0.0004674053770854078    train_acc : 0.03835379011028037\n",
            "train loss : 0.00040841054156528866    train_acc : 0.03835378868805882\n",
            "train loss : 0.0002134629405934816    train_acc : 0.04502045535396696\n",
            "train loss : 0.00044049600963154894    train_acc : 0.03669067757618878\n",
            "train loss : 0.0002175328825855486    train_acc : 0.04668623502804063\n",
            "train loss : 0.00018356798795567323    train_acc : 0.04335823265868162\n",
            "train loss : 0.000324361157728342    train_acc : 0.03668979105741796\n",
            "train loss : 0.00023403825369533598    train_acc : 0.045019567888563955\n",
            "train loss : 0.0002797591053751154    train_acc : 0.0416906771028739\n",
            "train loss : 0.00035163754078352757    train_acc : 0.0400222350277882\n",
            "train loss : 0.000520768658287539    train_acc : 0.043354678525348155\n",
            "train loss : 0.0003202201222822228    train_acc : 0.04168978916188019\n",
            "train loss : 0.0002732239533157055    train_acc : 0.043355567887553\n",
            "train loss : 0.00038437677819827884    train_acc : 0.03835645630287336\n",
            "train loss : 0.00023016561737801923    train_acc : 0.04668712344336153\n",
            "train loss : 0.00031099241696289456    train_acc : 0.04169156646583646\n",
            "train loss : 0.00046237581847361717    train_acc : 0.03668890216878178\n",
            "train loss : 0.00031687438898022046    train_acc : 0.03835290074782335\n",
            "train loss : 0.0005459891723018897    train_acc : 0.03168712154706551\n",
            "train loss : 0.0003583479937959471    train_acc : 0.0366835664648251\n",
            "train loss : 0.0003821366336089796    train_acc : 0.040019564568781245\n",
            "train loss : 0.00028782762492000484    train_acc : 0.04335467710110335\n",
            "train loss : 0.0002648550970151917    train_acc : 0.04668978916112059\n",
            "train loss : 0.00048695429265249013    train_acc : 0.03502490122088593\n",
            "train loss : 0.0007737141207806256    train_acc : 0.03168534661398447\n",
            "train loss : 0.00046971183678075747    train_acc : 0.03835023218486079\n",
            "train loss : 0.00044210378192085993    train_acc : 0.03502045345716526\n",
            "train loss : 0.000377538181007503    train_acc : 0.04001867757517715\n",
            "train loss : 0.00039490296171859135    train_acc : 0.036688009961373426\n",
            "train loss : 0.00032928283603311516    train_acc : 0.04168623360531273\n",
            "train loss : 0.00045997020242262584    train_acc : 0.0416888993245895\n",
            "train loss : 0.0003091576264521156    train_acc : 0.04002223407963978\n",
            "train loss : 0.0003911515173682224    train_acc : 0.04002134519150914\n",
            "train loss : 0.0004901198524950541    train_acc : 0.03502134471743547\n",
            "train loss : 0.0004048561047436777    train_acc : 0.04168534471718263\n",
            "train loss : 0.0003670801912408894    train_acc : 0.04002223218384916\n",
            "train loss : 0.00023604084916616343    train_acc : 0.04502134519049805\n",
            "train loss : 0.00037765090015129356    train_acc : 0.040024011384101596\n",
            "train loss : 0.00040654976942457936    train_acc : 0.03668801280607152\n",
            "train loss : 0.0003955270377253337    train_acc : 0.0416862336068299\n",
            "train loss : 0.000573936266662145    train_acc : 0.033355565991256976\n",
            "train loss : 0.0003717944479852048    train_acc : 0.041684456301862\n",
            "train loss : 0.0005990402118607028    train_acc : 0.03502223171002766\n",
            "train loss : 0.0005742559505153361    train_acc : 0.035018678523578685\n",
            "train loss : 0.000283226725772892    train_acc : 0.041685343295212576\n",
            "train loss : 0.00030533481457885943    train_acc : 0.04502223218309078\n",
            "train loss : 0.000310623221973944    train_acc : 0.045024011857164314\n",
            "train loss : 0.00047125709511088286    train_acc : 0.038357346139657154\n",
            "train loss : 0.0004558007043862727    train_acc : 0.03168712391794115\n",
            "train loss : 0.00030394986683115766    train_acc : 0.04335023313275624\n",
            "train loss : 0.0001922551292600313    train_acc : 0.046689786791004134\n",
            "train loss : 0.0004588689011202082    train_acc : 0.03669156788628854\n",
            "train loss : 0.00041773257391197765    train_acc : 0.03501956883620602\n",
            "train loss : 0.00033651451713634356    train_acc : 0.038352010436712644\n",
            "train loss : 0.00023663869783850137    train_acc : 0.040020454405566244\n",
            "train loss : 0.00044014171404989865    train_acc : 0.03835467757568297\n",
            "train loss : 0.00037246104407651347    train_acc : 0.04168712249470703\n",
            "train loss : 0.0002801227524880745    train_acc : 0.04502223313199718\n",
            "train loss : 0.00036469731902837775    train_acc : 0.038357345191003735\n",
            "train loss : 0.00030126541791585155    train_acc : 0.04335379058410187\n",
            "train loss : 0.0004380190339085167    train_acc : 0.040023122021644854\n",
            "train loss : 0.0003904136257153827    train_acc : 0.038354678998411544\n",
            "train loss : 0.00038106908969014387    train_acc : 0.03835378916213249\n",
            "train loss : 0.0002972011654672195    train_acc : 0.04668712202088647\n",
            "train loss : 0.00045659259822045947    train_acc : 0.03502489979841114\n",
            "train loss : 0.0005244694186097573    train_acc : 0.035018679946559154\n",
            "train loss : 0.0004509971344223471    train_acc : 0.03835200996263816\n",
            "train loss : 0.00034628450000570885    train_acc : 0.040020454405313405\n",
            "train loss : 0.0002269743502888377    train_acc : 0.0450213442423495\n",
            "train loss : 0.0003435027121766807    train_acc : 0.04169067805026258\n",
            "train loss : 0.0002198978589121111    train_acc : 0.04502223502829347\n",
            "train loss : 0.00044482518113062876    train_acc : 0.041690678525348426\n",
            "train loss : 0.0002736696538335822    train_acc : 0.04335556836188018\n",
            "train loss : 0.00029851177262926135    train_acc : 0.04335645630312633\n",
            "train loss : 0.00011159157582880479    train_acc : 0.048356456776695\n",
            "train loss : 0.0002883726182292767    train_acc : 0.04335912344361424\n",
            "train loss : 0.00038825547185594294    train_acc : 0.043356458199169924\n",
            "train loss : 0.00031708952096031536    train_acc : 0.04168979011103956\n",
            "train loss : 0.0002721415163480278    train_acc : 0.045022234554725885\n",
            "train loss : 0.0005176019915581493    train_acc : 0.03835734519176252\n",
            "train loss : 0.00031509767915135703    train_acc : 0.041687123917435606\n",
            "train loss : 0.0004258327948832264    train_acc : 0.041688899799422634\n",
            "train loss : 0.00045663255973751193    train_acc : 0.04168890074655969\n",
            "train loss : 0.00036201349967598435    train_acc : 0.041688900747064835\n",
            "train loss : 0.0002911537036475811    train_acc : 0.0416889007470651\n",
            "train loss : 0.00026999158059995323    train_acc : 0.0416889007470651\n",
            "train loss : 0.00031646692616354327    train_acc : 0.040022234080398435\n",
            "train loss : 0.0002991363981239669    train_acc : 0.041688011858176215\n",
            "train loss : 0.0003508938686359656    train_acc : 0.04168890027299103\n",
            "train loss : 0.0004085397409511306    train_acc : 0.03668890074681226\n",
            "train loss : 0.0003163831125729409    train_acc : 0.0416862340803983\n",
            "train loss : 0.0003679980922641242    train_acc : 0.04168889932484288\n",
            "train loss : 0.00033474798150003764    train_acc : 0.038355567412973246\n",
            "train loss : 0.0002687940959673306    train_acc : 0.04002045630262026\n",
            "train loss : 0.00023168775611265716    train_acc : 0.04502134424336139\n",
            "train loss : 0.00023025600355557941    train_acc : 0.04502401138359646\n",
            "train loss : 0.0002657319086299562    train_acc : 0.046690679472737924\n",
            "train loss : 0.00024279630157563454    train_acc : 0.043358235029052124\n",
            "train loss : 0.00024311578896826221    train_acc : 0.043356457725348824\n",
            "train loss : 0.0004605791983955552    train_acc : 0.03668979011078685\n",
            "train loss : 0.00044993894057898806    train_acc : 0.03835290122139242\n",
            "train loss : 0.00044703228387174305    train_acc : 0.0400204548806514\n",
            "train loss : 0.00031780287288619643    train_acc : 0.04335467757593635\n",
            "train loss : 0.0002524043352586057    train_acc : 0.0433564558280405\n",
            "train loss : 0.00028776606375541594    train_acc : 0.04168979010977496\n",
            "train loss : 0.00030665508630781476    train_acc : 0.04335556788805855\n",
            "train loss : 0.00036477631945993055    train_acc : 0.03835645630287363\n",
            "train loss : 0.0004360909440073415    train_acc : 0.03502045677669487\n",
            "train loss : 0.00042198973948044    train_acc : 0.036685344243614235\n",
            "train loss : 0.0002453928411168749    train_acc : 0.04501956551692993\n",
            "train loss : 0.00019132561711312203    train_acc : 0.04835734376827569\n",
            "train loss : 0.00044368893023187766    train_acc : 0.03835912391667641\n",
            "train loss : 0.00022660663493535708    train_acc : 0.04668712486608889\n",
            "train loss : 0.00036636018207745054    train_acc : 0.04169156646659524\n",
            "train loss : 0.0004062069099397187    train_acc : 0.03835556883544885\n",
            "train loss : 0.0003972089138370753    train_acc : 0.0400204563033789\n",
            "train loss : 0.00015221971065980817    train_acc : 0.04668801091002847\n",
            "train loss : 0.0003782818102189762    train_acc : 0.038358233605818685\n",
            "train loss : 0.0003435710753008179    train_acc : 0.043353791057923106\n",
            "train loss : 0.00040126925042920667    train_acc : 0.03835645535523089\n",
            "train loss : 0.0003010046698534841    train_acc : 0.04168712344285612\n",
            "train loss : 0.00030988381768427343    train_acc : 0.04002223313250285\n",
            "train loss : 0.0003498345689056458    train_acc : 0.040021345191004004\n",
            "train loss : 0.00027901392778163905    train_acc : 0.04168801138410187\n",
            "train loss : 0.0004361916566509692    train_acc : 0.03668890027273818\n",
            "train loss : 0.0003013656324272484    train_acc : 0.04168623408014546\n",
            "train loss : 0.0005749534631191957    train_acc : 0.035022232658176077\n",
            "train loss : 0.000346884667664868    train_acc : 0.04001867852408436\n",
            "train loss : 0.0003758985442796231    train_acc : 0.041688009961879505\n",
            "train loss : 0.0004779621928873528    train_acc : 0.036688900271979664\n",
            "train loss : 0.00033976951837380424    train_acc : 0.04168623408014506\n",
            "train loss : 0.00023866303401311152    train_acc : 0.04835556599150941\n",
            "train loss : 0.0003673187923836026    train_acc : 0.04002578963519547\n",
            "train loss : 0.0002756850111236788    train_acc : 0.04335468042113877\n",
            "train loss : 0.00032114334827993974    train_acc : 0.040023122496224604\n",
            "train loss : 0.0005610245987994212    train_acc : 0.036688012331997984\n",
            "train loss : 0.000394669578262182    train_acc : 0.0400195669399104\n",
            "train loss : 0.0003151659474933349    train_acc : 0.040021343769034615\n",
            "train loss : 0.00044202212038602643    train_acc : 0.04002134471667682\n",
            "train loss : 0.00048268826486557305    train_acc : 0.035021344717182226\n",
            "train loss : 0.00029396531312562036    train_acc : 0.03835201138384916\n",
            "train loss : 0.0003975749626018148    train_acc : 0.03835378773940472\n",
            "train loss : 0.0005312825584737066    train_acc : 0.033353788686794346\n",
            "train loss : 0.0004955165049947277    train_acc : 0.031684455353966294\n",
            "train loss : 0.00037898074428036564    train_acc : 0.03668356504285545\n",
            "train loss : 0.000386610706386777    train_acc : 0.04001956456802285\n",
            "train loss : 0.00024737546448163873    train_acc : 0.04335467710110295\n",
            "train loss : 0.0003586037722142763    train_acc : 0.04335645582778726\n",
            "train loss : 0.0004305383482915514    train_acc : 0.040023123443108155\n",
            "train loss : 0.00038342552931676133    train_acc : 0.04002134566583632\n",
            "train loss : 0.0005151831885314071    train_acc : 0.035021344717688446\n",
            "Looked at 38400/60000 samples\n",
            "train loss : 0.0004383164513789355    train_acc : 0.0350186780505161\n",
            "train loss : 0.00038018534119228163    train_acc : 0.038352009961626944\n",
            "train loss : 0.0004592901691306183    train_acc : 0.0333537877386462\n",
            "train loss : 0.0004735103165427583    train_acc : 0.031684455353460615\n",
            "train loss : 0.0002945168236051835    train_acc : 0.04001689837618851\n",
            "train loss : 0.00030939124973186676    train_acc : 0.04002134234580064\n",
            "train loss : 0.00037150836702427105    train_acc : 0.03835467804925109\n",
            "train loss : 0.0005526012063382268    train_acc : 0.03335378916162627\n",
            "train loss : 0.0003632116034762255    train_acc : 0.0433511220208862\n",
            "train loss : 0.0003666283823498162    train_acc : 0.03835645393174447\n",
            "train loss : 0.00022184173277417542    train_acc : 0.04168712344209693\n",
            "train loss : 0.0005019256237878344    train_acc : 0.03335556646583578\n",
            "train loss : 0.000377131968008875    train_acc : 0.041684456302115105\n",
            "train loss : 0.0003208390943046503    train_acc : 0.04335556504336113\n",
            "train loss : 0.00020526061969713825    train_acc : 0.045023122968023126\n",
            "train loss : 0.00031305779462664773    train_acc : 0.04335734566558294\n",
            "train loss : 0.0005007914936778048    train_acc : 0.03668979058435498\n",
            "train loss : 0.00028172686535709365    train_acc : 0.04168623455497832\n",
            "train loss : 0.0001506331632128747    train_acc : 0.048355565991762654\n",
            "train loss : 0.0002511966134744004    train_acc : 0.0450257896351956\n",
            "train loss : 0.0004128166955262098    train_acc : 0.0400240137544721\n",
            "train loss : 0.00013672677405259607    train_acc : 0.048354679474002384\n",
            "train loss : 0.00024134678505647225    train_acc : 0.043359122495719465\n",
            "train loss : 0.0003175892234530067    train_acc : 0.03835645819866438\n",
            "train loss : 0.0003859933618863787    train_acc : 0.04002045677770595\n",
            "train loss : 0.00022517801583630035    train_acc : 0.04502134424361478\n",
            "train loss : 0.00020345097733211954    train_acc : 0.05002401138359659\n",
            "train loss : 0.00047407272151286163    train_acc : 0.03336001280607125\n",
            "train loss : 0.0004927602965179612    train_acc : 0.035017792006829904\n",
            "train loss : 0.00037384777532993655    train_acc : 0.040018676155736975\n",
            "train loss : 0.000503557368115674    train_acc : 0.03668800996061639\n",
            "train loss : 0.0005002271108860419    train_acc : 0.03501956693864566\n",
            "train loss : 0.00036292433478954286    train_acc : 0.04001867710236728\n",
            "train loss : 0.0004146643594092185    train_acc : 0.03668800996112126\n",
            "train loss : 0.0002526209962693949    train_acc : 0.045019566938645936\n",
            "train loss : 0.00024637007004485277    train_acc : 0.04335734376903395\n",
            "train loss : 0.00031253758760710007    train_acc : 0.04168979058334348\n",
            "train loss : 0.00044018085831373224    train_acc : 0.04002223455497778\n",
            "train loss : 0.0003424756370970089    train_acc : 0.04668801185842932\n",
            "train loss : 0.00032086304943524405    train_acc : 0.0433582336063245\n",
            "train loss : 0.0002628492415328596    train_acc : 0.04335645772459004\n",
            "train loss : 0.00038214721293451415    train_acc : 0.03668979011078644\n",
            "train loss : 0.0005580231783019766    train_acc : 0.03668623455472575\n",
            "train loss : 0.0002888120357321224    train_acc : 0.04335289932509585\n",
            "train loss : 0.00037261954822244257    train_acc : 0.04002312154630672\n",
            "train loss : 0.00035258323410817307    train_acc : 0.041688012331491366\n",
            "train loss : 0.0005427432812079307    train_acc : 0.035022233606576796\n",
            "train loss : 0.00027271069594260455    train_acc : 0.04001867852459018\n",
            "train loss : 0.0003237697705030867    train_acc : 0.04168800996187978\n",
            "train loss : 0.00012013460653222403    train_acc : 0.050022233605313\n",
            "train loss : 0.0002648790526149389    train_acc : 0.045026678524589504\n",
            "train loss : 0.0002335540432319513    train_acc : 0.04669068089521312\n",
            "train loss : 0.0004120492104413025    train_acc : 0.03669156836314411\n",
            "train loss : 0.00015150443835596094    train_acc : 0.046686235503127005\n",
            "train loss : 0.000232777383253964    train_acc : 0.046691565992268334\n",
            "train loss : 0.0005765673114916091    train_acc : 0.03502490216852921\n",
            "train loss : 0.0005099525195542473    train_acc : 0.03335201328115655\n",
            "train loss : 0.0001749526845159628    train_acc : 0.048351121073749954\n",
            "train loss : 0.0004303361326528519    train_acc : 0.038359120597906\n",
            "train loss : 0.0003100801099856733    train_acc : 0.040020458197652216\n",
            "train loss : 0.00027418859103465185    train_acc : 0.04168801091103874\n",
            "train loss : 0.000451201220233171    train_acc : 0.03502223360581922\n",
            "train loss : 0.0004268007246758561    train_acc : 0.036685345191256435\n",
            "train loss : 0.0002771680681847522    train_acc : 0.04168623218410201\n",
            "train loss : 0.00032613385863457483    train_acc : 0.03835556599049819\n",
            "train loss : 0.00035282845872851136    train_acc : 0.0400204563018616\n",
            "train loss : 0.0001703806870544127    train_acc : 0.04668801091002766\n",
            "train loss : 0.0002576435411322575    train_acc : 0.04669156693915202\n",
            "train loss : 0.0003231789125232881    train_acc : 0.043358235502367545\n",
            "train loss : 0.0005550511055762624    train_acc : 0.03835645772560126\n",
            "train loss : 0.00045656726240441933    train_acc : 0.03668712344412032\n",
            "train loss : 0.00046221898166592555    train_acc : 0.036686233132503535\n",
            "train loss : 0.0002813880840204464    train_acc : 0.045019565991004\n",
            "train loss : 0.0003913395852618357    train_acc : 0.036690677101861864\n",
            "train loss : 0.0002296296465794655    train_acc : 0.04501956836112099\n",
            "train loss : 0.00028139742362278834    train_acc : 0.043357343769792604\n",
            "train loss : 0.0004997783681665409    train_acc : 0.03502312391667722\n",
            "train loss : 0.0003361372912608737    train_acc : 0.036685345666088894\n",
            "train loss : 0.0004437791966737474    train_acc : 0.04001956551768858\n",
            "train loss : 0.00020400590513353038    train_acc : 0.0500213437682761\n",
            "train loss : 0.00023664614839985214    train_acc : 0.04169334471667641\n",
            "train loss : 0.0003547266090249148    train_acc : 0.04168890311718223\n",
            "train loss : 0.0002595949594160893    train_acc : 0.041688900748329163\n",
            "train loss : 0.00032921057639778885    train_acc : 0.03835556741373244\n",
            "train loss : 0.0003967730597480046    train_acc : 0.041687122969287324\n",
            "train loss : 0.00023950076889439815    train_acc : 0.043355566465583625\n",
            "train loss : 0.0003384130845429709    train_acc : 0.04168978963544831\n",
            "train loss : 0.00038263198946051925    train_acc : 0.038355567887805565\n",
            "train loss : 0.0004552125453801374    train_acc : 0.036687122969540166\n",
            "train loss : 0.00026966289517719755    train_acc : 0.043352899798917086\n",
            "train loss : 0.000275313701395657    train_acc : 0.04335645487989276\n",
            "train loss : 0.0003607895850320204    train_acc : 0.04335645677593595\n",
            "train loss : 0.00035644493245438365    train_acc : 0.0416897901102805\n",
            "train loss : 0.00028803542943394637    train_acc : 0.04335556788805882\n",
            "train loss : 0.00048107123160728664    train_acc : 0.0350231229695403\n",
            "train loss : 0.0003877074570962289    train_acc : 0.035018678998917085\n",
            "train loss : 0.0003454571613233094    train_acc : 0.041685343295466096\n",
            "train loss : 0.0004373838999879922    train_acc : 0.03502223218309092\n",
            "train loss : 0.0003938938485515919    train_acc : 0.03835201185716432\n",
            "train loss : 0.0004162707151469619    train_acc : 0.03668712107299049\n",
            "train loss : 0.0005140365239366688    train_acc : 0.033352899797905594\n",
            "train loss : 0.00038575338051458915    train_acc : 0.04001778821322555\n",
            "train loss : 0.00040762268281897303    train_acc : 0.03668800948704705\n",
            "train loss : 0.0003268588312084247    train_acc : 0.03668623360505976\n",
            "train loss : 0.00019405523240447652    train_acc : 0.05001956599125603\n",
            "train loss : 0.000342432712127367    train_acc : 0.03836001043519533\n",
            "train loss : 0.00047007786647722965    train_acc : 0.03668712533889877\n",
            "train loss : 0.00024849397652634646    train_acc : 0.04668623313351408\n",
            "train loss : 0.00034857454417924506    train_acc : 0.04002489932433787\n",
            "train loss : 0.0004047341063950792    train_acc : 0.038354679946306314\n",
            "train loss : 0.0003646104593761939    train_acc : 0.036687122495971364\n",
            "train loss : 0.0003943145481247207    train_acc : 0.03835289979866452\n",
            "train loss : 0.0004099212169757154    train_acc : 0.04002045487989262\n",
            "train loss : 0.0002690989283498187    train_acc : 0.04668801090926928\n",
            "train loss : 0.0003195530487965359    train_acc : 0.04169156693915161\n",
            "train loss : 0.0004887273855375645    train_acc : 0.04002223550236755\n",
            "train loss : 0.00030610292704621246    train_acc : 0.040021345192267924\n",
            "train loss : 0.00031060026661317464    train_acc : 0.041688011384102545\n",
            "train loss : 0.00044618568445086885    train_acc : 0.03668890027273818\n",
            "train loss : 0.000350037821248504    train_acc : 0.04001956741347879\n",
            "train loss : 0.00036564749590985797    train_acc : 0.04002134376928719\n",
            "train loss : 0.0004993122274722652    train_acc : 0.030021344716676956\n",
            "train loss : 0.00026908270737621763    train_acc : 0.04501601138384889\n",
            "train loss : 0.0004093025671228891    train_acc : 0.041690675206071386\n",
            "train loss : 0.0003035631166323972    train_acc : 0.043355568360109904\n",
            "train loss : 0.0003301272374463836    train_acc : 0.04002312296979206\n",
            "train loss : 0.00029278420181179436    train_acc : 0.04335467899891722\n",
            "train loss : 0.0004115623189225255    train_acc : 0.04168978916213276\n",
            "train loss : 0.00030542999489854253    train_acc : 0.040022234554219806\n",
            "train loss : 0.00030010745573035214    train_acc : 0.04335467852509558\n",
            "train loss : 0.0005318724939857482    train_acc : 0.03668978916188005\n",
            "train loss : 0.0002495865929831205    train_acc : 0.04501956788755301\n",
            "train loss : 7.976957537909132e-05    train_acc : 0.05169067710287336\n",
            "train loss : 0.0005002358217683272    train_acc : 0.036694235027788194\n",
            "train loss : 0.0003679487725713331    train_acc : 0.04001957025868149\n",
            "train loss : 0.00043094816703905626    train_acc : 0.04002134377080463\n",
            "train loss : 0.0006952131942266852    train_acc : 0.03502134471667776\n",
            "train loss : 0.00037677662346921256    train_acc : 0.038352011383848895\n",
            "train loss : 0.000175687083015305    train_acc : 0.04668712107273806\n",
            "train loss : 0.0002884091866855671    train_acc : 0.04169156646457212\n",
            "train loss : 0.00028674226453213403    train_acc : 0.04335556883544778\n",
            "train loss : 0.0003301399783105337    train_acc : 0.04335645630337891\n",
            "train loss : 0.0003274181311838262    train_acc : 0.045023123443361804\n",
            "train loss : 0.00029052134083777073    train_acc : 0.04169067899916979\n",
            "train loss : 0.0004169592583747814    train_acc : 0.03668890169546623\n",
            "train loss : 0.000392198448675186    train_acc : 0.03501956741423758\n",
            "train loss : 0.00043449405028305973    train_acc : 0.036685343769287594\n",
            "train loss : 0.0004338385772874296    train_acc : 0.03835289885001029\n",
            "train loss : 0.0005312186750895272    train_acc : 0.03168712154605334\n",
            "train loss : 0.0002645433066830702    train_acc : 0.0450168997981579\n",
            "train loss : 0.0002085209782981102    train_acc : 0.048357342346559015\n",
            "train loss : 0.0002908606656867167    train_acc : 0.04002579058258483\n",
            "train loss : 0.0002415873156297725    train_acc : 0.045021347088310716\n",
            "train loss : 0.00041458165367689703    train_acc : 0.040024011385113765\n",
            "train loss : 0.000275823864172407    train_acc : 0.046688012806072064\n",
            "train loss : 0.00032057954650075197    train_acc : 0.04169156694016324\n",
            "train loss : 0.00042574368897427956    train_acc : 0.03835556883570142\n",
            "train loss : 0.0001697041763887583    train_acc : 0.05002045630337904\n",
            "train loss : 0.00021757487181419934    train_acc : 0.0416933442433618\n",
            "train loss : 0.00026040713947997405    train_acc : 0.04168890311692979\n",
            "train loss : 0.0003847789416893958    train_acc : 0.04002223408166237\n",
            "train loss : 0.00029677885435158333    train_acc : 0.04335467852484355\n",
            "train loss : 0.0004351353366516014    train_acc : 0.040023122495213254\n",
            "train loss : 0.0004006284828037337    train_acc : 0.04002134566533078\n",
            "train loss : 0.0004909654436299562    train_acc : 0.036688011384354846\n",
            "train loss : 0.0004297651301478168    train_acc : 0.04001956693940499\n",
            "train loss : 0.0004693394108587188    train_acc : 0.041688010435701014\n",
            "train loss : 0.00038079936372989936    train_acc : 0.03835556693889904\n",
            "train loss : 0.00024422786245911787    train_acc : 0.04668712296903408\n",
            "train loss : 0.0003626401730728688    train_acc : 0.038358233132250154\n",
            "train loss : 0.0002733567267263387    train_acc : 0.04335379105767054\n",
            "train loss : 0.00037150532928623646    train_acc : 0.04002312202189742\n",
            "train loss : 0.0003973349515646167    train_acc : 0.04168801233174501\n",
            "train loss : 0.0002525362864909728    train_acc : 0.04502223360657693\n",
            "train loss : 0.00042773150939389224    train_acc : 0.035024011857923504\n",
            "train loss : 0.0005022961980159476    train_acc : 0.036685346139657564\n",
            "train loss : 0.000421888800678574    train_acc : 0.040019565517941155\n",
            "train loss : 0.00034359557213900775    train_acc : 0.0366880104349429\n",
            "train loss : 0.0002887488230746721    train_acc : 0.045019566938898636\n",
            "train loss : 0.00042960130060114765    train_acc : 0.03669067710236742\n",
            "train loss : 0.00037087661469186356    train_acc : 0.035019568361121266\n",
            "train loss : 0.00033160102453467155    train_acc : 0.0416853437697926\n",
            "train loss : 0.00028752401167597577    train_acc : 0.040022232183343893\n",
            "train loss : 0.0003444361321909928    train_acc : 0.040021345190497784\n",
            "train loss : 0.0003569564081633242    train_acc : 0.04002134471743493\n",
            "train loss : 0.0003791058773918896    train_acc : 0.03835467805051596\n",
            "train loss : 0.0004706459153872132    train_acc : 0.03668712249496027\n",
            "train loss : 0.0003185096463579168    train_acc : 0.041686233131997305\n",
            "train loss : 0.00033546173429918385    train_acc : 0.04335556599100373\n",
            "train loss : 0.00039440333410822783    train_acc : 0.0366897896351952\n",
            "train loss : 0.0004558692611035491    train_acc : 0.035019567887805435\n",
            "train loss : 0.00037432213519609115    train_acc : 0.04168534376954016\n",
            "train loss : 0.0002878202146948422    train_acc : 0.04335556551667709\n",
            "train loss : 0.00033985439370190626    train_acc : 0.04168978963494223\n",
            "train loss : 0.0003155828415734509    train_acc : 0.04002223455447197\n",
            "train loss : 0.00034538560236291793    train_acc : 0.03835467852509572\n",
            "train loss : 0.00013982288388742232    train_acc : 0.050020455828546716\n",
            "train loss : 0.0004466496734213741    train_acc : 0.04002667757644189\n",
            "train loss : 0.0002602693092997017    train_acc : 0.04335468089470743\n",
            "train loss : 0.0005435870041662945    train_acc : 0.035023122496477176\n",
            "train loss : 0.0005563087424145919    train_acc : 0.03835201233199812\n",
            "train loss : 0.0005020106414773997    train_acc : 0.036687121073243734\n",
            "train loss : 0.0002910295276949045    train_acc : 0.0450195664645724\n",
            "train loss : 0.00041053290892437345    train_acc : 0.0383573437687811\n",
            "train loss : 0.0003539314460528098    train_acc : 0.041687123916676685\n",
            "train loss : 0.0004457971249999879    train_acc : 0.03835556646608889\n",
            "train loss : 0.0003618009006073731    train_acc : 0.041687122968781916\n",
            "train loss : 0.0003134974165553565    train_acc : 0.04335556646558335\n",
            "train loss : 0.00027872097059481913    train_acc : 0.04168978963544831\n",
            "train loss : 0.0003802218806561597    train_acc : 0.040022234554472236\n",
            "train loss : 0.00020066693829281154    train_acc : 0.04668801185842905\n",
            "train loss : 0.0005850749534787431    train_acc : 0.03169156693965783\n",
            "train loss : 0.00046898415657268215    train_acc : 0.03835023550236782\n",
            "train loss : 0.00044773067628527954    train_acc : 0.03668712012560126\n",
            "train loss : 0.000540185727255054    train_acc : 0.03668623313073366\n",
            "train loss : 0.0003130754411260692    train_acc : 0.04001956599100306\n",
            "train loss : 0.00032966408838590197    train_acc : 0.041688010435195204\n",
            "train loss : 0.00046794299588579473    train_acc : 0.0366889002722321\n",
            "train loss : 0.00026028446454104306    train_acc : 0.04168623408014519\n",
            "train loss : 0.0003948055855014499    train_acc : 0.03835556599150941\n",
            "train loss : 0.0003570368047900506    train_acc : 0.041687122968528806\n",
            "train loss : 0.0003802837398399961    train_acc : 0.043355566465583216\n",
            "train loss : 0.0002591649088047213    train_acc : 0.043356456302114973\n",
            "train loss : 0.0002801524818946405    train_acc : 0.04002312344336113\n",
            "train loss : 0.0003764629420821407    train_acc : 0.04002134566583646\n",
            "train loss : 0.0003030522028635441    train_acc : 0.040021344717688444\n",
            "train loss : 0.00021151520005120332    train_acc : 0.04668801138384943\n",
            "train loss : 0.0003747980002148483    train_acc : 0.04169156693940472\n",
            "train loss : 0.00028065742069899156    train_acc : 0.04502223550236768\n",
            "train loss : 0.000351233757342309    train_acc : 0.03835734519226793\n",
            "train loss : 0.0004553948950644628    train_acc : 0.03502045725076921\n",
            "train loss : 0.00038817323870721796    train_acc : 0.038352010910533745\n",
            "train loss : 0.0002638585715697991    train_acc : 0.041687121072485615\n",
            "train loss : 0.0003604090948819694    train_acc : 0.04335556646457199\n",
            "train loss : 0.0003260300846491877    train_acc : 0.04335645630211444\n",
            "train loss : 0.00028102496074526015    train_acc : 0.04335645677669447\n",
            "train loss : 0.0003473977348476351    train_acc : 0.038356456776947566\n",
            "train loss : 0.0003802587307877683    train_acc : 0.04002045677694771\n",
            "train loss : 0.00032319166446883875    train_acc : 0.04002134424361437\n",
            "train loss : 0.00037381452739601747    train_acc : 0.040021344716929926\n",
            "train loss : 0.00034426884864394066    train_acc : 0.04002134471718236\n",
            "train loss : 0.00037306572191987897    train_acc : 0.041688011383849165\n",
            "train loss : 0.0004003628414217264    train_acc : 0.03668890027273806\n",
            "train loss : 0.00035036420739075044    train_acc : 0.03835290074681213\n",
            "train loss : 0.00028164876734371386    train_acc : 0.04168712154706497\n",
            "train loss : 0.0004057650655811901    train_acc : 0.03668889979815844\n",
            "train loss : 0.0002647861832984451    train_acc : 0.04335290074655902\n",
            "train loss : 0.00043471989234259963    train_acc : 0.0366897882137315\n",
            "train loss : 0.00037572099016613416    train_acc : 0.03835290122038066\n",
            "train loss : 0.0005532326873845339    train_acc : 0.0333537882139842\n",
            "train loss : 0.0003757308548067255    train_acc : 0.036684455353714125\n",
            "train loss : 0.00037734409070679206    train_acc : 0.04001956504285531\n",
            "train loss : 0.0004205885818264043    train_acc : 0.03668801043468952\n",
            "train loss : 0.0003754336454227586    train_acc : 0.03835290027223184\n",
            "train loss : 0.00042939754267892603    train_acc : 0.03835378821347852\n",
            "train loss : 0.000244299875746517    train_acc : 0.04502045535371386\n",
            "train loss : 0.00020740296592026305    train_acc : 0.046690677576188645\n",
            "train loss : 0.00031768101515319456    train_acc : 0.04169156836137397\n",
            "train loss : 0.0003087950961752041    train_acc : 0.041688902169792735\n",
            "train loss : 0.00028250922273089955    train_acc : 0.040022234081157224\n",
            "train loss : 0.00026085111993954174    train_acc : 0.04502134519150995\n",
            "train loss : 0.00039218201178179004    train_acc : 0.040024011384102144\n",
            "train loss : 0.0003251539777858409    train_acc : 0.041688012806071525\n",
            "train loss : 0.0005027376867459162    train_acc : 0.033355566940163235\n",
            "train loss : 0.0003384362709923387    train_acc : 0.04001778963570142\n",
            "train loss : 0.0004550002739722148    train_acc : 0.03835467615447237\n",
            "train loss : 0.00015979554296736688    train_acc : 0.048353789160615716\n",
            "train loss : 0.0002618841147213046    train_acc : 0.041692455354219\n",
            "train loss : 0.0006049532472724802    train_acc : 0.038355569309522254\n",
            "train loss : 0.00035836024065969443    train_acc : 0.045020456303631745\n",
            "train loss : 0.000517243330865331    train_acc : 0.038357344243361936\n",
            "train loss : 0.00022134583483017246    train_acc : 0.04502045725026313\n",
            "train loss : 0.0003777359506887323    train_acc : 0.03669067757720014\n",
            "train loss : 0.00040847668195202096    train_acc : 0.03835290169470784\n",
            "train loss : 0.0003238115978725456    train_acc : 0.045020454880903844\n",
            "train loss : 0.000491208319890366    train_acc : 0.03669067757593648\n",
            "train loss : 0.00032847989743817007    train_acc : 0.041686235028040496\n",
            "train loss : 0.00018905893892259528    train_acc : 0.043355565992014955\n",
            "train loss : 0.0005687233569671324    train_acc : 0.03335645630186241\n",
            "train loss : 0.0003974903613600439    train_acc : 0.036684456776694324\n",
            "train loss : 0.0003042485167710131    train_acc : 0.04335289837694757\n",
            "train loss : 0.00018487496195105065    train_acc : 0.046689788212467706\n",
            "train loss : 0.00048398509620570007    train_acc : 0.036691567887046646\n",
            "train loss : 0.000493865360339477    train_acc : 0.035019568836206424\n",
            "train loss : 0.00028815398806970383    train_acc : 0.04335201043671264\n",
            "train loss : 0.00035025489876108507    train_acc : 0.04168978773889958\n",
            "train loss : 0.0003006206926374935    train_acc : 0.04168890122012742\n",
            "train loss : 0.0004523048902605199    train_acc : 0.033355567413984065\n",
            "train loss : 0.0004830915265668792    train_acc : 0.03501778963595412\n",
            "train loss : 0.00043090044437691315    train_acc : 0.03668534282113918\n",
            "train loss : 0.0005437801777467666    train_acc : 0.033352898849504604\n",
            "train loss : 0.0003870310820738022    train_acc : 0.038351121546053066\n",
            "train loss : 0.00011407500279943676    train_acc : 0.051687120598157894\n",
            "train loss : 0.0005223382269847288    train_acc : 0.03336089979765235\n",
            "train loss : 0.00044992228766311583    train_acc : 0.03501779247989208\n",
            "train loss : 0.00040583576976150953    train_acc : 0.04335200948932261\n",
            "train loss : 0.00017296332659307527    train_acc : 0.04835645440506097\n",
            "train loss : 0.0003404867440252409    train_acc : 0.04335912344234937\n",
            "train loss : 0.00027056110444544627    train_acc : 0.04168979153250258\n",
            "train loss : 0.00034533714491813915    train_acc : 0.041688901222150666\n",
            "train loss : 0.0003021086945811308    train_acc : 0.04335556741398514\n",
            "train loss : 0.00031490957078375705    train_acc : 0.04168978963595412\n",
            "train loss : 0.00026239351609196683    train_acc : 0.04168890122113918\n",
            "train loss : 0.0002953934425575407    train_acc : 0.04335556741398461\n",
            "train loss : 0.00032640154773682826    train_acc : 0.04168978963595412\n",
            "train loss : 0.0003195179656114297    train_acc : 0.040022234554472506\n",
            "train loss : 0.0002575965443269394    train_acc : 0.04502134519176238\n",
            "train loss : 0.00033582165250349127    train_acc : 0.04169067805076894\n",
            "train loss : 0.0002985652654139686    train_acc : 0.04168890169496041\n",
            "train loss : 0.0002338952412270847    train_acc : 0.04502223408090397\n",
            "train loss : 0.00047360029573387903    train_acc : 0.038357345191509816\n",
            "train loss : 0.00020697921735315933    train_acc : 0.04335379058410214\n",
            "train loss : 0.00041037945858755253    train_acc : 0.03835645535497819\n",
            "train loss : 0.0003432816195430338    train_acc : 0.04002045677618932\n",
            "train loss : 0.00017577456823603005    train_acc : 0.0483546775769473\n",
            "train loss : 0.0005199479741382155    train_acc : 0.035025789161374374\n",
            "train loss : 0.0003375331013829193    train_acc : 0.04001868042088607\n",
            "train loss : 0.0002945326126654641    train_acc : 0.0433546766295578\n",
            "train loss : 0.0003309200107975557    train_acc : 0.04002312249420243\n",
            "train loss : 0.0004279534949143687    train_acc : 0.03835467899866358\n",
            "train loss : 0.000379749452852317    train_acc : 0.03835378916213262\n",
            "train loss : 0.00029554138624179795    train_acc : 0.041687122020886466\n",
            "train loss : 0.0002683075066144917    train_acc : 0.04335556646507781\n",
            "train loss : 0.0004022141600606566    train_acc : 0.03502312296878138\n",
            "train loss : 0.0002973172834346007    train_acc : 0.04168534566558335\n",
            "train loss : 0.00023773942490529112    train_acc : 0.043355565517688315\n",
            "train loss : 0.00040529332496908033    train_acc : 0.0350231229682761\n",
            "train loss : 0.0003626333139310317    train_acc : 0.043352012332249744\n",
            "train loss : 0.0007121440244495588    train_acc : 0.0333564544065772\n",
            "train loss : 0.0005997130010643418    train_acc : 0.033351123442350174\n",
            "train loss : 0.00036843816485585733    train_acc : 0.03835112059916925\n",
            "train loss : 0.00032537327978370765    train_acc : 0.041687120597652894\n",
            "train loss : 0.0005333374296123364    train_acc : 0.040022233130985414\n",
            "train loss : 0.0004898687654259202    train_acc : 0.033354678524336524\n",
            "train loss : 0.0003915101888684973    train_acc : 0.04001778916187965\n",
            "train loss : 0.00041633664611196033    train_acc : 0.036688009487553006\n",
            "train loss : 0.0002278870572993131    train_acc : 0.04501956693839336\n",
            "train loss : 0.0004334681927485121    train_acc : 0.03669067710236714\n",
            "train loss : 0.0003574717963377276    train_acc : 0.0433529016944546\n",
            "train loss : 0.0003963467719250064    train_acc : 0.03668978821423704\n",
            "train loss : 0.00036656686768224997    train_acc : 0.04168623455371426\n",
            "train loss : 0.00014709792046602722    train_acc : 0.04835556599176198\n",
            "train loss : 0.00028291714647880585    train_acc : 0.0450257896351956\n",
            "train loss : 0.00026941825746143905    train_acc : 0.045024013754472104\n",
            "train loss : 0.0002979518282667724    train_acc : 0.04502401280733572\n",
            "train loss : 0.000320416285266254    train_acc : 0.04335734614016391\n",
            "train loss : 0.0002646450898878679    train_acc : 0.04502312391794142\n",
            "train loss : 0.00032592668558236403    train_acc : 0.04002401233275624\n",
            "train loss : 0.00033320440109266215    train_acc : 0.04168801280657747\n",
            "train loss : 0.0003371392644948731    train_acc : 0.04168890027349684\n",
            "train loss : 0.00044899890449086864    train_acc : 0.0383555674134792\n",
            "train loss : 0.00034462632979089127    train_acc : 0.043353789635953856\n",
            "train loss : 0.000426451170297438    train_acc : 0.041689788687805845\n",
            "train loss : 0.0004082674555269093    train_acc : 0.04002223455396683\n",
            "train loss : 0.0002166395375343051    train_acc : 0.04502134519176212\n",
            "train loss : 0.0004179567848305814    train_acc : 0.04002401138410227\n",
            "train loss : 0.00032516041282837447    train_acc : 0.040021346139404855\n",
            "train loss : 0.00033699061986543883    train_acc : 0.04002134471794102\n",
            "train loss : 0.0004446750565980103    train_acc : 0.033354678050516236\n",
            "train loss : 0.0004573341499797299    train_acc : 0.036684455828293605\n",
            "train loss : 0.000200460411511052    train_acc : 0.045019565043108424\n",
            "train loss : 0.0003250954363385318    train_acc : 0.045024010434689656\n",
            "train loss : 0.0002455150666667959    train_acc : 0.046690679472231836\n",
            "train loss : 0.00048317591537808493    train_acc : 0.038358235029051856\n",
            "train loss : 0.0003505718977196469    train_acc : 0.041687124392015495\n",
            "train loss : 0.0003994641501252357    train_acc : 0.04002223313300907\n",
            "train loss : 0.00037810620932041513    train_acc : 0.03835467852433761\n",
            "train loss : 0.0003864834966514813    train_acc : 0.04002045582854632\n",
            "train loss : 0.0003582789140416386    train_acc : 0.03835467757644189\n",
            "train loss : 0.00028929296883675645    train_acc : 0.0433537891613741\n",
            "train loss : 0.00036573777296147205    train_acc : 0.041689788687552735\n",
            "train loss : 0.00036168410179190105    train_acc : 0.04168890122063336\n",
            "train loss : 0.00023132154313085964    train_acc : 0.04335556741398434\n",
            "train loss : 0.00023770740609151714    train_acc : 0.04168978963595412\n",
            "train loss : 0.00022846530496269219    train_acc : 0.046688901221139174\n",
            "train loss : 0.000581705634173389    train_acc : 0.03169156741398461\n",
            "train loss : 0.00048657803220980783    train_acc : 0.038350235502620796\n",
            "train loss : 0.00038216213940770594    train_acc : 0.038353786792268064\n",
            "train loss : 0.00023611908330153992    train_acc : 0.04335378868628921\n",
            "train loss : 0.0004535172914143331    train_acc : 0.03335645535396602\n",
            "train loss : 0.0002871528548417979    train_acc : 0.04335112344285545\n",
            "train loss : 0.0002830865105142815    train_acc : 0.04168978726583619\n",
            "train loss : 0.0004114663252364344    train_acc : 0.03835556788654178\n",
            "train loss : 0.00039130802979199785    train_acc : 0.043353789636206154\n",
            "train loss : 0.00027284762253247435    train_acc : 0.04335645535447264\n",
            "train loss : 0.00030377189710929596    train_acc : 0.04168979010952238\n",
            "train loss : 0.00014550240665221317    train_acc : 0.04668890122139174\n",
            "train loss : 0.00026027443012571094    train_acc : 0.04169156741398474\n",
            "train loss : 0.0004051065343327044    train_acc : 0.040022235502620795\n",
            "train loss : 0.0004245825945482098    train_acc : 0.04168801185893473\n",
            "train loss : 0.0006570735713290715    train_acc : 0.030022233606324767\n",
            "train loss : 0.00029375787942358903    train_acc : 0.04501601185792338\n",
            "train loss : 0.00047253552879952717    train_acc : 0.03669067520632423\n",
            "train loss : 0.0005960733359856397    train_acc : 0.03335290169344337\n",
            "train loss : 0.0004238704189017509    train_acc : 0.0400177882142365\n",
            "train loss : 0.000416518286564316    train_acc : 0.0366880094870476\n",
            "train loss : 0.0004293499316298688    train_acc : 0.04001956693839309\n",
            "train loss : 0.000408033698289952    train_acc : 0.04168801043570048\n",
            "train loss : 0.00025425007087362573    train_acc : 0.04002223360556571\n",
            "train loss : 0.0003621170873119692    train_acc : 0.03835467852458964\n",
            "Looked at 51200/60000 samples\n",
            "train loss : 0.00037781834019314837    train_acc : 0.04002045582854644\n",
            "train loss : 0.0003008394476541577    train_acc : 0.04168801090977522\n",
            "train loss : 0.0005926634867475705    train_acc : 0.03502223360581855\n",
            "train loss : 0.00018871125359002676    train_acc : 0.0483520118579231\n",
            "train loss : 0.00024360289730908428    train_acc : 0.04169245440632423\n",
            "train loss : 0.00025141410759056026    train_acc : 0.04168890264235004\n",
            "train loss : 0.0004601025590511479    train_acc : 0.03668890074807592\n",
            "train loss : 0.00043571240870206686    train_acc : 0.03835290074706564\n",
            "train loss : 0.00045252288313530844    train_acc : 0.036687121547065105\n",
            "train loss : 0.0004928971314161715    train_acc : 0.03668623313149177\n",
            "train loss : 0.0004138210119132853    train_acc : 0.03501956599100346\n",
            "train loss : 0.0005986449166775452    train_acc : 0.03168534376852853\n",
            "train loss : 0.0005754765063945261    train_acc : 0.030016898850009882\n",
            "train loss : 0.00037671813626768454    train_acc : 0.03834934234605334\n",
            "train loss : 0.00034887591303327183    train_acc : 0.04168711964925123\n",
            "train loss : 0.0004101131174262414    train_acc : 0.040022233130479604\n",
            "train loss : 0.00038015917321381545    train_acc : 0.04002134519100293\n",
            "train loss : 0.00038026053150696403    train_acc : 0.04168801138410187\n",
            "train loss : 0.00032438131056065136    train_acc : 0.04168890027273819\n",
            "train loss : 0.0003388521898687248    train_acc : 0.03835556741347879\n",
            "train loss : 0.00025206594455660187    train_acc : 0.04502045630262052\n",
            "train loss : 0.00032652743545526095    train_acc : 0.04169067757669473\n",
            "train loss : 0.00033377282528287755    train_acc : 0.038355568361374236\n",
            "train loss : 0.00035742348921409295    train_acc : 0.04002045630312606\n",
            "train loss : 0.0003427675428734467    train_acc : 0.043354677576695\n",
            "train loss : 0.0003444190561009908    train_acc : 0.040023122494707575\n",
            "train loss : 0.0004015763407675019    train_acc : 0.041688012331997176\n",
            "train loss : 0.00028931393092503695    train_acc : 0.041688900273243734\n",
            "train loss : 0.00033476937237774334    train_acc : 0.04335556741347906\n",
            "train loss : 0.000347680107691147    train_acc : 0.04168978963595386\n",
            "train loss : 0.00038654181493967814    train_acc : 0.03835556788780584\n",
            "train loss : 0.00026944195733141183    train_acc : 0.03835378963620683\n",
            "train loss : 0.00030162005379732803    train_acc : 0.043353788687805976\n",
            "train loss : 0.00028874783022538785    train_acc : 0.0450231220206335\n",
            "train loss : 0.00035927525789633503    train_acc : 0.04002401233174434\n",
            "train loss : 0.0004744785193953572    train_acc : 0.03668801280657693\n",
            "train loss : 0.00020910270781857986    train_acc : 0.04668623360683018\n",
            "train loss : 0.00040423091765388674    train_acc : 0.038358232657923645\n",
            "train loss : 0.0005158947585239206    train_acc : 0.03335379105741756\n",
            "train loss : 0.0003555343080456105    train_acc : 0.03668445535523063\n",
            "train loss : 0.0003010995825411953    train_acc : 0.04001956504285612\n",
            "train loss : 0.0005482386783833124    train_acc : 0.03502134376802286\n",
            "train loss : 0.0003063093477480186    train_acc : 0.04168534471667628\n",
            "train loss : 0.0004862708097486492    train_acc : 0.03835556551718223\n",
            "train loss : 0.0002845362431338516    train_acc : 0.040020456301609164\n",
            "train loss : 0.0003906038832985841    train_acc : 0.03835467757669419\n",
            "train loss : 0.0005107523042589995    train_acc : 0.03668712249470757\n",
            "train loss : 0.0004438231786642409    train_acc : 0.03501956646533051\n",
            "train loss : 0.0004065820731269282    train_acc : 0.04001867710211485\n",
            "train loss : 0.00018461225240270547    train_acc : 0.04668800996112112\n",
            "train loss : 0.00027798319751850154    train_acc : 0.04169156693864593\n",
            "train loss : 0.00012130214481827858    train_acc : 0.05168890216903395\n",
            "train loss : 0.0005338434572946193    train_acc : 0.03836090074782349\n",
            "train loss : 0.0002530038962997824    train_acc : 0.045020459147065504\n",
            "train loss : 0.0003072848860746086    train_acc : 0.04169067757821177\n",
            "train loss : 0.00038860169680618315    train_acc : 0.03835556836137505\n",
            "train loss : 0.0003696882773909659    train_acc : 0.04168712296979274\n",
            "train loss : 0.00029661859957639234    train_acc : 0.04502223313225055\n",
            "train loss : 0.00041012123216993687    train_acc : 0.04002401185767053\n",
            "train loss : 0.0003989396951198038    train_acc : 0.03835467947299075\n",
            "train loss : 0.0002342129420419057    train_acc : 0.04502045582905226\n",
            "train loss : 0.00012621358048498256    train_acc : 0.05002401090977549\n",
            "train loss : 0.0004744028890133518    train_acc : 0.036693346139151886\n",
            "train loss : 0.0004988251101093293    train_acc : 0.03668623645127421\n",
            "train loss : 0.0003213148084201345    train_acc : 0.04168623265944068\n",
            "train loss : 0.0003540324773135142    train_acc : 0.0433555659907517\n",
            "train loss : 0.00029804988477525403    train_acc : 0.04168978963519507\n",
            "train loss : 0.000324972866290272    train_acc : 0.040022234554472104\n",
            "train loss : 0.0002155445143372819    train_acc : 0.04668801185842905\n",
            "train loss : 0.000312119555056084    train_acc : 0.04002490027299117\n",
            "train loss : 0.0003745968340935071    train_acc : 0.04335467994681227\n",
            "train loss : 0.00032835247332250284    train_acc : 0.03835645582930497\n",
            "train loss : 0.0002502870032109917    train_acc : 0.04335379010977563\n",
            "train loss : 0.00025294150095351106    train_acc : 0.04668978868805855\n",
            "train loss : 0.00026187520517339266    train_acc : 0.03835823455396696\n",
            "train loss : 0.000351327054878306    train_acc : 0.04002045772509545\n",
            "train loss : 0.0003114125343922404    train_acc : 0.04002134424412005\n",
            "train loss : 0.00040075755549604436    train_acc : 0.03335467805026353\n",
            "train loss : 0.0002573144943260253    train_acc : 0.04668445582829347\n",
            "train loss : 0.0004481422113053397    train_acc : 0.03502489837644176\n",
            "train loss : 0.0003507418180681161    train_acc : 0.041685346612467436\n",
            "train loss : 0.0003898171256778069    train_acc : 0.038355565518193316\n",
            "train loss : 0.0004315700545666375    train_acc : 0.038353789634943035\n",
            "train loss : 0.00034274795990068424    train_acc : 0.0383537886878053\n",
            "train loss : 0.0003008559658227543    train_acc : 0.040020455353966825\n",
            "train loss : 0.0005522754518357436    train_acc : 0.03002134424285545\n",
            "train loss : 0.00036719362400522063    train_acc : 0.03834934471692952\n",
            "train loss : 0.00045021958308994793    train_acc : 0.03502045298384903\n",
            "train loss : 0.00020531685928322252    train_acc : 0.04668534424159138\n",
            "train loss : 0.0003390102591608886    train_acc : 0.03835823218359552\n",
            "train loss : 0.0005306467898160349    train_acc : 0.03668712439049792\n",
            "train loss : 0.0003997888493796174    train_acc : 0.03668623313300826\n",
            "train loss : 0.0004175147801679113    train_acc : 0.04001956599100427\n",
            "train loss : 0.0003700942909129483    train_acc : 0.041688010435195204\n",
            "train loss : 0.0003750031519527122    train_acc : 0.0366889002722321\n",
            "train loss : 0.0005618492606125193    train_acc : 0.03168623408014519\n",
            "train loss : 0.0001840116108705119    train_acc : 0.04501689932484274\n",
            "train loss : 0.0003266631190790203    train_acc : 0.040024009012973254\n",
            "train loss : 0.0003819183507826494    train_acc : 0.036688012804806915\n",
            "train loss : 0.0005153690187665763    train_acc : 0.03668623360682923\n",
            "train loss : 0.00036534565154226794    train_acc : 0.04168623265792364\n",
            "train loss : 0.000262808401836096    train_acc : 0.041688899324084226\n",
            "train loss : 0.0002978235941989968    train_acc : 0.04168890074630618\n",
            "train loss : 0.00045983961902522314    train_acc : 0.03502223408039803\n",
            "train loss : 0.00043253281366391956    train_acc : 0.036685345191509545\n",
            "train loss : 0.00032313813198222885    train_acc : 0.04335289885076881\n",
            "train loss : 0.00043426460769138665    train_acc : 0.04168978821272041\n",
            "train loss : 0.00035397932533023815    train_acc : 0.04335556788704679\n",
            "train loss : 0.0002963550182215563    train_acc : 0.04502312296953976\n",
            "train loss : 0.00025107391713947893    train_acc : 0.04335734566558375\n",
            "train loss : 0.00024744532335090047    train_acc : 0.04502312391768831\n",
            "train loss : 0.00038226911047738545    train_acc : 0.0400240123327561\n",
            "train loss : 0.0003567977400282735    train_acc : 0.04168801280657747\n",
            "train loss : 0.00034352862271476637    train_acc : 0.0383555669401635\n",
            "train loss : 0.00041351272118585804    train_acc : 0.03502045630236809\n",
            "train loss : 0.00042476150488033557    train_acc : 0.03335201091002793\n",
            "train loss : 0.0005320463280841784    train_acc : 0.031684454405818685\n",
            "train loss : 0.00041473348160324966    train_acc : 0.040016898375683105\n",
            "train loss : 0.00031544663344075806    train_acc : 0.04002134234580036\n",
            "train loss : 0.0002802429617661065    train_acc : 0.04502134471591776\n",
            "train loss : 0.000246433470939065    train_acc : 0.04335734471718183\n",
            "train loss : 0.00044268679515748263    train_acc : 0.03668979058384916\n",
            "train loss : 0.0003308628416691686    train_acc : 0.04335290122164472\n",
            "train loss : 0.0004727984414851207    train_acc : 0.03668978821398487\n",
            "train loss : 0.00043240350855437145    train_acc : 0.035019567887047465\n",
            "train loss : 0.0004095148889879282    train_acc : 0.03501867710287309\n",
            "train loss : 0.0003077697601238176    train_acc : 0.04335200996112153\n",
            "train loss : 0.00037183791859130434    train_acc : 0.040023121071979265\n",
            "train loss : 0.00022214706437038412    train_acc : 0.04502134566457172\n",
            "train loss : 0.00033845771183276836    train_acc : 0.0416906780510211\n",
            "train loss : 0.0005475243370735895    train_acc : 0.03668890169496055\n",
            "train loss : 0.00031429144331831426    train_acc : 0.043352900747570644\n",
            "train loss : 0.0003138318994977157    train_acc : 0.04168978821373204\n",
            "train loss : 0.0005479748797596475    train_acc : 0.03335556788704732\n",
            "train loss : 0.0003937220094337559    train_acc : 0.038351122969539755\n",
            "train loss : 0.0002730226694487325    train_acc : 0.045020453932250425\n",
            "train loss : 0.0003036847928459879    train_acc : 0.04002401090876387\n",
            "train loss : 0.0002364806965308346    train_acc : 0.04502134613915134\n",
            "train loss : 0.000475938621766584    train_acc : 0.03669067805127421\n",
            "train loss : 0.0002564185118694344    train_acc : 0.04668623502829401\n",
            "train loss : 0.0003664646145108551    train_acc : 0.038358232658681754\n",
            "train loss : 0.0004364095133461636    train_acc : 0.036687124390751294\n",
            "train loss : 0.00041701137977186403    train_acc : 0.0366862331330084\n",
            "train loss : 0.000184324512163572    train_acc : 0.0483528993243376\n",
            "train loss : 0.0002586228037565442    train_acc : 0.04335912154630631\n",
            "train loss : 0.00031662766045383294    train_acc : 0.04168979153149136\n",
            "train loss : 0.00019439076203190512    train_acc : 0.04668890122215013\n",
            "train loss : 0.00041274071807235124    train_acc : 0.04002490074731848\n",
            "train loss : 0.00042999676514241203    train_acc : 0.036688013280398564\n",
            "train loss : 0.0002343938080027293    train_acc : 0.045019566940416214\n",
            "train loss : 0.000557969299174815    train_acc : 0.03335734376903489\n",
            "train loss : 0.0005906540925170144    train_acc : 0.03335112391667682\n",
            "train loss : 0.00043160408437383787    train_acc : 0.04001778726608889\n",
            "train loss : 0.0005714516040468224    train_acc : 0.03335467615320858\n",
            "train loss : 0.00037301681413633473    train_acc : 0.03835112249394838\n",
            "train loss : 0.0003462335009149678    train_acc : 0.04335378726533011\n",
            "train loss : 0.00040699058641533174    train_acc : 0.038356455353208176\n",
            "train loss : 0.00020685868800412042    train_acc : 0.04668712344285505\n",
            "train loss : 0.0002326127374296227    train_acc : 0.04335823313250285\n",
            "train loss : 0.0002606510351225601    train_acc : 0.04335645772433734\n",
            "train loss : 0.000329865169484357    train_acc : 0.04002312344411965\n",
            "train loss : 0.00038834353190101995    train_acc : 0.038354678999170194\n",
            "train loss : 0.0005599720421891818    train_acc : 0.030020455828799558\n",
            "train loss : 0.00029460928509730556    train_acc : 0.0433493442431087\n",
            "train loss : 0.0002546055229063569    train_acc : 0.04668978631692966\n",
            "train loss : 0.00015819692840107576    train_acc : 0.0466915678860357\n",
            "train loss : 0.00038325208358886467    train_acc : 0.04002490216953922\n",
            "train loss : 0.00041092068045898506    train_acc : 0.04168801328115709\n",
            "train loss : 0.00038757890399344533    train_acc : 0.03835556694041662\n",
            "train loss : 0.00036411735200848075    train_acc : 0.03835378963570155\n",
            "train loss : 0.0004429978331077574    train_acc : 0.033353788687805704\n",
            "train loss : 0.00022461772824394164    train_acc : 0.045017788687300164\n",
            "train loss : 0.0003387962169693049    train_acc : 0.041690676153966565\n",
            "train loss : 0.00028339567366638427    train_acc : 0.043355568360615444\n",
            "train loss : 0.00010452424972631348    train_acc : 0.05168978963645899\n",
            "train loss : 0.0003244770566913254    train_acc : 0.04169423455447278\n",
            "train loss : 0.0002923292739126573    train_acc : 0.04168890359176238\n",
            "train loss : 0.00027985078402746697    train_acc : 0.041688900748582274\n",
            "train loss : 0.0003103398042377281    train_acc : 0.04335556741373258\n",
            "train loss : 0.00037411052824430765    train_acc : 0.038356456302620655\n",
            "train loss : 0.00027636396195539567    train_acc : 0.043353790110028066\n",
            "train loss : 0.00033038150844206306    train_acc : 0.041689788688058685\n",
            "train loss : 0.0005107972743329855    train_acc : 0.0333555678873003\n",
            "train loss : 0.00034010663846981354    train_acc : 0.04168445630287323\n",
            "train loss : 0.00038513997531053676    train_acc : 0.03835556504336153\n",
            "train loss : 0.00038721394782244425    train_acc : 0.03835378963468979\n",
            "train loss : 0.00035482093086372475    train_acc : 0.04002045535447183\n",
            "train loss : 0.0002207867116101081    train_acc : 0.04502134424285571\n",
            "train loss : 0.0003936116277806159    train_acc : 0.04335734471692952\n",
            "train loss : 0.0003328087713654641    train_acc : 0.04168979058384903\n",
            "train loss : 0.0005887475841726905    train_acc : 0.02668890122164472\n",
            "train loss : 0.0005414117032810574    train_acc : 0.035014234080651546\n",
            "train loss : 0.00015366585671395855    train_acc : 0.04668534092484301\n",
            "train loss : 0.0002937215832160287    train_acc : 0.041691565515159915\n",
            "train loss : 0.00038450440821474323    train_acc : 0.04002223550160809\n",
            "train loss : 0.00042059278213213506    train_acc : 0.04002134519226753\n",
            "train loss : 0.0003801728368108709    train_acc : 0.03835467805076921\n",
            "train loss : 0.0002423687375958922    train_acc : 0.04002045582829374\n",
            "train loss : 0.0004323995041104092    train_acc : 0.036688010909775096\n",
            "train loss : 0.00024021730295431624    train_acc : 0.04335290027248521\n",
            "train loss : 0.0005115766263827833    train_acc : 0.03168978821347866\n",
            "train loss : 0.0003959610885480682    train_acc : 0.04001690122038053\n",
            "train loss : 0.00013699803748798897    train_acc : 0.045021342347317535\n",
            "train loss : 0.00034072333798937837    train_acc : 0.04002401138258523\n",
            "train loss : 0.00027879840180198954    train_acc : 0.04502134613940405\n",
            "train loss : 0.00022398789599760332    train_acc : 0.04669067805127435\n",
            "train loss : 0.00029125092485149823    train_acc : 0.04002490169496068\n",
            "train loss : 0.00014249699527983453    train_acc : 0.048354679947570645\n",
            "train loss : 0.00028362349519313687    train_acc : 0.03835912249597204\n",
            "train loss : 0.00047863265395614406    train_acc : 0.03502045819866452\n",
            "train loss : 0.0006139168460661593    train_acc : 0.03335201091103929\n",
            "train loss : 0.0005259956937005517    train_acc : 0.03335112107248589\n",
            "train loss : 0.0003088842628278187    train_acc : 0.04335112059790532\n",
            "train loss : 0.00044986649252562406    train_acc : 0.03502312059765222\n",
            "train loss : 0.0003535434884171888    train_acc : 0.03835201233098542\n",
            "train loss : 0.0005997235449304762    train_acc : 0.03335378773990986\n",
            "train loss : 0.000251266755786056    train_acc : 0.04001778868679462\n",
            "train loss : 0.0002159082907560807    train_acc : 0.04835467615396629\n",
            "train loss : 0.0004978501745869384    train_acc : 0.035025789160615446\n",
            "train loss : 0.0003208065840376117    train_acc : 0.041685347087552324\n",
            "train loss : 0.00019724555782809643    train_acc : 0.04668889885178003\n",
            "train loss : 0.00038374084572247576    train_acc : 0.04169156741272095\n",
            "train loss : 0.0004625344309189068    train_acc : 0.04002223550262012\n",
            "train loss : 0.0003590838127153703    train_acc : 0.04168801185893473\n",
            "train loss : 0.0004019986606180836    train_acc : 0.03668890027299143\n",
            "train loss : 0.0004333047681923199    train_acc : 0.041686234080145594\n",
            "train loss : 0.0003132074879865581    train_acc : 0.04335556599150941\n",
            "train loss : 0.00033116276263169507    train_acc : 0.0400231229685288\n",
            "train loss : 0.00028001149461109886    train_acc : 0.043354678998916554\n",
            "train loss : 0.00040867699983366564    train_acc : 0.04168978916213276\n",
            "train loss : 0.0003907573211129647    train_acc : 0.040022234554219806\n",
            "train loss : 0.0005152969303124712    train_acc : 0.03835467852509558\n",
            "train loss : 0.0002913169621754961    train_acc : 0.04002045582854672\n",
            "train loss : 0.00021533233285264594    train_acc : 0.043354677576441895\n",
            "train loss : 0.00019132482773653834    train_acc : 0.046689789161374104\n",
            "train loss : 0.00019477427476123488    train_acc : 0.046691567887552736\n",
            "train loss : 0.00014869573180131633    train_acc : 0.04669156883620669\n",
            "train loss : 0.00035008362771386174    train_acc : 0.04002490217004598\n",
            "train loss : 0.0003075917380218575    train_acc : 0.04335467994782403\n",
            "train loss : 0.00023847060308547364    train_acc : 0.04502312249597217\n",
            "train loss : 0.00033913748654739427    train_acc : 0.04002401233199786\n",
            "train loss : 0.0002473678201950551    train_acc : 0.04335467947324373\n",
            "train loss : 0.000578759007242548    train_acc : 0.03502312249571906\n",
            "train loss : 0.0002861144372582898    train_acc : 0.043352012331997716\n",
            "train loss : 0.0002261791135061455    train_acc : 0.04502312107324373\n",
            "train loss : 0.00023402333754990836    train_acc : 0.04669067899790573\n",
            "train loss : 0.00034641021919514707    train_acc : 0.04169156836213222\n",
            "train loss : 0.0003129850641591404    train_acc : 0.0433555688364598\n",
            "train loss : 0.0003464841212598855    train_acc : 0.040023122970046114\n",
            "train loss : 0.0005091279678008699    train_acc : 0.03502134566558402\n",
            "train loss : 0.0002943279822631657    train_acc : 0.04168534471768831\n",
            "train loss : 0.00042857192370520503    train_acc : 0.0366888988505161\n",
            "train loss : 0.0002597222564613195    train_acc : 0.04668623407938694\n",
            "train loss : 0.00027931290996540894    train_acc : 0.04335823265817567\n",
            "train loss : 0.0002984190652536417    train_acc : 0.04335645772408436\n",
            "train loss : 0.0007497991129912148    train_acc : 0.03168979011078618\n",
            "train loss : 0.0006150871131157959    train_acc : 0.028350234554725753\n",
            "train loss : 0.0002676121218794365    train_acc : 0.04668178679176252\n",
            "train loss : 0.000529811756972408    train_acc : 0.036691563619622274\n",
            "train loss : 0.0003292397742680253    train_acc : 0.0383529021672638\n",
            "train loss : 0.00034054930136390323    train_acc : 0.040020454881155874\n",
            "train loss : 0.0002787757704794449    train_acc : 0.04168801090926995\n",
            "train loss : 0.0002204454606792736    train_acc : 0.041688900272484945\n",
            "train loss : 0.00045681668793710184    train_acc : 0.03502223408014533\n",
            "train loss : 0.00011104242363757849    train_acc : 0.05168534519150941\n",
            "train loss : 0.0003908476188254843    train_acc : 0.036694232184102143\n",
            "train loss : 0.0002715817875838    train_acc : 0.04335290359049819\n",
            "train loss : 0.00039486473039982984    train_acc : 0.038356454881914935\n",
            "train loss : 0.0006239090833941364    train_acc : 0.026687123442603686\n",
            "train loss : 0.0003940714345759222    train_acc : 0.04001423313250272\n",
            "train loss : 0.0004345532551313181    train_acc : 0.036688007591004\n",
            "train loss : 0.0002465333491587588    train_acc : 0.04501956693738187\n",
            "train loss : 0.0003614967709620807    train_acc : 0.03835734376903327\n",
            "train loss : 0.0001550940643758689    train_acc : 0.04668712391667682\n",
            "train loss : 0.0005363696431038651    train_acc : 0.03835823313275556\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018244284437969327    test_acc : 0.2496006389776358\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002119756070896983    test_acc : 0.22044600843123846\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023154322989284992    test_acc : 0.24032091376495604\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0012499322183430195    test_acc : 0.27033648854238007\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0012235998874530196    test_acc : 0.30038446162473603\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00120631349273026    test_acc : 0.2705283848614209\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015540627064183354    test_acc : 0.270432998034701\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021886788308620453    test_acc : 0.2604486677253505\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024141862522810698    test_acc : 0.22048066667004906\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002101972931995988    test_acc : 0.2702731011714698\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023317744489759207    test_acc : 0.21052802907722515\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002675920259207487    test_acc : 0.22032117581174832\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014496115036308765    test_acc : 0.26028856605690653\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017395340837538242    test_acc : 0.27040028295864826\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029079855885356665    test_acc : 0.24048051208612986\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017322595231235027    test_acc : 0.2603529728820643\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001807783730328083    test_acc : 0.2404484120539363\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023777440655976534    test_acc : 0.22041676808962918\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021239067427814007    test_acc : 0.2403208203453343\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002605013083666563    test_acc : 0.2104323348892822\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001794160925783217    test_acc : 0.2602569723159402\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00230532162822783    test_acc : 0.25043213090196786\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002297809347510338    test_acc : 0.27036879275048553\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024232848081737757    test_acc : 0.24048041147843607\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0008011111058294773    test_acc : 0.3002890747970557\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021958076395094395    test_acc : 0.24057600343385638\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016827601939439774    test_acc : 0.2503692524071369\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017992709763348103    test_acc : 0.28035261741983114\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002161875367164612    test_acc : 0.22054425756364165\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0012790526961907744    test_acc : 0.27027330433726404\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0035589197650551796    test_acc : 0.20054400416721171\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002335186116397381    test_acc : 0.23027330352769076\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018642963841557503    test_acc : 0.23036828531478498\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001586918719112873    test_acc : 0.26032066544829\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025776198599487543    test_acc : 0.22048025771708718\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016801099991425872    test_acc : 0.25030504874669995\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002108744578436017    test_acc : 0.25040033561899905\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015218668850138783    test_acc : 0.250400640049901\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002319970401003957    test_acc : 0.23043258990431278\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016729494091123343    test_acc : 0.260320870894263\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002613435033708811    test_acc : 0.2504323350507804\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002633658703416586    test_acc : 0.25040074228450726\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019059684127569199    test_acc : 0.2504006413491518\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023673945106565952    test_acc : 0.22044856434935828\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0007984881522133946    test_acc : 0.28025702416725035\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002574076410382986    test_acc : 0.24051200327209984\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029943627305328846    test_acc : 0.20044892013824953\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024397268425673246    test_acc : 0.24025702530395604\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015793690690770745    test_acc : 0.23040018218946953\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018137528095394373    test_acc : 0.25033674179613247\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027852817438542843    test_acc : 0.19049628352011544\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023720301687717438    test_acc : 0.22025717662466493\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015519744483754039    test_acc : 0.28025641270487117\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001921624643728137    test_acc : 0.2504960268776513\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021137751173228025    test_acc : 0.2703689968909829\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014978168765082955    test_acc : 0.28041651436706383\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022601697128266096    test_acc : 0.23052848726634845\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002896835096180439    test_acc : 0.2104010494800842\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0009559181635268033    test_acc : 0.2902089490398725\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0009099815506488085    test_acc : 0.3004479519138654\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017931147012859583    test_acc : 0.25056053658758426\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003377408254891634    test_acc : 0.18051297296034371\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002685610670596361    test_acc : 0.2302093066228765\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030740988440811634    test_acc : 0.22038405529272487\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029463793616741896    test_acc : 0.21036863915428985\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001742068794555962    test_acc : 0.250272743256084\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016288961051031947    test_acc : 0.26038425796567444\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002250213176012039    test_acc : 0.26041656312449096\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002676756586879492    test_acc : 0.21049653854033384\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013116616755723953    test_acc : 0.2802252285576369\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001499423524364829    test_acc : 0.27046397836599884\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016604320844635367    test_acc : 0.2504647411449393\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002572139725089073    test_acc : 0.22044876914103814\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002298125997185707    test_acc : 0.24032092258511514\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022216022480279207    test_acc : 0.24038441189324317\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00208867690525949    test_acc : 0.24038461473448322\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001482810010202229    test_acc : 0.27033669205985456\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024310112930834293    test_acc : 0.2304962833612136\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002938966965302825    test_acc : 0.2203849721513138\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001841869787313044    test_acc : 0.25030474431997224\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0011780342319980264    test_acc : 0.280352411323706\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002042227191850543    test_acc : 0.25049633358250384\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021620008628815413    test_acc : 0.2504009467526598\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022314749658107758    test_acc : 0.26038466756151013\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022644270211458206    test_acc : 0.23046448775578757\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024669822305440903    test_acc : 0.2603209728043316\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001850317814387381    test_acc : 0.24044830981726625\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002035544952377677    test_acc : 0.2603528699994162\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029347592499107122    test_acc : 0.21049633504792148\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027286196127533913    test_acc : 0.2303051001119742\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003763972083106637    test_acc : 0.18044825910578907\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002830672077834606    test_acc : 0.22022507431024213\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001474766875617206    test_acc : 0.27027228458246083\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002794831059873104    test_acc : 0.22051205202742\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023061472456902266    test_acc : 0.23033709920775533\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025119672063738108    test_acc : 0.22038446357574365\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022091837599873543    test_acc : 0.21036864045870843\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002371033187955618    test_acc : 0.25027274326025145\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017360117053613067    test_acc : 0.2603842579656877\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002227702410891652    test_acc : 0.24044851200628015\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020437040366232395    test_acc : 0.25036884508628204\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0035258589778095484    test_acc : 0.2004804116456431\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022667988669127226    test_acc : 0.24025712591580078\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026208623312413692    test_acc : 0.2304001825109131\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002075547818094492    test_acc : 0.23036869067894863\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0010867377277463675    test_acc : 0.2802887178615941\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002343005035072565    test_acc : 0.23052807897080382\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002161358017474413    test_acc : 0.220385073734731\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0038510183803737164    test_acc : 0.20038461684899275\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017917028162628412    test_acc : 0.2502408454212428\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025337301194667816    test_acc : 0.23043207937834265\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002038322389125824    test_acc : 0.2603208692631896\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026155447121709585    test_acc : 0.23046428392735846\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0012200635392218828    test_acc : 0.29027304883043886\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020543106365948915    test_acc : 0.24054400335089599\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022255994845181704    test_acc : 0.2403851246113447\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020696823485195637    test_acc : 0.2503686425706433\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021002255380153656    test_acc : 0.23043248767594457\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002304062945768237    test_acc : 0.2403528194494439\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022762445732951164    test_acc : 0.26035256491836883\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021529649384319782    test_acc : 0.2404484107505379\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017197913257405162    test_acc : 0.24038481920367583\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003001059638336301    test_acc : 0.20044851379937276\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003054161323234439    test_acc : 0.2103049473284325\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021602653432637453    test_acc : 0.2502725397678225\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018261385848745704    test_acc : 0.26038425731555215\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002970286412164569    test_acc : 0.22048046088599219\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003087686374783516    test_acc : 0.20038492160027474\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001722754561342299    test_acc : 0.24025682083578362\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0041583143174648285    test_acc : 0.1804800537406894\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00214198580943048    test_acc : 0.2302092014496508\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026402955409139395    test_acc : 0.22038405495670815\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023615544196218252    test_acc : 0.22035266471232176\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017683054320514202    test_acc : 0.2702726922195282\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023156162351369858    test_acc : 0.2404801044479857\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00122306018602103    test_acc : 0.28032102269791687\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001709340256638825    test_acc : 0.2604802588584598\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020965333096683025    test_acc : 0.24044881871839766\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001679338631220162    test_acc : 0.2503688460661929\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014123865403234959    test_acc : 0.2803526161216172\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021943296305835247    test_acc : 0.2504963342368103\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0006295997882261872    test_acc : 0.30032107455027734\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015582286287099123    test_acc : 0.2605441567877006\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017568337498232722    test_acc : 0.2604170739833473\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00197191652841866    test_acc : 0.24044861684978705\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024731801822781563    test_acc : 0.24038481986213991\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003787380876019597    test_acc : 0.19046448824237106\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018856306560337543    test_acc : 0.2601931772787296\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029371113050729036    test_acc : 0.19052777372932503\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017029002774506807    test_acc : 0.27017740502788923\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003407930489629507    test_acc : 0.2205117488978527\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024026071187108755    test_acc : 0.23033709823929024\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018674660241231322    test_acc : 0.24035251469086036\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002216643886640668    test_acc : 0.23040048726738294\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031542389187961817    test_acc : 0.2104006405344006\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021707250270992517    test_acc : 0.2402888199378096\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029692004900425673    test_acc : 0.21043223265155847\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002639743499457836    test_acc : 0.22032086975288037\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0038050024304538965    test_acc : 0.2003844117244501\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017754884902387857    test_acc : 0.26022487032499825\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020403293892741203    test_acc : 0.2604160538988019\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027822016272693872    test_acc : 0.24044861359073103\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032883300445973873    test_acc : 0.21043274317441127\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001778530073352158    test_acc : 0.270240999179471\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003115118946880102    test_acc : 0.21052792651495036\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020932559855282307    test_acc : 0.2502732521613896\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002227320335805416    test_acc : 0.27036828515067535\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0012030306970700622    test_acc : 0.27043248653402774\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001263501588255167    test_acc : 0.27043269165026845\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019374775001779199    test_acc : 0.2304965900691702\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002475061919540167    test_acc : 0.24035302424942226\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002020719926804304    test_acc : 0.2603525655726818\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.004181216470897198    test_acc : 0.19052828295710122\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002072148025035858    test_acc : 0.25020935553660417\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00415667612105608    test_acc : 0.1805118509761553\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024120276793837547    test_acc : 0.24019332859736792\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028551591094583273    test_acc : 0.170495825330982\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0034877487923949957    test_acc : 0.17027314960169643\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024694562889635563    test_acc : 0.24016061709137923\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002156604314222932    test_acc : 0.25036792529422164\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002266417955979705    test_acc : 0.25040053650253746\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027410162147134542    test_acc : 0.2504006406917014\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022632130421698093    test_acc : 0.2603846665836796\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0008259780588559806    test_acc : 0.30035266666640154\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016115481266751885    test_acc : 0.2605442577209789\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002111043082550168    test_acc : 0.2204809720693961\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026309045497328043    test_acc : 0.21036894879255397\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002095815958455205    test_acc : 0.23030469312713275\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002197908703237772    test_acc : 0.2403524111601506\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00142129126470536    test_acc : 0.26035256361393017\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002372742397710681    test_acc : 0.2104963340690541\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020911216270178556    test_acc : 0.24028912566795224\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025601477827876806    test_acc : 0.21043223362833213\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022060510236769915    test_acc : 0.24028892087421194\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018158453749492764    test_acc : 0.2503683352104607\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023732450790703297    test_acc : 0.23043248669396316\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027179440949112177    test_acc : 0.20041671720988488\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002755784662440419    test_acc : 0.21030484574188463\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002316167112439871    test_acc : 0.22032046276594852\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018912643427029252    test_acc : 0.260288563778805\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024914557579904795    test_acc : 0.22048015515584282\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018929860088974237    test_acc : 0.2702730995372391\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002229069359600544    test_acc : 0.24048010574932024\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020124437287449837    test_acc : 0.22041686934744195\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022590323351323605    test_acc : 0.24032082066884167\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002999481512233615    test_acc : 0.20044830933121036\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002054783748462796    test_acc : 0.28019312558891757\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0027755575720220804    test_acc : 0.22054374800507642\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002564632100984454    test_acc : 0.22035317491375422\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003193550743162632    test_acc : 0.21036854049493212\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019398895092308521    test_acc : 0.2303046918226675\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001237333519384265    test_acc : 0.27030448783329925\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002012578770518303    test_acc : 0.2404802060314163\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001908013829961419    test_acc : 0.24038492078604287\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002370343543589115    test_acc : 0.23040059080123337\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0029121788684278727    test_acc : 0.22038466642428511\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020706222858279943    test_acc : 0.2303366922249977\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0011347983963787556    test_acc : 0.26032056451190094\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017513006459921598    test_acc : 0.2704003851901339\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025536995381116867    test_acc : 0.22051246129453717\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001882951706647873    test_acc : 0.230337100515318\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002033827593550086    test_acc : 0.25033654025723745\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022877748124301434    test_acc : 0.23043238511264294\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015315740602090955    test_acc : 0.2603208702399765\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030507370829582214    test_acc : 0.2104962328122683\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00240270490758121    test_acc : 0.21033704866713188\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001773625728674233    test_acc : 0.26025666788711543\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001788999536074698    test_acc : 0.26041615548845726\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002067842986434698    test_acc : 0.22048056279708772\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003016858361661434    test_acc : 0.19040089636676386\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030684308148920536    test_acc : 0.21027284631427082\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017249704105779529    test_acc : 0.2602564627677772\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002423524158075452    test_acc : 0.23046407815580758\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0009432360529899597    test_acc : 0.2802890226139163\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024951985105872154    test_acc : 0.2005760032671371\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015510286903008819    test_acc : 0.280193533556764\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003003725316375494    test_acc : 0.2005756981902772\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002449622843414545    test_acc : 0.24025743034565583\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002728101797401905    test_acc : 0.23040018348353242\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002233721548691392    test_acc : 0.2203846651229506\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016865910729393363    test_acc : 0.24032071777994551\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002444996265694499    test_acc : 0.22041636012070273\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001597265130840242    test_acc : 0.2602888701601301\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016390139935538173    test_acc : 0.27040028393022403\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032196708489209414    test_acc : 0.20054440985281222\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019494259031489491    test_acc : 0.24025733038291633\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003336749505251646    test_acc : 0.22041615760505726\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016917175380513072    test_acc : 0.27027289507222063\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002410518005490303    test_acc : 0.2404801050960774\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002626285655423999    test_acc : 0.21043284378624946\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002616977784782648    test_acc : 0.2203208717053874\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00207560695707798    test_acc : 0.24032051396710985\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002490839222446084    test_acc : 0.22041635946954347\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001908869598992169    test_acc : 0.2403208190398388\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028502820059657097    test_acc : 0.2104323348851113\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013754707761108875    test_acc : 0.2702409978750323\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.003359382040798664    test_acc : 0.2205119520698883\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00172320194542408    test_acc : 0.28025722668392933\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002293445635586977    test_acc : 0.2105599272418017\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023432259913533926    test_acc : 0.22032127772281726\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001181327155791223    test_acc : 0.2802566175007119\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0017451889580115676    test_acc : 0.2604800530910566\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001526829320937395    test_acc : 0.2704008947383101\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014618803979828954    test_acc : 0.26044856515890835\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019167514983564615    test_acc : 0.25043274301967705\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016788025386631489    test_acc : 0.2504007435879223\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001810716581530869    test_acc : 0.23043259023510518\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020390921272337437    test_acc : 0.28028892201353073\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025820238515734673    test_acc : 0.24051210518215183\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0026613224763423204    test_acc : 0.2204169715820516\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016495444579049945    test_acc : 0.2702728976727861\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013996025081723928    test_acc : 0.2704321817817022\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0030146827921271324    test_acc : 0.21052853732198626\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0037780546117573977    test_acc : 0.20035312631732263\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002822824753820896    test_acc : 0.23027269369430456\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014202652964740992    test_acc : 0.26032036004375175\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0016155533958226442    test_acc : 0.2504323334186701\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024896152317523956    test_acc : 0.23043269116108203\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0024262196384370327    test_acc : 0.2503368456586616\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025357892736792564    test_acc : 0.20048030941105002\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013422999763861299    test_acc : 0.270209202266489\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0013203320559114218    test_acc : 0.2904000294002124\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001410995377227664    test_acc : 0.25052843459872276\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014867960708215833    test_acc : 0.2703691004300279\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021479155402630568    test_acc : 0.23049638690233237\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002805108204483986    test_acc : 0.22038497248211608\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032662302255630493    test_acc : 0.21036864208460743\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002364892279729247    test_acc : 0.23030469214723515\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.00251863244920969    test_acc : 0.22038436003880904\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0032842615619301796    test_acc : 0.2103686401279195\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002577677834779024    test_acc : 0.23030469214098379\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0018694528844207525    test_acc : 0.2503364367161054\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002376670017838478    test_acc : 0.2304323847818406\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002046935260295868    test_acc : 0.25033684467981415\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0009021339938044548    test_acc : 0.2903365394398716\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002424837788566947    test_acc : 0.2305601806371881\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0025778585113584995    test_acc : 0.2603212785323872\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0021391822956502438    test_acc : 0.23046428523492776\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002378148725256324    test_acc : 0.24035292103908923\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0019716271199285984    test_acc : 0.25036853968383094\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0014514500508084893    test_acc : 0.260384564024549\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002486656652763486    test_acc : 0.23046448742499856\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0023220819421112537    test_acc : 0.2403529216850639\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001175528741441667    test_acc : 0.28032061636321104\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0010728368069976568    test_acc : 0.27046428311937126\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0015030807117000222    test_acc : 0.2504647421185922\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.002468402497470379    test_acc : 0.23043279470325428\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.001139195403084159    test_acc : 0.27030489710767813\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0028578690253198147    test_acc : 0.21052813066168588\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0020746744703501463    test_acc : 0.2502732528136156\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0031179802026599646    test_acc : 0.21046413179812656\n",
            "Looked at 59968/10000 samples\n",
            "test loss : 0.0022767651826143265    test_acc : 0.24028902278529754\n",
            "Looked at 29984/10000 samples\n",
            "test loss : 0.0012777423253282905    test_acc : 0.28032041221337156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names))\n",
        "model_2"
      ],
      "metadata": {
        "id": "yf7UgkWyPQJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96adea7-24c3-4761-d6c2-06efe570fd0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModelV2(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                             lr=0.1)"
      ],
      "metadata": {
        "id": "DU-2S-hJtXQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
        "                                           end=train_time_end_model_2,\n",
        "                                           device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "4653649f7e1a491cbb0387d191c9fb20",
            "7f42f7ee44034f08bada4054100b491d",
            "de2266de4f2242098f03fbf2db67cf74",
            "055972a2331946cfaf3b68355ac36e21",
            "9a178178c2fe4d338f6e74b1f17a6d87",
            "cef70f63167e40f38a930b4a2295d91b",
            "5899c4b829544afb98aeb357183e047d",
            "d587c42d02e24231b029f50d069a71f4",
            "66093c065b854067ad2d10b637fa552b",
            "15a4c6e38cc34e62bc3ae78346b277c9",
            "393b7ddd963f4f10ab63327afdcc6059"
          ]
        },
        "id": "J2bq6Rj5tYrF",
        "outputId": "68d4454c-dc57-44b9-dd9a-de0e22fc9d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4653649f7e1a491cbb0387d191c9fb20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 0.59172 | Train accuracy: 78.54%\n",
            "Test loss: 0.39393 | Test accuracy: 85.90%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 0.36431 | Train accuracy: 86.85%\n",
            "Test loss: 0.35599 | Test accuracy: 86.76%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 0.32800 | Train accuracy: 87.99%\n",
            "Test loss: 0.32630 | Test accuracy: 88.23%\n",
            "\n",
            "train time on cuda: 36.198469 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = model_1.to(device)\n"
      ],
      "metadata": {
        "id": "xbQYl7NY6k2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move values to device\n",
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    \"\"\"Evaluates a given model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "        device (str, optional): Target device to compute on. Defaults to device.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 1 results with device-agnostic code\n",
        "model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")\n",
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba-QNCgQ534s",
        "outputId": "966364c6-6fe9-4878-e0de-85851ba3172d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV2',\n",
              " 'model_loss': 0.6850008964538574,\n",
              " 'model_acc': 75.01996805111821}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "3f64bfe53d1249dc9a13d7633ecc085a",
            "2a0b291154304d6081bcbcc14538abfc",
            "00c2ad43dc9246d1ae68310535068f84",
            "28879aff012b498b9e4998c296a61a3e",
            "7006127553dd473e822b2fbc0c5609f5",
            "9f61648423d24b918e6f2fd0960b4d9d",
            "4242529895d140c48e583e433beceda5",
            "c757ef1ef66b425b9643bed05ab79723",
            "f9877cbefb27466aafc3baa7558ecf89",
            "79838c39924e4309be9a6f56ad3e0f65",
            "78932f4ddd4f460faa014bf8237516a7"
          ]
        },
        "id": "YwpO8S759PxO",
        "outputId": "71548947-93cd-4855-e87a-96ec8d55762e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f64bfe53d1249dc9a13d7633ecc085a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 0.64302 | Train accuracy: 76.43%\n",
            "Test loss: 0.68500 | Test accuracy: 75.02%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 0.64302 | Train accuracy: 76.43%\n",
            "Test loss: 0.68500 | Test accuracy: 75.02%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 0.64302 | Train accuracy: 76.43%\n",
            "Test loss: 0.68500 | Test accuracy: 75.02%\n",
            "\n",
            "train time on cuda: 28.489816 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c09BjBSk7Loe",
        "outputId": "42cc9c6e-e9ba-4b85-c1fb-e5872aca9667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModelV2(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                             lr=0.1)"
      ],
      "metadata": {
        "id": "EmPGS_f88q2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
        "                                           end=train_time_end_model_2,\n",
        "                                           device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "32560de5bdd541b5a09118a707ea4b26",
            "c568c52541e848fc8a0d7070419dd995",
            "a29258a914d0486090072534fd99fdfb",
            "7c67efbff78a4aa1ac5faf17ffd67bb7",
            "6517f3cd12304fe39ae957a038401664",
            "16a06086214e467c865b9fa024709b66",
            "f41b281a6c8d476990b79f211471fe22",
            "1e2de3935b374b059085ab7d87e91ad1",
            "64f7fbe3d9f94167af7abba3aea213a5",
            "11d556973ebf48b1b4a143e53382aa2f",
            "33dc1cf5ebbb47ec90dd4393d155392b"
          ]
        },
        "id": "DR9Q0-pg7iFF",
        "outputId": "8966fb9a-c7ee-4763-afa5-d54e953de041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32560de5bdd541b5a09118a707ea4b26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 0.59402 | Train accuracy: 78.48%\n",
            "Test loss: 0.40540 | Test accuracy: 85.42%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 0.36215 | Train accuracy: 86.92%\n",
            "Test loss: 0.34992 | Test accuracy: 87.00%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 0.32601 | Train accuracy: 88.11%\n",
            "Test loss: 0.32238 | Test accuracy: 88.15%\n",
            "\n",
            "train time on cuda: 33.845912 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_2 results\n",
        "model_2_results = eval_model(\n",
        "    model=model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INFj2XzT76yP",
        "outputId": "fcbd73f3-24a2-4e6e-b450-ce41b9970155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV2',\n",
              " 'model_loss': 0.32237526774406433,\n",
              " 'model_acc': 88.14896166134186}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "YW8hyKDhtl8Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "bf8ac22b-a1d6-4a83-c734-7751919e0873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            model_name  model_loss  model_acc\n",
              "0       FashionMNISTVO    0.468694  83.266773\n",
              "1  FashionMNISTModelV2    0.685001  75.019968\n",
              "2  FashionMNISTModelV2    0.322375  88.148962"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b0c1c94-b265-46af-a228-df4f8ee38347\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_loss</th>\n",
              "      <th>model_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FashionMNISTVO</td>\n",
              "      <td>0.468694</td>\n",
              "      <td>83.266773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FashionMNISTModelV2</td>\n",
              "      <td>0.685001</td>\n",
              "      <td>75.019968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FashionMNISTModelV2</td>\n",
              "      <td>0.322375</td>\n",
              "      <td>88.148962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b0c1c94-b265-46af-a228-df4f8ee38347')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b0c1c94-b265-46af-a228-df4f8ee38347 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b0c1c94-b265-46af-a228-df4f8ee38347');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_7b3ee16b-8d6e-4e33-8d1a-4335f2a2d18e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('compare_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7b3ee16b-8d6e-4e33-8d1a-4335f2a2d18e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('compare_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "compare_results",
              "summary": "{\n  \"name\": \"compare_results\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"FashionMNISTModelV2\",\n          \"FashionMNISTVO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18243501276422078,\n        \"min\": 0.32237526774406433,\n        \"max\": 0.6850008964538574,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4686939154570095,\n          0.6850008964538574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.635963033647113,\n        \"min\": 75.01996805111821,\n        \"max\": 88.14896166134186,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          83.2667731629393,\n          75.01996805111821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training times to results comparison\n",
        "compare_results[\"training_time\"] = [total_train_time_model_0,\n",
        "                                    total_train_time_model_1,\n",
        "                                    total_train_time_model_2]\n",
        "compare_results"
      ],
      "metadata": {
        "id": "mpe8_ZkpttwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "f0b847a9-239f-4450-d9b1-bbfac9b6b670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            model_name  model_loss  model_acc  training_time\n",
              "0       FashionMNISTVO    0.468694  83.266773      46.562297\n",
              "1  FashionMNISTModelV2    0.685001  75.019968      28.489816\n",
              "2  FashionMNISTModelV2    0.322375  88.148962      33.845912"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-985072de-560e-4f5a-8098-706fc5960bd9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_loss</th>\n",
              "      <th>model_acc</th>\n",
              "      <th>training_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FashionMNISTVO</td>\n",
              "      <td>0.468694</td>\n",
              "      <td>83.266773</td>\n",
              "      <td>46.562297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FashionMNISTModelV2</td>\n",
              "      <td>0.685001</td>\n",
              "      <td>75.019968</td>\n",
              "      <td>28.489816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FashionMNISTModelV2</td>\n",
              "      <td>0.322375</td>\n",
              "      <td>88.148962</td>\n",
              "      <td>33.845912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-985072de-560e-4f5a-8098-706fc5960bd9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-985072de-560e-4f5a-8098-706fc5960bd9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-985072de-560e-4f5a-8098-706fc5960bd9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_2111ddc3-4d36-4646-a043-8f93e1e1b9d4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('compare_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2111ddc3-4d36-4646-a043-8f93e1e1b9d4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('compare_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "compare_results",
              "summary": "{\n  \"name\": \"compare_results\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"FashionMNISTModelV2\",\n          \"FashionMNISTVO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18243501276422078,\n        \"min\": 0.32237526774406433,\n        \"max\": 0.6850008964538574,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4686939154570095,\n          0.6850008964538574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.635963033647113,\n        \"min\": 75.01996805111821,\n        \"max\": 88.14896166134186,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          83.2667731629393,\n          75.01996805111821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.28267870784171,\n        \"min\": 28.48981640000011,\n        \"max\": 46.56229680800001,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          46.56229680800001,\n          28.48981640000011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our model results\n",
        "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
        "plt.xlabel(\"accuracy (%)\")\n",
        "plt.ylabel(\"model\");"
      ],
      "metadata": {
        "id": "RhIoYKlht2fB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "dab2bdec-9628-4582-83bd-96a4160d9b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)"
      ],
      "metadata": {
        "id": "aqmsGJT5t6HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
      ],
      "metadata": {
        "id": "W5ebHix_uHtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1185d424-c8c1-4716-ec15-100636d95280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sample image shape: torch.Size([1, 28, 28])\n",
            "Test sample label: 5 (Sandal)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "BwnPn-MzuOF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd32ccf4-1eee-4602-abcd-08653ef97bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5.7124e-08, 1.2049e-08, 1.2608e-08, 3.4206e-07, 7.4491e-09, 9.9989e-01,\n",
              "         4.3415e-07, 5.3206e-06, 4.8112e-06, 9.8176e-05],\n",
              "        [4.6157e-02, 7.0875e-01, 9.4557e-04, 1.0931e-01, 5.5394e-02, 2.8641e-05,\n",
              "         7.7943e-02, 8.4706e-04, 4.0272e-04, 2.2800e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "um9T4-OcuS_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2056121e-000f-4d3e-ce27-4d2f81677198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Are our predictions in the same form as our test labels?\n",
        "test_labels, pred_classes"
      ],
      "metadata": {
        "id": "drMENB0cuhIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379c4828-15f6-4115-a55c-300e3ba6ec4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "7ZqMpjs3uhu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "61474cf1-da97-44b8-bd41-94fb21272ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GU_l1cvaumYv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}